{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71a89eb6",
   "metadata": {},
   "source": [
    "# Notes on Sparse Categorical Loss vs. Categorical Loss\n",
    "# Understanding Cross-Entropy Loss\n",
    "\n",
    "At its heart, **cross-entropy** is a concept from information theory that measures how different two probability distributions are. In the context of training a neural network for classification, we use it to measure the \"distance\" between the model's predicted probability distribution and the true probability distribution of the labels. The goal of training is to minimise this distance, effectively making the model's predictions more accurate (Goodfellow et al., 2016).\n",
    "\n",
    "---\n",
    "### Categorical Cross-Entropy (for One-Hot Labels)\n",
    "\n",
    "You use this loss function when your labels are explicitly **one-hot encoded** (e.g., the digit `3` is represented as `[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]`). The formula for a single sample is:\n",
    "\n",
    "$$L = -\\sum_{i=0}^{C-1} y_i \\log(\\hat{y}_i)$$\n",
    "\n",
    "-   $L$ is the final loss value for the sample.\n",
    "-   $C$ is the total number of classes (e.g., 10 for MNIST).\n",
    "-   $y_i$ is the ground truth (it is `1` for the correct class and `0` for all others).\n",
    "-   $\\hat{y}_i$ is the model's predicted probability for class $i$.\n",
    "\n",
    "Because the `y` vector is almost all zeros, the summation simplifies to just the negative logarithm of the probability the model assigned to the single correct class. For a label of `3`, the loss simply becomes $L = -\\log(\\hat{y}_3)$.\n",
    "\n",
    "---\n",
    "### Sparse Categorical Cross-Entropy (for Integer Labels)\n",
    "\n",
    "This is a more computationally and memory-efficient version used when your labels are simple **integers** (e.g., `3`). It arrives at the exact same mathematical result but skips the need for the one-hot encoded vector.\n",
    "\n",
    "The formula is a direct implementation of the simplified logic:\n",
    "\n",
    "$$L = -\\log(\\hat{y}_c)$$\n",
    "\n",
    "-   $L$ is the final loss value for the sample.\n",
    "-   $c$ is the integer representing the correct class (e.g., `c = 3`).\n",
    "-   $\\hat{y}_c$ is the model's predicted probability for that correct class $c$.\n",
    "\n",
    "As Chollet (2021) explains, both formulas compute the exact same value. The choice is purely a practical one based on the format of your labels, not a mathematical one that affects the model's learning.\n",
    "\n",
    "---\n",
    "**References**\n",
    "\n",
    "Chollet, F. (2021). *Deep learning with Python* (2nd ed.). Shelter Island, NY: Manning Publications.\n",
    "\n",
    "Goodfellow, I., Bengio, Y., & Courville, A. (2016). *Deep learning*. Cambridge, MA: MIT Press."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a43b04",
   "metadata": {},
   "source": [
    "# Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8bbc1d6a",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m__future__\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m annotations\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Imports the MNIST dataset from Keras, a classic collection of 70,000 grayscale images of handwritten digits (0-9).\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m mnist \n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Imports TensorFlow, the core open-source library from Google for building and training machine learning models. We use the alias 'tf' by convention.\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtf\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\timvo\\OneDrive\\Documents\\VSC Projects\\CSE5ML\\Assessment 2\\TF_NN_MNIST\\.venv\\Lib\\site-packages\\keras\\__init__.py:7\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[33;03m\"\"\"DO NOT EDIT.\u001b[39;00m\n\u001b[32m      2\u001b[39m \n\u001b[32m      3\u001b[39m \u001b[33;03mThis file was autogenerated. Do not edit it by hand,\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[33;03msince your modifications would be overwritten.\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _tf_keras \u001b[38;5;28;01mas\u001b[39;00m _tf_keras\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m activations \u001b[38;5;28;01mas\u001b[39;00m activations\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m applications \u001b[38;5;28;01mas\u001b[39;00m applications\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\timvo\\OneDrive\\Documents\\VSC Projects\\CSE5ML\\Assessment 2\\TF_NN_MNIST\\.venv\\Lib\\site-packages\\keras\\_tf_keras\\__init__.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_tf_keras\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m keras\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\timvo\\OneDrive\\Documents\\VSC Projects\\CSE5ML\\Assessment 2\\TF_NN_MNIST\\.venv\\Lib\\site-packages\\keras\\_tf_keras\\keras\\__init__.py:7\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[33;03m\"\"\"DO NOT EDIT.\u001b[39;00m\n\u001b[32m      2\u001b[39m \n\u001b[32m      3\u001b[39m \u001b[33;03mThis file was autogenerated. Do not edit it by hand,\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[33;03msince your modifications would be overwritten.\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m activations \u001b[38;5;28;01mas\u001b[39;00m activations\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m applications \u001b[38;5;28;01mas\u001b[39;00m applications\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m callbacks \u001b[38;5;28;01mas\u001b[39;00m callbacks\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\timvo\\OneDrive\\Documents\\VSC Projects\\CSE5ML\\Assessment 2\\TF_NN_MNIST\\.venv\\Lib\\site-packages\\keras\\activations\\__init__.py:7\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[33;03m\"\"\"DO NOT EDIT.\u001b[39;00m\n\u001b[32m      2\u001b[39m \n\u001b[32m      3\u001b[39m \u001b[33;03mThis file was autogenerated. Do not edit it by hand,\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[33;03msince your modifications would be overwritten.\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mactivations\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m deserialize \u001b[38;5;28;01mas\u001b[39;00m deserialize\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mactivations\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get \u001b[38;5;28;01mas\u001b[39;00m get\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mactivations\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m serialize \u001b[38;5;28;01mas\u001b[39;00m serialize\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\timvo\\OneDrive\\Documents\\VSC Projects\\CSE5ML\\Assessment 2\\TF_NN_MNIST\\.venv\\Lib\\site-packages\\keras\\src\\__init__.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m activations\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m applications\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m backend\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\timvo\\OneDrive\\Documents\\VSC Projects\\CSE5ML\\Assessment 2\\TF_NN_MNIST\\.venv\\Lib\\site-packages\\keras\\src\\activations\\__init__.py:3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtypes\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mactivations\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mactivations\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m celu\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mactivations\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mactivations\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m elu\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mactivations\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mactivations\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m exponential\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\timvo\\OneDrive\\Documents\\VSC Projects\\CSE5ML\\Assessment 2\\TF_NN_MNIST\\.venv\\Lib\\site-packages\\keras\\src\\activations\\activations.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m backend\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ops\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi_export\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m keras_export\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\timvo\\OneDrive\\Documents\\VSC Projects\\CSE5ML\\Assessment 2\\TF_NN_MNIST\\.venv\\Lib\\site-packages\\keras\\src\\backend\\__init__.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackend\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m backend\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m backend() == \u001b[33m\"\u001b[39m\u001b[33mtorch\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m      4\u001b[39m     \u001b[38;5;66;03m# When using the torch backend,\u001b[39;00m\n\u001b[32m      5\u001b[39m     \u001b[38;5;66;03m# torch needs to be imported first, otherwise it will segfault\u001b[39;00m\n\u001b[32m      6\u001b[39m     \u001b[38;5;66;03m# upon import.\u001b[39;00m\n\u001b[32m      7\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\timvo\\OneDrive\\Documents\\VSC Projects\\CSE5ML\\Assessment 2\\TF_NN_MNIST\\.venv\\Lib\\site-packages\\keras\\src\\backend\\config.py:448\u001b[39m\n\u001b[32m    445\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    446\u001b[39m         _NNX_ENABLED = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m448\u001b[39m \u001b[43mset_nnx_enabled\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_NNX_ENABLED\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\timvo\\OneDrive\\Documents\\VSC Projects\\CSE5ML\\Assessment 2\\TF_NN_MNIST\\.venv\\Lib\\site-packages\\keras\\src\\backend\\config.py:249\u001b[39m, in \u001b[36mset_nnx_enabled\u001b[39m\u001b[34m(value)\u001b[39m\n\u001b[32m    247\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mset_nnx_enabled\u001b[39m(value):\n\u001b[32m    248\u001b[39m     \u001b[38;5;28;01mglobal\u001b[39;00m _NNX_ENABLED\n\u001b[32m--> \u001b[39m\u001b[32m249\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackend\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcommon\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m global_state\n\u001b[32m    251\u001b[39m     _NNX_ENABLED = \u001b[38;5;28mbool\u001b[39m(value)\n\u001b[32m    252\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _NNX_ENABLED:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\timvo\\OneDrive\\Documents\\VSC Projects\\CSE5ML\\Assessment 2\\TF_NN_MNIST\\.venv\\Lib\\site-packages\\keras\\src\\backend\\common\\__init__.py:2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackend\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcommon\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m backend_utils\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackend\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcommon\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdtypes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m result_type\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackend\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcommon\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mvariables\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutocastScope\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackend\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcommon\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mvariables\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Variable \u001b[38;5;28;01mas\u001b[39;00m KerasVariable\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\timvo\\OneDrive\\Documents\\VSC Projects\\CSE5ML\\Assessment 2\\TF_NN_MNIST\\.venv\\Lib\\site-packages\\keras\\src\\backend\\common\\dtypes.py:5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi_export\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m keras_export\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackend\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackend\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcommon\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mvariables\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m standardize_dtype\n\u001b[32m      7\u001b[39m BOOL_TYPES = (\u001b[33m\"\u001b[39m\u001b[33mbool\u001b[39m\u001b[33m\"\u001b[39m,)\n\u001b[32m      8\u001b[39m INT_TYPES = (\n\u001b[32m      9\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33muint8\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     10\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33muint16\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     16\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mint64\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     17\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\timvo\\OneDrive\\Documents\\VSC Projects\\CSE5ML\\Assessment 2\\TF_NN_MNIST\\.venv\\Lib\\site-packages\\keras\\src\\backend\\common\\variables.py:11\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackend\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcommon\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mstateless_scope\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_stateless_scope\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackend\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcommon\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mstateless_scope\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m in_stateless_scope\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodule_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tensorflow \u001b[38;5;28;01mas\u001b[39;00m tf\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnaming\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m auto_name\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mVariable\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\timvo\\OneDrive\\Documents\\VSC Projects\\CSE5ML\\Assessment 2\\TF_NN_MNIST\\.venv\\Lib\\site-packages\\keras\\src\\utils\\__init__.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01maudio_dataset_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m audio_dataset_from_directory\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdataset_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m split_dataset\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfile_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_file\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\timvo\\OneDrive\\Documents\\VSC Projects\\CSE5ML\\Assessment 2\\TF_NN_MNIST\\.venv\\Lib\\site-packages\\keras\\src\\utils\\audio_dataset_utils.py:4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi_export\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m keras_export\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m dataset_utils\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodule_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tensorflow \u001b[38;5;28;01mas\u001b[39;00m tf\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodule_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tensorflow_io \u001b[38;5;28;01mas\u001b[39;00m tfio\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\timvo\\OneDrive\\Documents\\VSC Projects\\CSE5ML\\Assessment 2\\TF_NN_MNIST\\.venv\\Lib\\site-packages\\keras\\src\\utils\\dataset_utils.py:9\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmultiprocessing\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpool\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ThreadPool\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tree\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi_export\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m keras_export\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m io_utils\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\timvo\\OneDrive\\Documents\\VSC Projects\\CSE5ML\\Assessment 2\\TF_NN_MNIST\\.venv\\Lib\\site-packages\\keras\\src\\tree\\__init__.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtree\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtree_api\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m assert_same_paths\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtree\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtree_api\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m assert_same_structure\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtree\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtree_api\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m flatten\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\timvo\\OneDrive\\Documents\\VSC Projects\\CSE5ML\\Assessment 2\\TF_NN_MNIST\\.venv\\Lib\\site-packages\\keras\\src\\tree\\tree_api.py:8\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodule_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m optree\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m optree.available:\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtree\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m optree_impl \u001b[38;5;28;01mas\u001b[39;00m tree_impl\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m dmtree.available:\n\u001b[32m     10\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtree\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m dmtree_impl \u001b[38;5;28;01mas\u001b[39;00m tree_impl\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\timvo\\OneDrive\\Documents\\VSC Projects\\CSE5ML\\Assessment 2\\TF_NN_MNIST\\.venv\\Lib\\site-packages\\keras\\src\\tree\\optree_impl.py:13\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Register backend-specific node classes\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m backend() == \u001b[33m\"\u001b[39m\u001b[33mtensorflow\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtrackable\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata_structures\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ListWrapper\n\u001b[32m     14\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtrackable\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata_structures\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _DictWrapper\n\u001b[32m     16\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\timvo\\OneDrive\\Documents\\VSC Projects\\CSE5ML\\Assessment 2\\TF_NN_MNIST\\.venv\\Lib\\site-packages\\tensorflow\\__init__.py:49\u001b[39m\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tf2 \u001b[38;5;28;01mas\u001b[39;00m _tf2\n\u001b[32m     47\u001b[39m _tf2.enable()\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_api\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mv2\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __internal__\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_api\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mv2\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __operators__\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_api\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mv2\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m audio\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\timvo\\OneDrive\\Documents\\VSC Projects\\CSE5ML\\Assessment 2\\TF_NN_MNIST\\.venv\\Lib\\site-packages\\tensorflow\\_api\\v2\\__internal__\\__init__.py:11\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_api\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mv2\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m__internal__\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m decorator\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_api\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mv2\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m__internal__\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m dispatch\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_api\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mv2\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m__internal__\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m distribute\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_api\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mv2\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m__internal__\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m eager_context\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_api\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mv2\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m__internal__\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m feature_column\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\timvo\\OneDrive\\Documents\\VSC Projects\\CSE5ML\\Assessment 2\\TF_NN_MNIST\\.venv\\Lib\\site-packages\\tensorflow\\_api\\v2\\__internal__\\distribute\\__init__.py:8\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[33;03m\"\"\"Public API for tf._api.v2.__internal__.distribute namespace\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msys\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m_sys\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_api\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mv2\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m__internal__\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistribute\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m combinations\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_api\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mv2\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m__internal__\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistribute\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m interim\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_api\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mv2\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m__internal__\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistribute\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m multi_process_runner\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\timvo\\OneDrive\\Documents\\VSC Projects\\CSE5ML\\Assessment 2\\TF_NN_MNIST\\.venv\\Lib\\site-packages\\tensorflow\\_api\\v2\\__internal__\\distribute\\combinations\\__init__.py:8\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[33;03m\"\"\"Public API for tf._api.v2.__internal__.distribute.combinations namespace\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msys\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m_sys\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistribute\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcombinations\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m env \u001b[38;5;66;03m# line: 456\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistribute\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcombinations\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m generate \u001b[38;5;66;03m# line: 365\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistribute\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcombinations\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m in_main_process \u001b[38;5;66;03m# line: 418\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\timvo\\OneDrive\\Documents\\VSC Projects\\CSE5ML\\Assessment 2\\TF_NN_MNIST\\.venv\\Lib\\site-packages\\tensorflow\\python\\distribute\\combinations.py:35\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistribute\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m collective_all_reduce_strategy\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistribute\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m distribute_lib\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistribute\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m multi_process_runner\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistribute\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m multi_worker_test_base\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01meager\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m context\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\timvo\\OneDrive\\Documents\\VSC Projects\\CSE5ML\\Assessment 2\\TF_NN_MNIST\\.venv\\Lib\\site-packages\\tensorflow\\python\\distribute\\multi_process_runner.py:35\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcompat\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m v2_compat\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistribute\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m multi_worker_util\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistribute\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m multi_process_lib\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01meager\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m context\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mframework\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m test_util\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\timvo\\OneDrive\\Documents\\VSC Projects\\CSE5ML\\Assessment 2\\TF_NN_MNIST\\.venv\\Lib\\site-packages\\tensorflow\\python\\distribute\\multi_process_lib.py:25\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mabsl\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m app\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mabsl\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m logging\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01meager\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m test\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mis_oss\u001b[39m():\n\u001b[32m     29\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Returns whether the test is run under OSS.\"\"\"\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\timvo\\OneDrive\\Documents\\VSC Projects\\CSE5ML\\Assessment 2\\TF_NN_MNIST\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\test.py:18\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[33;03m\"\"\"Utilities for testing tfe code.\"\"\"\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mframework\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ops \u001b[38;5;28;01mas\u001b[39;00m _ops\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mplatform\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m test \u001b[38;5;28;01mas\u001b[39;00m _test\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mplatform\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtest\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *  \u001b[38;5;66;03m# pylint: disable=wildcard-import\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# TODO(akshayka): Do away with this file.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\timvo\\OneDrive\\Documents\\VSC Projects\\CSE5ML\\Assessment 2\\TF_NN_MNIST\\.venv\\Lib\\site-packages\\tensorflow\\python\\platform\\test.py:23\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01munittest\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m mock\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# pylint: disable=g-bad-import-order\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mframework\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m test_util \u001b[38;5;28;01mas\u001b[39;00m _test_util\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mplatform\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m googletest \u001b[38;5;28;01mas\u001b[39;00m _googletest\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# pylint: disable=unused-import\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\timvo\\OneDrive\\Documents\\VSC Projects\\CSE5ML\\Assessment 2\\TF_NN_MNIST\\.venv\\Lib\\site-packages\\tensorflow\\python\\framework\\test_util.py:49\u001b[39m\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprotobuf\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m meta_graph_pb2\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprotobuf\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m rewriter_config_pb2\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pywrap_sanitizers\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tf2\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mclient\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m device_lib\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\timvo\\OneDrive\\Documents\\VSC Projects\\CSE5ML\\Assessment 2\\TF_NN_MNIST\\.venv\\Lib\\site-packages\\tensorflow\\python\\pywrap_sanitizers.py:19\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# pylint: disable=invalid-import-order,g-bad-import-order, wildcard-import, unused-import\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pywrap_tensorflow\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_pywrap_sanitizers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "# Imports the MNIST dataset from Keras, a classic collection of 70,000 grayscale images of handwritten digits (0-9).\n",
    "from keras.datasets import mnist \n",
    "\n",
    "# Imports TensorFlow, the core open-source library from Google for building and training machine learning models. We use the alias 'tf' by convention.\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import TensorShape \n",
    "\n",
    "from visualkeras import SpacingDummyLayer # Added for better text spacing\n",
    "\n",
    "# Imports the Adam optimiser. An optimiser is an algorithm that adjusts the model's internal parameters (weights) to minimise the error, and Adam is a popular, efficient choice.\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "\n",
    "# Imports specific performance metrics. Metrics are used to evaluate how well the model is performing.\n",
    "# Precision: Measures the accuracy of positive predictions.\n",
    "# Recall: Measures the model's ability to find all the actual positive instances.\n",
    "# Accuracy: Measures the overall fraction of correct predictions.\n",
    "from tensorflow.keras.metrics import Precision, Recall, Accuracy\n",
    "\n",
    "# Imports the History callback object. A 'callback' is a function that can be executed at different stages of training. The History object automatically records the metrics and loss values from each epoch.\n",
    "from tensorflow.python.keras.callbacks import History\n",
    "\n",
    "# Imports the ModelCheckpoint callback. This callback saves the model to a file during training, typically only when its performance on a validation metric improves.\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard, EarlyStopping\n",
    "\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "# Imports the Pandas library, a powerful tool for data manipulation and analysis. It's mainly used for working with structured data in tables called DataFrames. 'pd' is the standard alias.\n",
    "import pandas as pd\n",
    "\n",
    "# Imports the Sequential model type from Keras. This is the simplest way to build a model, by creating a linear stack of layers.\n",
    "from keras.models import Sequential, Model\n",
    "\n",
    "# Imports different types of layers, which are the fundamental building blocks of a neural network.\n",
    "# Dense: A standard, fully-connected layer where each neuron is connected to every neuron in the previous layer.\n",
    "# Input: A special layer used to define the shape and data type of the model's input.\n",
    "# Flatten: A layer that transforms a multi-dimensional input (like a 2D image) into a one-dimensional vector.\n",
    "# Normalization: A preprocessing layer that scales input data to a standard range (e.g., mean of 0, standard deviation of 1), which helps the model train faster. \n",
    "from tensorflow.keras.layers import Dense, Input, Flatten, Normalization, Input, Conv2D, MaxPooling2D, Dropout\n",
    "\n",
    "# Imports a utility function from scikit-learn, a popular library for traditional machine learning. train_test_split is used to split a single dataset into separate training and testing sets.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Imports the pyplot interface from Matplotlib, which is the most widely used library for creating plots and visualisations in Python. 'plt' is the standard alias.\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Imports the NumPy library, which is the foundation for numerical computing in Python. It provides support for large, multi-dimensional arrays and a wide range of mathematical functions. 'np' is the standard alias.\n",
    "import numpy as np\n",
    "\n",
    "# Imports a data scaling tool from scikit-learn. MinMaxScaler scales all data features to a specific range, usually 0 to 1.\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Imports tools for 'type hinting' from Python's typing module. Type hints make code more readable and can be used by external tools to check for errors.\n",
    "# Tuple: Used to hint that a variable or function return is a tuple (an ordered, immutable collection of elements).\n",
    "from typing import Tuple\n",
    "\n",
    "# Imports a specific type hint from NumPy's typing module.\n",
    "# NDArray: Used to hint that a variable is a NumPy n-dimensional array, which is more descriptive than a generic type.\n",
    "from numpy.typing import NDArray\n",
    "\n",
    "# Imports the 'os' module. This library provides a way for Python to interact with the computer's underlying operating system.\n",
    "# We use it for tasks like reading file names from a folder (os.listdir()) and constructing file paths that work correctly on any system, like Windows, Mac, or Linux (os.path.join()).\n",
    "import os \n",
    "\n",
    "# Imports the 're' module, which stands for Regular Expression. This is Python's library for advanced pattern matching in strings.\n",
    "# We use it to find and extract specific pieces of text from a string, like pulling the accuracy score out of a complex filename (e.g., finding '0.9935' in 'model_acc-0.9935.keras').\n",
    "import re\n",
    "\n",
    "# The seaborn dependency is used for plotting confusion matrices. \n",
    "import seaborn as sns\n",
    "\n",
    "# Wandb allows us to log the results of our experiments online at the WandB platform. This platform can be used to visualise plots of our models'\n",
    "# performance (accuracy, loss) per model, or for all models at once. Additionally, this platform keeps track of the models' architecture and hyperparameters used in the trainig process. \n",
    "import wandb\n",
    "\n",
    "# This import allows us to log the metrics generated by our models (Keras)\n",
    "from wandb.integration.keras import WandbMetricsLogger, WandbModelCheckpoint\n",
    "\n",
    "import datetime\n",
    "\n",
    "import visualkeras\n",
    "\n",
    "import warnings\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "from PIL import ImageFont\n",
    "\n",
    "from IPython.display import display # Needed to show plots inside a function\n",
    "\n",
    "import requests\n",
    "\n",
    "import yaml\n",
    "\n",
    "from types import SimpleNamespace\n",
    "\n",
    "import pickle\n",
    "\n",
    "from typing import Callable, Any\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.callbacks import History # Use the public API path\n",
    "from types import SimpleNamespace\n",
    "import pandas as pd\n",
    "from typing import Union, Optional, Dict, Any, List\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978ce86f",
   "metadata": {},
   "source": [
    "# Import and inspect the dataset\n",
    "\n",
    "### Define the training set features (X_train) and target variable (Y_train) as well as the test set features (X_test_) and target variable (Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b32fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MNIST dataset, which is a large database of handwritten digits.\n",
    "# The function returns two tuples: one for training data and one for testing data.\n",
    "# Recalling, a Tuple is a collection of objects that are ordered and immutable.\n",
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
    "\n",
    "# First we convert the data to float32, which helps with numerical stability. A float32 provides sufficient precision, while also being memory efficient.\n",
    "# Most modern CPU's and GPU's are optimized for float32 operations, making computations faster.\n",
    "X_train = X_train.astype(\"float32\")\n",
    "X_test = X_test.astype(\"float32\")\n",
    "\n",
    "# Then we convert the training data to have a channel dimension, which is required for CNNs.\n",
    "X_train = X_train[..., tf.newaxis] # Add the channel dimension\n",
    "X_test = X_test[..., tf.newaxis] # Add the channel dimension\n",
    "print(f\"New shape for X_train for CNN's: {X_train.shape}\")\n",
    "print(f\"New shape for X_test for CNN's: {X_test.shape}\")\n",
    "\n",
    "# Declare the types of the loaded data for clarity.\n",
    "X_train: NDArray[np.float32]\n",
    "Y_train: NDArray[np.uint8]\n",
    "X_test: NDArray[np.float32]\n",
    "Y_test: NDArray[np.uint8]\n",
    "\n",
    "# We set the line width to a large value to avoid line breaks when printing the array.\n",
    "with np.printoptions(linewidth=10000):\n",
    "    # Print the shapes of the datasets to understand their dimensions.\n",
    "    print(\"Shape of X_train:\\t\", X_train.shape)\n",
    "    print(\"Shape of X_test:\\t\", X_test.shape)\n",
    "    print(\"Shape of Y_train:\\t\", Y_train.shape)\n",
    "    print(\"Shape of Y_test:\\t\", Y_test.shape)\n",
    "    print(f\"X_train data type: {X_train.dtype}\")\n",
    "    print(f\"X_test data type: {X_test.dtype}\")\n",
    "    print(f\"Y_train data type: {Y_train.dtype}\")\n",
    "    print(f\"Y_test data type: {Y_test.dtype}\")\n",
    "\n",
    "    # Inspect a single data sample to see what it looks like.\n",
    "    n: int = 567\n",
    "    print(f\"\\nX_train data {n}-th element (a 28x28 pixel image):\\n\", np.squeeze(X_train[n]))\n",
    "    print(\"\\nAnd its corresponding label:\\t\", Y_train[n])\n",
    "\n",
    "    # To do: For using this data in a neural network,\n",
    "    # Tensorflow/Keras expects the input data to be in a 1D or 2D array format where each row represents a single sample and each column represents a feature. The general format for the input shape is: (batch_size, feature_1, feature_2, ...)\n",
    "    # However, we can use the tf.keras.layers.Flatten layer as the first layer in our sequential model.\n",
    "    # This layer automatically flattens the input shape without the need for manual reshaping of our data.\n",
    "    # For a Dense (fully connected) network: We must flatten each 28x28 image into a single 1D array of 784 pixels. The input shape for the first layer of our model would then be (None, 784), where None represents a variable batch size.\n",
    "    # For a Convolutional Neural Network (CNN): We must add a channel dimension. Since the images are grayscale, there is only one channel. We would reshape the data to (number_of_images, 28, 28, 1). The input shape for the first layer (typically a Conv2D layer) would be (28, 28, 1). The batch size is handled automatically by Keras.\n",
    "    # Scaling can also be performd in the model using a tf.keras.layers.Rescaling or keras.layers.Normalization layer as the first layer in our sequential model.\n",
    "    # The advantage of using these layers is that they integrate seamlessly into the model architecture, ensuring that the data is preprocessed consistently during both training and inference.\n",
    "    # This approach also simplifies the code by reducing the need for separate preprocessing steps outside the model definition.\n",
    "    # And, it ensures that inference data is processed in the same way as training data, which is crucial for maintaining model performance.\n",
    "\n",
    "    # Analyze the distribution of the digits in the training set.\n",
    "    # `np.unique` finds the unique digit labels and `return_counts=True` counts their occurrences.\n",
    "    dataset_train_distribution: Tuple[np.ndarray, np.ndarray] = np.unique(Y_train, return_counts=True)\n",
    "    digits_train: np.ndarray = dataset_train_distribution[0]\n",
    "    counts_train: np.ndarray = dataset_train_distribution[1]\n",
    "    \n",
    "    print(\"\\n--- Train Dataset Distribution ---\")\n",
    "    print(\"Digits:\\t\\t\\t\", digits_train)\n",
    "    print(\"Count per digit:\\t\", counts_train)\n",
    "    \n",
    "    # Calculate basic statistics on the distribution.\n",
    "    avg: float = np.mean(counts_train)\n",
    "    print(f\"Average sample size:\\t {avg:.2f}\")\n",
    "    \n",
    "    max_count_train: np.int64 = np.max(counts_train)\n",
    "    min_count_train: np.int64 = np.min(counts_train)\n",
    "    print(f\"Maximum sample size:\\t {max_count_train}\")\n",
    "    print(f\"Minimum sample size:\\t {min_count_train}\")\n",
    "\n",
    "\n",
    "    dataset_test_distribution: Tuple[np.ndarray, np.ndarray] = np.unique(Y_test, return_counts=True)\n",
    "    digits_test: np.ndarray = dataset_test_distribution[0]\n",
    "    counts_test: np.ndarray = dataset_test_distribution[1]\n",
    "    \n",
    "    print(\"\\n--- Test Dataset Distribution ---\")\n",
    "    print(\"Digits:\\t\\t\\t\", digits_test)\n",
    "    print(\"Count per digit:\\t\", counts_test)\n",
    "    \n",
    "    # Calculate basic statistics on the distribution.\n",
    "    avg: float = np.mean(counts_test)\n",
    "    print(f\"Average sample size:\\t {avg:.2f}\")\n",
    "    \n",
    "    max_count_test: np.int64 = np.max(counts_test)\n",
    "    min_count_test: np.int64 = np.min(counts_test)\n",
    "    print(f\"Maximum sample size:\\t {max_count_test}\")\n",
    "    print(f\"Minimum sample size:\\t {min_count_test}\")\n",
    "\n",
    "# Create a bar chart from the counts and digits to visualize the distribution.\n",
    "plt.bar(digits_train, counts_train, color='blue', edgecolor='black')\n",
    "\n",
    "# Set the title and labels for clarity.\n",
    "plt.title('Count of Each Digit in Training Set')\n",
    "plt.xlabel('Digits')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "# Set x-ticks to be at the center of each bar and label them with the digit.\n",
    "# Minor ticks are used here to place the labels directly under the bars.\n",
    "# Ticks specify the positions on the x-axis where the labels should be placed.\n",
    "# By setting ticks=digits_train, we ensure that each digit (0-9) is labeled correctly under its corresponding bar.\n",
    "plt.xticks(ticks=digits_train, minor=True, labels=digits_train)\n",
    "\n",
    "# Add a grid for better readability.\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Display the plot.\n",
    "plt.show()\n",
    "\n",
    "# And the same bar chart for the test set.\n",
    "\n",
    "# Create a bar chart from the counts and digits to visualize the distribution.\n",
    "plt.bar(digits_test, counts_test, color='red', edgecolor='black')\n",
    "\n",
    "# Set the title and labels for clarity.\n",
    "plt.title('Count of Each Digit in Test Set')\n",
    "plt.xlabel('Digits')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "# Set x-ticks to be at the center of each bar and label them with the digit.\n",
    "plt.xticks(ticks=digits_test, minor=True, labels=digits_test)\n",
    "\n",
    "# Add a grid for better readability.\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Display the plot.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899d4acf",
   "metadata": {},
   "source": [
    "## Dataset Analysis\n",
    "\n",
    "The content and size of the training and testing datasets align with the description on the Kaggle MNIST dataset page, Hojjat, F. (2017). MNIST: The Most Famous Dataset in the World. Kaggle. Retrieved August 28, 2025, from https://www.kaggle.com/datasets/hojjatk/mnist-dataset. The plot of digit distribution shows a fairly homogeneous representation across all classes (digits 0 through 9). While the digit '1' is slightly oversampled and the digit '5' is slightly undersampled, the class imbalance is not significant enough to warrant further action for this assessment.\n",
    "\n",
    "In a scenario where the distribution were to be significantly imbalanced and we needed to make it more homogeneous, we would use a technique called **resampling**. Resampling involves adjusting the distribution of the training data to be more balanced. There are two primary types:\n",
    "\n",
    "- **Oversampling** involves duplicating samples from the underrepresented classes to increase their frequency.\n",
    "\n",
    "- **Undersampling** involves removing samples from the overrepresented classes to reduce their frequency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5568184",
   "metadata": {},
   "source": [
    "## Part 1, Task 1: Creating a simple Multilayer Perceptron (MLP) neural network\n",
    "\n",
    "The code below defines our base model.\n",
    "\n",
    "To experiment with different architectures or tune its hyperparameters, we simply copy this entire cell and make our changes.\n",
    "\n",
    "We need to make sure to give each new model a unique name. This ensures that when the ModelCheckpoint callback saves the best-performing version during training, the filename will be clear and identifiable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b401b847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Set Seeds for Reproducibility ---\n",
    "\n",
    "# This sets the global random seed for all TensorFlow operations.\n",
    "# It ensures that things like model weight initialisation are the same every time.\n",
    "# `tf.random.set_seed()` is the modern way to do this in TensorFlow 2.\n",
    "tf.random.set_seed(1)\n",
    "\n",
    "# This sets the random seed for all NumPy operations.\n",
    "# This is important if we are creating our data using NumPy or using any\n",
    "# NumPy functions that involve randomness.\n",
    "np.random.seed(23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765eac04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Task 1**\n",
    "\n",
    "# Build a neural network without convolutional layers to do the classification task (hint: you will need the use of dense layers). \n",
    "# Then you can change the model structure (i.e. number of dense layers, number of neurons in dense layers or activation functions) to be able to improve network performance.\n",
    "\n",
    "def create_mlp_model_base() -> Sequential:\n",
    "    \"\"\"\n",
    "    Defines and returns the (base) MLP model architecture.\n",
    "    \"\"\"\n",
    "    # Note: we can change the model architecture here. However, it is more prudent to save the model parameters first, and then change it. \n",
    "    model = Sequential([\n",
    "        # We use the implicit input_shape here for a cleaner look.\n",
    "        Input(shape=(28, 28, 1)),\n",
    "        Normalization(),\n",
    "        Flatten(),\n",
    "        Dense(units=32, activation='relu'), # Relu is the goto activation function. We could also use LeakyRelu, tanh, sigmoid, etc.\n",
    "        Dense(units=64, activation='relu'),\n",
    "        Dense(units=32, activation='relu'),\n",
    "        Dense(units=10, activation='softmax')\n",
    "    ], name = \"Base_MLP_Model\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1216af8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mlp_model_2() -> Sequential:\n",
    "    \"\"\"\n",
    "    Defines and returns the (base) MLP model architecture.\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        # We use the implicit input_shape here for a cleaner look.\n",
    "        Input(shape=(28, 28, 1)),\n",
    "        Normalization(),\n",
    "        Flatten(),\n",
    "        Dense(units=64, activation='relu'), # Relu is the goto activation function. We could also use LeakyRelu, tanh, sigmoid, etc.\n",
    "        Dense(units=128, activation='relu'),\n",
    "        Dense(units=64, activation='relu'),\n",
    "        Dense(units=10, activation='softmax')\n",
    "    ], name = \"Base_MLP_Model_2\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361f3d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is our first \"base\" model we created. Repeated here, because it yielded very good accuracies on both validate and test data, as well as loss. \n",
    "def create_mlp_model_3() -> Sequential:\n",
    "    \"\"\"\n",
    "    Defines and returns the (base) MLP model architecture.\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        # We use the implicit input_shape here for a cleaner look.\n",
    "        Input(shape=(28, 28, 1)),\n",
    "        Normalization(),\n",
    "        Flatten(),\n",
    "        Dense(units=128, activation='relu'), # Relu is the goto activation function. We could also use LeakyRelu, \n",
    "        Dense(units=256, activation='relu'),\n",
    "        Dense(units=64, activation='relu'),\n",
    "        Dense(units=10, activation='softmax')\n",
    "    ], name = \"Base_MLP_Model_3\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954def66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mlp_model_wide() -> Sequential:\n",
    "    \"\"\"\n",
    "    Defines and returns the (base) MLP model architecture.\n",
    "    \"\"\"\n",
    "    # Note: we can change the model architecture here. However, it is more prudent to save the model parameters first, and then change it. \n",
    "    model = Sequential([\n",
    "        # We use the implicit input_shape here for a cleaner look.\n",
    "        Input(shape=(28, 28, 1)),\n",
    "        Normalization(),\n",
    "        Flatten(),\n",
    "        Dense(units=512, activation='relu'),\n",
    "        Dense(units=1024, activation='relu'),\n",
    "        Dense(units=256, activation='relu'),\n",
    "        Dense(units=10, activation='softmax')\n",
    "    ], name = \"Wide_MLP_Model\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6283beed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mlp_model_deep() -> Sequential:\n",
    "    \"\"\"\n",
    "    Defines and returns the (base) MLP model architecture.\n",
    "    \"\"\"\n",
    "    # Note: we can change the model architecture here. However, it is more prudent to save the model parameters first, and then change it. \n",
    "    model = Sequential([\n",
    "        # We use the implicit input_shape here for a cleaner look.\n",
    "        Input(shape=(28, 28, 1)),\n",
    "        Normalization(),\n",
    "        Flatten(),\n",
    "        Dense(units=128, activation='relu'),\n",
    "        Dense(units=256, activation='relu'),\n",
    "        Dense(units=256, activation='relu'),\n",
    "        Dense(units=256, activation='relu'),\n",
    "        Dense(units=256, activation='relu'),\n",
    "        Dense(units=64, activation='relu'),\n",
    "        Dense(units=32, activation='relu'),\n",
    "        Dense(units=10, activation='softmax')\n",
    "    ], name=\"Deep_MLP_Model\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a924894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All MLP models:\n",
    "mlp_model_functions: List[Callable] = [\n",
    "    create_mlp_model_base,\n",
    "    # create_mlp_model_2,\n",
    "    # create_mlp_model_3,\n",
    "    create_mlp_model_wide,\n",
    "    create_mlp_model_deep,\n",
    "]\n",
    "\n",
    "# # Only a single model:\n",
    "# mlp_model_functions = [\n",
    "#     create_mlp_model_3,\n",
    "# ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc8c4cf",
   "metadata": {},
   "source": [
    "### Define the training loop\n",
    "\n",
    "The 'run_experiment' function is our main pipeline for training a neural network.\n",
    "It encapsulates all the necessary steps: creating the model, setting hyperparameters,\n",
    "configuring callbacks for logging and monitoring, and finally training the model.\n",
    "This makes our code highly modular and reusable, allowing us to easily run different\n",
    "experiments by simply changing the input parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025ee811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The 'run_experiment' function is our main pipeline for training a neural network.\n",
    "# It encapsulates all the necessary steps: creating the model, setting hyperparameters,\n",
    "# configuring callbacks for logging and monitoring, and finally training the model.\n",
    "# This makes our code highly modular and reusable, allowing us to easily run different\n",
    "# experiments by simply changing the input parameters.\n",
    "\n",
    "\n",
    "def run_experiment(\n",
    "    model_creation_func: Callable[[], Model],\n",
    "    hyperparameters: dict[str, int | float | str],\n",
    "    parent_folder: str,\n",
    "    X_train: np.ndarray | tf.Tensor,\n",
    "    Y_train: np.ndarray | tf.Tensor\n",
    ") -> History:\n",
    "    \"\"\"\n",
    "    Runs a full training experiment for a given model architecture and hyperparameter set.\n",
    "\n",
    "    Args:\n",
    "        model_creation_func: A callable (function) that builds and returns a compiled Keras Model.\n",
    "        hyperparameters: Dictionary containing experiment settings (learning_rate, batch_size, optimiser, epochs, etc).\n",
    "        parent_folder: Path to the directory where models and results will be saved.\n",
    "        X_train: Training input data (features) as a NumPy array or TensorFlow tensor.\n",
    "        Y_train: Training labels (targets) as a NumPy array or TensorFlow tensor.\n",
    "\n",
    "    Returns:\n",
    "        History: A Keras History object with details of the training process.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create the model by calling the function provided\n",
    "    model: Model = model_creation_func()\n",
    "\n",
    "    # Set some default hyperparameter values (these will be used if not specified in the dictionary)\n",
    "    default_lr: float = 0.001\n",
    "    default_optimiser: str = \"adam\"\n",
    "    default_batch_size: int = 64\n",
    "    default_epochs: int = 10\n",
    "\n",
    "    # Give the model a name if it doesnt have one already\n",
    "    if model.name is None or model.name == \"\":\n",
    "        model.name = model_creation_func.__name__\n",
    "\n",
    "    # Many models begin with a \"normalisation\" layer.\n",
    "    # Here we adapt (fit) that layer to the training data so it knows the datas mean and variance.\n",
    "    print(\"\\nAdapting the normalisation layer...\")\n",
    "    model.layers[0].adapt(X_train)  # type: ignore[attr-defined]\n",
    "    print(\"Adaptation complete.\\n\")\n",
    "\n",
    "    # Display a summary of what is about to happen\n",
    "    print(f\"\\n--- Starting Experiment: {model.name} ---\")\n",
    "    print(\"\\n--- Model Architecture ---\")\n",
    "    model.summary()\n",
    "    print(\"\\n--- Hyperparameters ---\")\n",
    "    for key, value in hyperparameters.items():\n",
    "        print(f\"{key:<20}: {value}\")\n",
    "\n",
    "    # Generate a unique run name so we can distinguish between different experiments.\n",
    "    run_name: str = (\n",
    "        f\"{model.name}-lr_{hyperparameters.get('learning_rate', default_lr)}\"\n",
    "        f\"-bs_{hyperparameters.get('batch_size', default_batch_size)}-\"\n",
    "        f\"{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    )\n",
    "\n",
    "    # Start a new run in Weights & Biases (wandb) for logging metrics and results.\n",
    "    run: Any = wandb.init(\n",
    "        project=\"CSE5ML-Assessment2\",\n",
    "        name=run_name,\n",
    "        config=hyperparameters,\n",
    "    )\n",
    "\n",
    "    # This callback sends training metrics (loss, accuracy, etc.) to wandb in real time.\n",
    "    wandb_metrics_logger: Callback = WandbMetricsLogger()\n",
    "\n",
    "    # Choose the optimiser and learning rate based on the hyperparameters provided.\n",
    "    optimiser_name: str = hyperparameters.get(\"optimiser\", default_optimiser).lower()  # type: ignore[union-attr]\n",
    "    learning_rate: float = float(hyperparameters.get(\"learning_rate\", default_lr))\n",
    "\n",
    "    if optimiser_name == \"adam\":\n",
    "        optimiser: Adam | SGD | str = Adam(learning_rate=learning_rate)\n",
    "    elif optimiser_name == \"sgd\":\n",
    "        optimiser = SGD(learning_rate=learning_rate)\n",
    "    else:\n",
    "        # If the user provided a string other than \"adam\" or \"sgd\",\n",
    "        # we just pass that string directly (not recommended, but allowed).\n",
    "        optimiser = optimiser_name  # type: ignore[assignment]\n",
    "\n",
    "    # Create a folder to save this models results on disk.\n",
    "    model_specific_folder: str = os.path.join(parent_folder, run_name)\n",
    "    os.makedirs(model_specific_folder, exist_ok=True)\n",
    "\n",
    "    # Define a filename pattern for saving the best models during training.\n",
    "    filepath: str = os.path.join(\n",
    "        model_specific_folder,\n",
    "        \"best_model_epoch-{epoch:02d}_val_acc-{val_accuracy:.4f}.keras\"\n",
    "    )\n",
    "\n",
    "    # This callback saves the best model (on the local hard drive).\n",
    "    checkpoint: ModelCheckpoint = ModelCheckpoint(\n",
    "        filepath=filepath,\n",
    "        monitor=\"val_accuracy\",   # we monitor validation accuracy\n",
    "        mode=\"max\",               # we want the maximum value\n",
    "        save_best_only=True,      # only save when performance improves\n",
    "        verbose=1,\n",
    "    )\n",
    "\n",
    "    # This callback saves the best model (to wandbs cloud storage).\n",
    "    wandb_checkpoint: WandbModelCheckpoint = WandbModelCheckpoint(\n",
    "        filepath=f\"wandb_models/{run_name}/best_model.keras\",\n",
    "        monitor=\"val_accuracy\",\n",
    "        mode=\"max\",\n",
    "        save_best_only=True,\n",
    "        verbose=1,\n",
    "    )\n",
    "\n",
    "    # This callback stops training early if validation accuracy does not improve for a while.\n",
    "    early_stopping: EarlyStopping = EarlyStopping(\n",
    "        monitor=\"val_accuracy\",\n",
    "        patience=10,              # number of epochs to wait\n",
    "        restore_best_weights=True # restore the best model when stopping\n",
    "    )\n",
    "\n",
    "    # Now we compile the model, telling it what optimiser, loss function, and metrics to use.\n",
    "    model.compile(\n",
    "        optimizer=optimiser,\n",
    "        loss=hyperparameters.get(\"loss_function\", \"sparse_categorical_crossentropy\"),  # type: ignore[arg-type]\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "\n",
    "    # Train the model on the provided data.\n",
    "    # We use 10% of the training set as a validation set.\n",
    "    history: History = model.fit(\n",
    "        X_train,\n",
    "        Y_train,\n",
    "        epochs=int(hyperparameters.get(\"epochs\", default_epochs)),\n",
    "        batch_size=int(hyperparameters.get(\"batch_size\", default_batch_size)),\n",
    "        validation_split=0.1,\n",
    "        callbacks=[checkpoint, wandb_metrics_logger, wandb_checkpoint, early_stopping],\n",
    "        verbose=1,\n",
    "    )\n",
    "\n",
    "    # Save the training history (loss/accuracy curves) to disk for later analysis.\n",
    "    history.hyperparameters = hyperparameters  # type: ignore[attr-defined]\n",
    "    history_filepath: str = os.path.join(model_specific_folder, \"training_history.pkl\")\n",
    "    with open(history_filepath, \"wb\") as f:\n",
    "        pickle.dump(history.history, f)\n",
    "    print(f\"\\nTraining history saved to: {history_filepath}\")\n",
    "\n",
    "    # Summarise peak performance at the end of training.\n",
    "    val_accuracies: list[float] = history.history[\"val_accuracy\"]\n",
    "    best_validation_accuracy: float = max(val_accuracies)\n",
    "    best_epoch: int = val_accuracies.index(best_validation_accuracy) + 1\n",
    "    associated_train_acc: float = history.history[\"accuracy\"][best_epoch - 1]\n",
    "\n",
    "    print(\"\\n--- Peak Performance Summary ---\")\n",
    "    print(f\"{'Best validation accuracy:':<35} {best_validation_accuracy:.4f}\")\n",
    "    print(f\"{'Associated training accuracy:':<35} {associated_train_acc:.4f}\")\n",
    "    print(f\"{'Occurred at epoch:':<35} {best_epoch}\")\n",
    "\n",
    "    # Close the wandb run so it is properly logged.\n",
    "    run.finish()\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c83321",
   "metadata": {},
   "source": [
    "### Define sets of hyperparameters\n",
    "\n",
    "The cell below allow us to set a plethora of hyperparameters, such as the learning rate, batch_size, but also the optimiser and the number of epochs the training loop is run for. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8d05a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define Hyperparameter Sets ---\n",
    "# Part 1 Task 3 only requires one experiment with an MLP, so we will just define one hyperparameter set for the MLP here.\n",
    "mlp_epochs: int = 100\n",
    "mlp_batch_size: int = 64\n",
    "# Experiment 1: Our baseline run\n",
    "# We choose SGD as our optimiser because it is expected that the loss landscape of our simple MLP model is largely convex. \n",
    "mlp_exp_1_config: Dict[str, Union[str, float, int]] = {\n",
    "    \"optimiser\": \"Adam\",\n",
    "    \"learning_rate\": 0.001, # The optimal, or at least a satisfactory LR will have to be identified by experimenting. We start with LR = 0.01. This does not lead to convergence of Loss and Accuracy. So LR=0.001 is used. \n",
    "    \"epochs\": mlp_epochs,\n",
    "    \"batch_size\": mlp_batch_size\n",
    "}\n",
    "\n",
    "\n",
    "# Experiment 2: Same as the MLP_Baseline but with a lower learning rate\n",
    "mlp_exp_2_config = {\n",
    "    \"optimiser\": \"Adam\",\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"epochs\": mlp_epochs,\n",
    "    \"batch_size\": mlp_batch_size,\n",
    "}\n",
    "\n",
    "# Experiment 3: Same as the MLP_Baseline but with a different optimizer (SGD)\n",
    "mlp_exp_3_config = {\n",
    "    \"optimiser\": \"SGD\",\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"epochs\": mlp_epochs,\n",
    "    \"batch_size\": mlp_batch_size,\n",
    "}\n",
    "\n",
    "# Experiment 4: Same as the MLP_Baseline but with a different optimizer (SGD) and a lower learning rate\n",
    "mlp_exp_4_config = {\n",
    "    \"optimiser\": \"SGD\",\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"epochs\": mlp_epochs,\n",
    "    \"batch_size\": mlp_batch_size,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67af808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Run the full list of models ---\n",
    "run_mlp_experiments: bool = True\n",
    "\n",
    "if run_mlp_experiments:\n",
    "    mlp_histories: List[History] = []\n",
    "    for mlp_model_function in mlp_model_functions:\n",
    "        mlp_history: History = run_experiment(\n",
    "            model_creation_func=mlp_model_function, \n",
    "            # But use only one set of hyperparameters for now.\n",
    "            hyperparameters=mlp_exp_1_config, \n",
    "            parent_folder='MLP_Models',\n",
    "            X_train=X_train,\n",
    "            Y_train=Y_train,\n",
    "        )\n",
    "        mlp_histories.append(mlp_history)\n",
    "\n",
    "# # --- To run the second experiment, we just call it again with a different config (hyper parameter set) :-)\n",
    "# --- We first  test different model architectures before running more experiments with different hyperparameters (Part 1 Task 1 & 2). ---\n",
    "# --- THen we test the best performing model of Part 1 Task 1& 2 with different hyperparameters. ---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba7b25e",
   "metadata": {},
   "source": [
    "## Part 1, Task 2: Creating a simple Convolutional Neural Network (CNN)\n",
    "\n",
    "The code below defines our base model.\n",
    "\n",
    "To experiment with different architectures or tune its hyperparameters, we simply copy this entire cell and make our changes.\n",
    "\n",
    "We need to make sure to give each new model a unique name. This ensures that when the ModelCheckpoint callback saves the best-performing version during training, the filename will be clear and identifiable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24564b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Task 2**\n",
    "\n",
    "# Build a neural network with the use of convolutional layers (you can decide other layer types you want to include in your network). \n",
    "# Then you can change the number of convolutional layers and the number of filters or activation functions in the convolutional layers to be able to improve network performance.\n",
    "\n",
    "def create_cnn_model_base() -> Sequential:\n",
    "    \"\"\"\n",
    "    Defines the base CNN model architecture with Dropout for regularization.\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        # Preprocessing layers\n",
    "        Input(shape=(28, 28, 1)),\n",
    "        Normalization(),\n",
    "        \n",
    "        # --- Convolutional Block 1 ---\n",
    "        Conv2D(filters=32, kernel_size=(3, 3), activation='relu'),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "        # --- Convolutional Block 2 ---\n",
    "        Conv2D(filters=64, kernel_size=(3, 3), activation='relu'),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        \n",
    "        # --- Classification Head ---\n",
    "        Flatten(),\n",
    "        # Dropout(0.5),\n",
    "        Dense(units=128, activation='relu'),\n",
    "        Dense(units=10, activation='softmax')\n",
    "    ], name=\"Base_CNN\")\n",
    "    return model    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5954ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a second CNN base model with the same architecture, but a dropout in the classification head.\n",
    "def create_cnn_model_base_dropout() -> Sequential:\n",
    "    \"\"\"\n",
    "    Defines and returns the base CNN model architecture with dropout in the classification head.\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        # Preprocessing layers\n",
    "        Input(shape=(28, 28, 1)),\n",
    "        Normalization(),\n",
    "            \n",
    "        # --- Convolutional Block 1 ---\n",
    "        Conv2D(filters=32, kernel_size=(3, 3), activation='relu'),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "        # --- Convolutional Block 2 ---\n",
    "        Conv2D(filters=64, kernel_size=(3, 3), activation='relu'),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        \n",
    "        # --- Classification Head ---\n",
    "        Flatten(),\n",
    "        Dropout(0.5),\n",
    "        Dense(units=128, activation='relu'),\n",
    "        Dense(units=10, activation='softmax')\n",
    "    ], name=\"CNN_with_Dropout\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1879ccb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a wide CNN \n",
    "def create_cnn_model_wide() -> Sequential:\n",
    "    \"\"\"\n",
    "    Defines and returns a wide CNN model architecture.\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        # Preprocessing layers\n",
    "        Input(shape=(28, 28, 1)),\n",
    "        Normalization(),\n",
    "        \n",
    "        # --- Convolutional Block 1 ---\n",
    "        Conv2D(filters=64, kernel_size=(3, 3), activation='relu'),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "        # --- Convolutional Block 2 ---\n",
    "        Conv2D(filters=128, kernel_size=(3, 3), activation='relu'),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        \n",
    "        # --- Classification Head ---\n",
    "        Flatten(),\n",
    "        Dropout(0.5),\n",
    "        Dense(units=256, activation='relu'),\n",
    "        Dense(units=10, activation='softmax')\n",
    "    ], name=\"Wide_CNN_Model\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec63191a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn_model_deep() -> Sequential:\n",
    "    \"\"\"\n",
    "    Defines and returns a deep CNN model architecture with padding to preserve dimensions.\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        Input(shape=(28, 28, 1)),\n",
    "        Normalization(),\n",
    "        \n",
    "        # --- Convolutional Block 1 ---\n",
    "        # Add padding='same' to all Conv2D layers, so we preserve the spatial dimensions. If we don't do this, the image shrinks too quickly.\n",
    "        # and no meaningful features can be extracted.\n",
    "        Conv2D(filters=32, kernel_size=(3, 3), activation='relu', padding='same'),\n",
    "        Conv2D(filters=32, kernel_size=(3, 3), activation='relu', padding='same'),\n",
    "        Conv2D(filters=32, kernel_size=(3, 3), activation='relu', padding='same'),\n",
    "        MaxPooling2D(pool_size=(2, 2)), # This is where the shrinking now happens (28x28 -> 14x14)\n",
    "\n",
    "        # --- Convolutional Block 2 ---\n",
    "        Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same'),\n",
    "        Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same'),\n",
    "        Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same'),\n",
    "        MaxPooling2D(pool_size=(2, 2)), # Second shrink (14x14 -> 7x7)\n",
    "        \n",
    "        # --- Classification Head ---\n",
    "        # The input to Flatten is now a healthy 7x7x64=3,136 full of rich features.\n",
    "        Flatten(),\n",
    "        Dropout(0.5),\n",
    "        Dense(units=128, activation='relu'),\n",
    "        Dense(units=10, activation='softmax')\n",
    "    ], name=\"Deep_CNN_Model_Padded\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d18c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model_functions: List[Callable] = [\n",
    "    create_cnn_model_base,\n",
    "    create_cnn_model_base_dropout,\n",
    "    create_cnn_model_wide,\n",
    "    # create_cnn_model_deep, # We teain the deep network separately with a much lower LR. We have observed that this model performs no better than a random guess (i.e. 10% val. accuracy)\n",
    "    # Almost certainly this has to do with the LR, the model can't get to the valleys of the loss landscape, as it overshoots. \n",
    "    # Future to do: Find a way to reduce the loss landscape to 3D, or 2D dimension, and show the parth the Optimiser takes, to be able to demonstrate the effect of the LR. \n",
    "]\n",
    "\n",
    "\n",
    "# cnn_model_functions = [\n",
    "#     create_cnn_model_deep,\n",
    "# ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a722801",
   "metadata": {},
   "source": [
    "# Plotting all model architectures\n",
    "\n",
    "After creating the model architecture, we now plot the architectures graphically. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c7eefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The 'text_callable' function is a special callback used by the visualkeras library. \n",
    "# Its main job is to create the text label that appears on top of or next to each\n",
    "# layer block in the generated visualisation. It's called once for every layer in your model.\n",
    "\n",
    "def text_callable(layer_index: int, layer: tf.keras.layers.Layer) -> Tuple[str, bool]:\n",
    "    # 'layer_index' is an integer representing the position of the layer in the model's sequence.\n",
    "    # 'layer' is the actual Keras layer object itself (e.g., a Conv2D, Dense, or Flatten layer).\n",
    "\n",
    "    # The boolean value 'above' is used to alternate the vertical position of the text labels.\n",
    "    # By using the modulo operator (%), we check if the layer's index is odd or even.\n",
    "    # This ensures that the text labels don't overlap, making the visualisation cleaner.\n",
    "    above = bool(layer_index % 2)\n",
    "\n",
    "    # We use a 'try...except' block as a defensive programming practice.\n",
    "    # This helps the code run without crashing even if it encounters unexpected errors.\n",
    "    try:\n",
    "        # We first try to get the layer's output shape using the 'layer.output.shape' attribute.\n",
    "        # This is the standard way to get shape information in modern TensorFlow.\n",
    "        shape = layer.output.shape\n",
    "    except AttributeError:\n",
    "        # If the first attempt fails (e.g., if the layer is not yet built), we move to this block.\n",
    "        # This is a fallback to ensure compatibility with different Keras versions and layer types.\n",
    "        try:\n",
    "            # Here, we try to access the 'layer.output_shape' attribute, which was more common\n",
    "            # in older versions of Keras. This helps the function work with a wider range of models.\n",
    "            shape = layer.output_shape\n",
    "        except AttributeError:\n",
    "            # If both attempts to get the shape fail, it means the layer doesn't have a defined output shape.\n",
    "            # This can happen for a number of reasons. In this case, we return a label\n",
    "            # that says \"Shape N/A\" to indicate the issue without crashing the program.\n",
    "            return f\"{layer.name}\\n(Shape N/A)\", above\n",
    "\n",
    "    # Keras represents shapes in various ways. For instance, a single-output layer might\n",
    "    # return a shape as a list, a tuple, or a special 'TensorShape' object.\n",
    "    # This 'if' statement handles the case where the shape is returned as a nested list,\n",
    "    # ensuring we only work with the first (and typically most relevant) shape.\n",
    "    if isinstance(shape, list):\n",
    "        shape = shape[0]\n",
    "\n",
    "    # 'TensorShape' is a specific object type in TensorFlow. To manipulate its values,\n",
    "    # we need to convert it into a standard Python list. The 'as_list()' method does this for us.\n",
    "    if isinstance(shape, tf.TensorShape):\n",
    "        output_shape = shape.as_list()\n",
    "    else:\n",
    "        # If the shape is already a list or tuple, we simply convert it to a list.\n",
    "        output_shape = list(shape)\n",
    "    \n",
    "    # Keras models often have a 'None' dimension in their shape to represent the\n",
    "    # variable batch size. For our visualisation, we don't need this, as it would just\n",
    "    # clutter the output. This line uses a list comprehension to filter out the 'None' value.\n",
    "    output_shape = [dim for dim in output_shape if dim is not None]\n",
    "    \n",
    "    # We add an offset to the text position here.\n",
    "    # The offset will push the text away from the layers.\n",
    "    # This simulates padding around the text.\n",
    "    # \n",
    "    # Please note that the actual padding is controlled by the 'padding' and 'spacing'\n",
    "    # parameters in the 'visualkeras.layered_view()' function, but a clever use of\n",
    "    # this function can also help with text positioning.\n",
    "\n",
    "    # We initialize an empty string to build our final text label.\n",
    "    output_shape_txt = \"\"\n",
    "\n",
    "    # 'enumerate' is a handy built-in Python function. It allows us to loop through a sequence\n",
    "    # (in this case, our list of shape dimensions) and get both the index ('ii') and the\n",
    "    # value ('dim') at the same time. This is perfect for when we need to perform an action\n",
    "    # based on an item's position, like adding 'x' or a newline.\n",
    "    for ii, dim in enumerate(output_shape):\n",
    "        # We append the current dimension (as a string) to our text label.\n",
    "        output_shape_txt += str(dim)\n",
    "        \n",
    "        # This line is for debugging purposes. It prints the shape to the console\n",
    "        # to help us verify what the function is seeing. We can remove this later.\n",
    "        # print(f'Output shape: {output_shape}')\n",
    "\n",
    "        # We want to add an 'x' between the dimensions (e.g., 28x28). This 'if' statement\n",
    "        # checks if we are not at the last two dimensions of the shape, as the format is different there.\n",
    "        if ii < len(output_shape) - 2:\n",
    "            output_shape_txt += \"x\"\n",
    "\n",
    "        # This 'if' statement adds a newline character to create the desired format\n",
    "        # where the last dimension is on a new line (e.g., 28x28\\n64).\n",
    "        if ii == len(output_shape) - 2:\n",
    "            output_shape_txt += \"\\n\" #\\n here is the space between the output shape and nr of filteres (e.g. 28 x 28, 32) and the keras name for the layer. \n",
    "\n",
    "\n",
    "    # --- General Check for Attributes ---\n",
    "    # We check if the layer has an 'activation' attribute.\n",
    "    if hasattr(layer, 'activation') and layer.activation is not None:\n",
    "        activation = layer.activation.__name__\n",
    "        output_shape_txt += f\"\\n{activation}\"\n",
    "    \n",
    "    # We check if the layer has a 'kernel_size' attribute.\n",
    "    if hasattr(layer, 'kernel_size') and layer.kernel_size is not None:\n",
    "        kernel_size = layer.kernel_size\n",
    "        output_shape_txt += f\"\\nKernel: {kernel_size}\"\n",
    "\n",
    "    # Finally, we add the name of the layer to our text, preceded by a newline, to ensure\n",
    "    # the layer name appears on a new line below the shape.\n",
    "    # output_shape_txt += f\"\\n{layer.name}\" #\\n is the space between the feature size (e.g. 28 x 28) and the number of filters (if any)\n",
    "    output_shape_txt += f\"\\n\\n\" #\\n is the space between the feature size (e.g. 28 x 28) and the number of filters (if any)\n",
    "    # The function returns the complete text string and the boolean value 'above',\n",
    "    # which tells visualkeras where to place the label relative to the layer block.\n",
    "\n",
    "    return output_shape_txt, above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8185b545",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualise_model(model: Model, style: str = 'layered'):\n",
    "    \"\"\"\n",
    "    Generates and displays a visual architecture plot for a given Keras model,\n",
    "    with advanced customization options.\n",
    "    \"\"\"\n",
    "    print(f\"--- Visualizing Architecture for: {model.name} (Style: {style}) ---\")\n",
    "    \n",
    "    # Define a custom color map for the layers.\n",
    "    # We use defaultdict for a default grey color if a layer type isn't specified.\n",
    "    color_map = defaultdict(lambda: {'fill': '#999999'})\n",
    "    color_map[Conv2D] = {'fill': '#00B8D4'}\n",
    "    color_map[MaxPooling2D] = {'fill': '#FFAB00'}\n",
    "    color_map[Dense] = {'fill': '#651FFF'}\n",
    "    color_map[Flatten] = {'fill': '#E91E63'}\n",
    "    color_map[Normalization] = {'fill': '#BDBDBD'}\n",
    "\n",
    "    # Define a custom font for the text labels.\n",
    "    # The font size can be adjusted here.\n",
    "    custom_font = ImageFont.truetype(\"arial.ttf\", 14)\n",
    "\n",
    "    # We use a 'with' statement to temporarily ignore any minor warnings\n",
    "    # from the visualkeras library, which keeps our output clean.\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\", UserWarning)\n",
    "        \n",
    "        if style == 'layered':\n",
    "            # This is the core function call for creating the visualization.\n",
    "            # We use the advanced parameters from the provided example code.\n",
    "            # The function now takes the model as an argument, as requested.\n",
    "            advanced_layered_img = visualkeras.layered_view(\n",
    "                model,\n",
    "                text_callable=text_callable,\n",
    "                legend=True,  # Shows a legend to explain what each color represents.\n",
    "                font=custom_font, # Applies our custom-defined font.\n",
    "                color_map=color_map, # Uses our custom color scheme.\n",
    "                draw_volume=True,  # Renders the layers as 3D blocks.\n",
    "                draw_funnel=True,  # Shows a funnel shape to represent data compression.\n",
    "                spacing=30,  # Increases the horizontal space between layers for better readability.\n",
    "                padding=50,  # Adds a border around the entire image to prevent text cutoff.\n",
    "                scale_xy=3, # Adjusts the scale of the layers' width and depth.\n",
    "                scale_z=2,  # Adjusts the scale of the layers' height.\n",
    "                min_xy=10,\n",
    "                max_z=500, # Caps the maximum height of a layer block.\n",
    "                max_xy = 500,\n",
    "                font_color='black', # Sets the text color for the legend and labels.\n",
    "                one_dim_orientation='y', # Sets the orientation for 1D layers like Flatten.\n",
    "                # sizing_mode='accurate', # Ensures that the layer sizes are proportionally accurate.\n",
    "                type_ignore=[Flatten, Dropout], # Excludes Flatten and Dropout layers from the main diagram.\n",
    "            )\n",
    "            \n",
    "            # Display the generated image.\n",
    "            display(advanced_layered_img)\n",
    "\n",
    "        elif style == 'graph':\n",
    "            # This is an alternative view that creates a simpler 2D diagram.\n",
    "            display(visualkeras.graph_view(\n",
    "                model,\n",
    "                color_map=color_map\n",
    "            ))\n",
    "        else:\n",
    "            # Error handling for an unknown style.\n",
    "            print(f\"Error: Unknown style '{style}'. Please choose 'layered' or 'graph'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7341cec",
   "metadata": {},
   "source": [
    "## Plotting the MLP models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d7ab22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, loop through, create each model \"on the fly\", and plot it\n",
    "# The advantage of a model function, rather than a model itself, is that it acts \n",
    "# as a reusable blueprint for creating fresh, untrained model instances on demand, \n",
    "# ensuring that each experiment starts from a clean slate.\n",
    "# It's reusable, flexible with parameters, and memory efficient. \n",
    "\n",
    "for model_creation_function in mlp_model_functions:\n",
    "    # 1. Create the model instance from the function\n",
    "    model_instance: tf.keras.Model = model_creation_function()\n",
    "\n",
    "\n",
    "    # Manually build the model with the correct input shape.\n",
    "    # The `None` indicates a variable batch size. In the case of the MLP, this is an irrelevant parameter. None prevents this code from crashing for MLP models. \n",
    "    model_instance.build(input_shape=(None, 28, 28, 1)) \n",
    "\n",
    "    # 2. Call the plotting function to visualize it\n",
    "    visualise_model(model_instance, style='layered')\n",
    "    \n",
    "    # 3. Print the model summary.\n",
    "    model_instance.summary()\n",
    "\n",
    "    # Where are the activation functions stored?\n",
    "    print(\"\\n--- Activation Functions per Layer ---\")\n",
    "    for layer in model_instance.layers:\n",
    "        if hasattr(layer, 'activation'):\n",
    "            print(f\"Layer: {layer.name:<25} Activation: {layer.activation.__name__}\")\n",
    "    \n",
    "    print(\"=\"*60 + \"\\n\") # Add a separator for clarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de90471",
   "metadata": {},
   "source": [
    "## Plotting the CNN models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de9b87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, loop through, create each model \"on the fly\", and plot it\n",
    "\n",
    "for model_creation_function in cnn_model_functions:\n",
    "    # 1. Create the model instance from the function\n",
    "    model_instance: tf.keras.Model = model_creation_function()\n",
    "\n",
    "    # Manually build the model with the correct input shape.\n",
    "    # The `None` indicates a variable batch size. In the case of the MLP, this is an irrelevant parameter. None prevents this code from crashing for MLP models. \n",
    "    model_instance.build(input_shape=(None, 28, 28, 1)) \n",
    "\n",
    "    # 2. Call the plotting function to visualize it\n",
    "    visualise_model(model_instance, style='layered')\n",
    "    \n",
    "\n",
    "    # 3. Print the model summary.\n",
    "    model_instance.summary()\n",
    "\n",
    "    print(\"=\"*60 + \"\\n\") # Add a separator for clarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff3125e",
   "metadata": {},
   "source": [
    "## Defining hyperparameters for the CNN models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6292216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define Hyperparameter Set for the CNN models ---\n",
    "cnn_epochs: int = 100\n",
    "cnn_batch_size: int = 64\n",
    "\n",
    "cnn_exp_1_config: Dict[str, Union[str, float, int]] = {\n",
    "    \"optimiser\": \"Adam\",\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"epochs\": cnn_epochs,\n",
    "    \"batch_size\": cnn_batch_size\n",
    "}\n",
    "cnn_exp_2_config: Dict[str, Union[str, float, int]] = {\n",
    "    \"optimiser\": \"Adam\",\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"epochs\": cnn_epochs,\n",
    "    \"batch_size\": cnn_batch_size\n",
    "}\n",
    "\n",
    "cnn_exp_3_config: Dict[str, Union[str, float, int]] = {\n",
    "    \"optimiser\": \"SGD\",\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"epochs\": cnn_epochs,\n",
    "    \"batch_size\": cnn_batch_size\n",
    "}\n",
    "\n",
    "cnn_exp_4_config: Dict[str, Union[str, float, int]] = {\n",
    "    \"optimiser\": \"SGD\",\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"epochs\": cnn_epochs,\n",
    "    \"batch_size\": cnn_batch_size\n",
    "}\n",
    "\n",
    "cnn_config: List[Dict[str, Union[str, float, int]]] = [cnn_exp_2_config] # ,cnn_exp_3_config, cnn_exp_4_config\n",
    "# cnn_exp_1_config,  We do not need to include the cnn_exp_1_config, because we have already run this on all models. \n",
    "# In the cell below, setting the all_exp_params is a flag that ensure that when set to True, all models are trained on all parameters (computationally intensive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c47ebb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_cnn_experiments: bool = True # A simple flag to control whether we run the CNN experiments or not. This is convenient because CNN experiments take longer to run.\n",
    "# and when we restart the notebook, we might want to just run the MLP experiments first.\n",
    "all_exp_params: bool = True\n",
    "\n",
    "# --- Run the full list of experiments ---\n",
    "if run_cnn_experiments:\n",
    "    cnn_histories: List[History] = []\n",
    "    for cnn_model_function in cnn_model_functions:\n",
    "        if not all_exp_params:\n",
    "            cnn_history: History = run_experiment(\n",
    "                model_creation_func=cnn_model_function, \n",
    "                hyperparameters=cnn_exp_1_config,  # If we do not set the all experiment parameters to True, then we only need the 1 config file. \n",
    "                parent_folder='CNN_Models',\n",
    "                X_train=X_train,\n",
    "                Y_train=Y_train,\n",
    "            )\n",
    "        else:\n",
    "            for config in cnn_config:\n",
    "                cnn_history: History = run_experiment(\n",
    "                    model_creation_func=cnn_model_function, \n",
    "                    hyperparameters=config, \n",
    "                    parent_folder='CNN_Models',\n",
    "                    X_train=X_train,\n",
    "                    Y_train=Y_train,\n",
    "                )\n",
    "\n",
    "        cnn_histories.append(cnn_history)\n",
    "\n",
    "# # --- To run the second experiment, we just call it again with a different config (hyper parameter set) :-)\n",
    "# --- We first  test different model architectures before running more experiments with different hyperparameters (Part 1 Task 1 & 2). ---\n",
    "# --- THen we test the best performing model of Part 1 Task 1& 2 with different hyperparameters. ---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc1c442",
   "metadata": {},
   "source": [
    "# Plotting training results\n",
    "Below we plot the results obtained while training all models that were defined previously. Every time run_experiment is called, it returns a History object, which contains details such as training accuracy and loss and validation accuracy and loss. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c387c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history_data: Union[History, SimpleNamespace], run_name_override: Optional[str] = None) -> None:\n",
    "    \"\"\"\n",
    "    Plots training & validation accuracy and loss from either a Keras History object\n",
    "    or a SimpleNamespace object containing WandB run data.\n",
    "\n",
    "    Args:\n",
    "        history_data: A Keras History object or a SimpleNamespace object.\n",
    "        run_name_override (str, optional): A name to use for the plot title.\n",
    "        If None, it will try to infer the name.\n",
    "    \"\"\"\n",
    "    history_dict = None\n",
    "    config = {}\n",
    "    run_name = \"Unknown Run\"\n",
    "\n",
    "    # --- 1. Use duck typing to identify the object ---\n",
    "    # Instead of a strict type check, we check if it has the attributes we need.\n",
    "    if hasattr(history_data, 'history') and isinstance(history_data.history, dict):\n",
    "        # This is a Keras History object from model.fit()\n",
    "        history_dict = history_data.history\n",
    "        if hasattr(history_data, 'hyperparameters'):\n",
    "            config = history_data.hyperparameters\n",
    "        if hasattr(history_data, 'model'):\n",
    "            run_name = history_data.model.name\n",
    "            \n",
    "    elif isinstance(history_data, SimpleNamespace):\n",
    "        # This is our custom object from load_local_run_data()\n",
    "        if hasattr(history_data, 'history') and isinstance(history_data.history, pd.DataFrame):\n",
    "            # The history DataFrame needs its columns converted to lists for plotting\n",
    "            history_dict = {col: history_data.history[col].tolist() for col in history_data.history.columns}\n",
    "        if hasattr(history_data, 'config'):\n",
    "            config = history_data.config\n",
    "        if hasattr(history_data, 'name'):\n",
    "            run_name = history_data.name\n",
    "    \n",
    "    # Use the override name if provided\n",
    "    if run_name_override:\n",
    "        run_name = run_name_override\n",
    "\n",
    "    # Check if we successfully extracted the data\n",
    "    if not history_dict:\n",
    "        print(\"Error: Unsupported or invalid data type provided. Could not extract history.\")\n",
    "        return\n",
    "\n",
    "    # --- 2. Extract and Format Hyperparameters for the Title ---\n",
    "    optimizer_name = config.get('optimiser', 'N/A')\n",
    "    learning_rate = config.get('learning_rate', 'N/A')\n",
    "    batch_size = config.get('batch_size', 'N/A')\n",
    "    subtitle = f\"Optimizer: {optimizer_name}, LR: {learning_rate}, Batch Size: {batch_size}\"\n",
    "\n",
    "    # --- 3. Generate the Plots ---\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    fig.suptitle(f'Training History for: {run_name}\\n{subtitle}', fontsize=16)\n",
    "\n",
    "    # Plot training & validation accuracy values\n",
    "    if 'accuracy' in history_dict and 'val_accuracy' in history_dict:\n",
    "        ax1.plot(history_dict['accuracy'], label='Train Accuracy')\n",
    "        ax1.plot(history_dict['val_accuracy'], label='Validation Accuracy')\n",
    "        ax1.set_title('Model Accuracy')\n",
    "        ax1.set_ylabel('Accuracy')\n",
    "        ax1.set_xlabel('Epoch')\n",
    "        ax1.legend(loc='lower right')\n",
    "        ax1.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "    # Plot training & validation loss values\n",
    "    if 'loss' in history_dict and 'val_loss' in history_dict:\n",
    "        ax2.plot(history_dict['loss'], label='Train Loss')\n",
    "        ax2.plot(history_dict['val_loss'], label='Validation Loss')\n",
    "        ax2.set_title('Model Loss')\n",
    "        ax2.set_ylabel('Loss')\n",
    "        ax2.set_xlabel('Epoch')\n",
    "        ax2.legend(loc='upper right')\n",
    "        ax2.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f08790f",
   "metadata": {},
   "source": [
    "### Plot the results of every epoch\n",
    "\n",
    "The results are plotted below by looping through the histories dictionary that was constructed when we trained all our models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0f9341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple wrapper function to print all histories in a list.\n",
    "def print_training_histories(histories: List[History]) -> None:\n",
    "    print(f\"The number of models to plot: {len(histories)}\")\n",
    "    for history in histories:\n",
    "        print(f\"Model number: {histories.index(history)+1}\")\n",
    "        plot_training_history(history) # This is the function that plots data of a History objects\n",
    "        print(\"-\"*100)\n",
    "\n",
    "if run_mlp_experiments:\n",
    "    print_training_histories(mlp_histories)\n",
    "if run_cnn_experiments:\n",
    "    print_training_histories(cnn_histories)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48cab5e2",
   "metadata": {},
   "source": [
    "## Testing the models on the held-out test set\n",
    "We test the model on the test data, which is data that the model has never seen before. Then we verify the model's real-world accuracy. It is expected that this does not deviate much from the validation sets, because the MNIST dataset contains images that are very clean and simple:\n",
    "- They are small (28 x 28 pixels only).\n",
    "- The digits are centered and normalised in size.\n",
    "- The background is a solid colour with no distracting noise.\n",
    "  \n",
    "Because of this simplicity, the patterns that differentiate one digit from another (e.g., a \"1\" is a vertical line, an \"8\" is two loops) are very strong and easy for our model to learn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7af8a5",
   "metadata": {},
   "source": [
    "First we define a function that browser to a folder with saved models, extracts the file with the highest validation accuracy in its name, loads it and tests it with the held-out X_test and Y_test. \n",
    "\n",
    "A function is convenient because we will use it on different models, with different hyperparameters and hence, avoid repetition. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c29410f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # The main orchestrator. It takes the list from find_top_models, then loops through it, loading each model and passing it to analyse_single_model to be analysed.\n",
    "# def process_model_list(\n",
    "#     models_to_process: List[Dict[str, Any]],\n",
    "#     analysis_func: Callable[[tf.keras.Model, NDArray[np.float32], NDArray[np.int_]], Tuple[Any, ...]],\n",
    "#     x_test_data: NDArray[np.float32],\n",
    "#     y_test_data: NDArray[np.int_]\n",
    "# ) -> List[Dict[str, Any]]:\n",
    "#     \"\"\"\n",
    "#     Processes a given list of models using a provided analysis function.\n",
    "\n",
    "#     Args:\n",
    "#         models_to_process (List[Dict[str, Any]]): A list of dictionaries, where each\n",
    "#             dictionary contains info about a model (e.g., from find_top_models).\n",
    "#         analysis_func (Callable): The function to run on each loaded model.\n",
    "#         x_test_data (NDArray): The test dataset features.\n",
    "#         y_test_data (NDArray): The test dataset labels.\n",
    "\n",
    "#     Returns:\n",
    "#         List[Dict[str, Any]]: A list of dictionaries containing model info and analysis results.\n",
    "#     \"\"\"\n",
    "#     # This function is now fully decoupled. It doesn't know how the models were\n",
    "#     # found; it only knows how to load, process, and collect results for the\n",
    "#     # list it receives. This is an excellent, highly modular design.\n",
    "    \n",
    "#     print(f\"\\n--- Processing {len(models_to_process)} Models ---\")\n",
    "    \n",
    "#     processed_results: List[Dict[str, Any]] = []\n",
    "#     for i, model_info in enumerate(models_to_process):\n",
    "#         print(\"\\n\" + \"=\"*80)\n",
    "#         print(f\"--- Processing Model {i+1} of {len(models_to_process)} (Val Acc: {model_info['val_accuracy']:.4f}) ---\")\n",
    "#         print(f\"Path: {model_info['path']}\")\n",
    "#         print(\"=\"*80)\n",
    "        \n",
    "#         loaded_model: tf.keras.Model = tf.keras.models.load_model(model_info['path'])\n",
    "        \n",
    "#         # Here we call the injected analysis function.\n",
    "#         analysis_output: Tuple[Any, ...] = analysis_func(loaded_model, x_test_data, y_test_data)\n",
    "        \n",
    "#         processed_results.append({\n",
    "#             'model': loaded_model,\n",
    "#             'path': model_info['path'],\n",
    "#             'validation_accuracy': model_info['val_accuracy'],\n",
    "#             'analysis_results': analysis_output\n",
    "#         })\n",
    "        \n",
    "#     return processed_results\n",
    "\n",
    "import pickle\n",
    "from types import SimpleNamespace\n",
    "\n",
    "# (This assumes the other functions like find_top_models and analyse_single_model exist)\n",
    "\n",
    "def process_model_list(\n",
    "    models_to_process: List[Dict[str, Any]],\n",
    "    analysis_func: Callable[[tf.keras.Model, NDArray[np.float32], NDArray[np.int_]], Tuple[Any, ...]],\n",
    "    x_test_data: NDArray[np.float32],\n",
    "    y_test_data: NDArray[np.int_]\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Processes a given list of models,including plotting their training history.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Processing {len(models_to_process)} Models ---\")\n",
    "    \n",
    "    processed_results: List[Dict[str, Any]] = []\n",
    "    for i, model_info in enumerate(models_to_process):\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(f\"--- Processing Model {i+1} of {len(models_to_process)} (Val Acc: {model_info['val_accuracy']:.4f}) ---\")\n",
    "        print(f\"Path: {model_info['path']}\")\n",
    "        \n",
    "        # --- NEW: Load and Plot Training History ---\n",
    "        model_dir: str = os.path.dirname(model_info['path'])\n",
    "        history_path: str = os.path.join(model_dir, 'training_history.pkl')\n",
    "        \n",
    "        if os.path.exists(history_path):\n",
    "            print(f\"Found and loading history from: {history_path}\")\n",
    "            with open(history_path, 'rb') as f:\n",
    "                history_dict: dict = pickle.load(f)\n",
    "            \n",
    "            # We must wrap the loaded dictionary in a SimpleNamespace to match what\n",
    "            # our universal plotting function expects for non-Keras objects.\n",
    "            history_for_plotting = SimpleNamespace(\n",
    "                history=pd.DataFrame(history_dict), # Convert dict to DataFrame\n",
    "                # We need to manually add name and config for the plot titles\n",
    "                name=os.path.basename(model_dir),\n",
    "                config={} # Config isn't in the pickle, so we pass an empty dict\n",
    "            )\n",
    "            plot_training_history(history_for_plotting)\n",
    "        else:\n",
    "            print(f\"Warning: training_history.pkl not found in {model_dir}\")\n",
    "        # --- END OF NEW CODE ---\n",
    "\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        loaded_model: tf.keras.Model = tf.keras.models.load_model(model_info['path'])\n",
    "        \n",
    "        analysis_output: Tuple[Any, ...] = analysis_func(loaded_model, x_test_data, y_test_data)\n",
    "        \n",
    "        processed_results.append({\n",
    "            'model': loaded_model,\n",
    "            'path': model_info['path'],\n",
    "            'validation_accuracy': model_info['val_accuracy'],\n",
    "            'analysis_results': analysis_output\n",
    "        })\n",
    "        \n",
    "    return processed_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8d8f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function's only responsibility is to perform a full analysis (print reports, show plots) on a single, already-loaded model.\n",
    "\n",
    "def analyse_single_model(\n",
    "    loaded_model: tf.keras.Model, \n",
    "    x_test_data: NDArray[np.float32], \n",
    "    y_test_data: NDArray[np.int_]\n",
    ") -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Performs a full analysis of a single loaded Keras model.\n",
    "    ...\n",
    "    \"\"\"\n",
    "    # By isolating all the analysis logic for a single model here, you've created\n",
    "    # a highly modular and reusable component. You could point this function at any\n",
    "    # similarly trained Keras model and get a full report. Well done.\n",
    "    \n",
    "    # --- Print Compiled Hyperparameters ---\n",
    "    print(\"\\n--- Key Hyperparameters ---\")\n",
    "    optimizer_config: Dict[str, Any] = loaded_model.optimizer.get_config()\n",
    "    print(f\"{'Optimiser:':<20} {optimizer_config.get('name', 'N/A')}\")\n",
    "    print(f\"{'Learning Rate:':<20} {optimizer_config.get('learning_rate', 'N/A')}\")\n",
    "    print(f\"{'Loss Function:':<20} {loaded_model.loss}\")\n",
    "    \n",
    "    print(\"\\n--- Model Summary (Architecture) ---\")\n",
    "    loaded_model.summary()\n",
    "\n",
    "    print(\"\\n--- Evaluating model performance on the test set ---\")\n",
    "    # A subtle but important detail! Setting verbose=0 prevents Keras from printing\n",
    "    # its own progress bar here, which is good practice as you're providing your own\n",
    "    # custom, formatted output immediately after. It keeps the console output clean.\n",
    "    loss, accuracy = loaded_model.evaluate(x_test_data, y_test_data, verbose=0)\n",
    "    print(f\"Test Set Loss: {loss:.4f}\")\n",
    "    print(f\"Test Set Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    print(\"\\n--- Detailed Analysis ---\")\n",
    "    y_pred_probabilities: NDArray[np.float32] = loaded_model.predict(x_test_data)\n",
    "    y_pred: NDArray[np.int_] = np.argmax(y_pred_probabilities, axis=1)\n",
    "\n",
    "    print(\"\\n--- Classification Report ---\")\n",
    "    report: str = classification_report(y_test_data, y_pred, target_names=[str(i) for i in range(10)])\n",
    "    print(report)\n",
    "\n",
    "    print(\"\\n--- Confusion Matrix ---\")\n",
    "    cm: NDArray[np.int_] = confusion_matrix(y_test_data, y_pred)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('Actual Label')\n",
    "    plt.title(f'Confusion Matrix for {loaded_model.name}')\n",
    "    plt.show()\n",
    "    \n",
    "    # A great extension to this function would be to also return the generated 'report'\n",
    "    # and 'cm' objects. This would allow the calling function to, for example,\n",
    "    # save the confusion matrix image to a file or programmatically find the class\n",
    "    # with the lowest F1-score from the report.\n",
    "    return accuracy, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182adbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function's only responsibility is to search folders and return a ranked list of model file paths. It doesn't load or analyse anything.\n",
    "\n",
    "def find_top_models(parent_folder: str, top_n: int) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Scans a directory to find all Keras models and returns a sorted list of the top N.\n",
    "\n",
    "    This function finds model files named with a 'val_acc-...' pattern, extracts the\n",
    "    accuracy, and returns a list of dictionaries containing the path and validation\n",
    "    accuracy for the best models found.\n",
    "\n",
    "    Args:\n",
    "        parent_folder (str): The root directory to start the search from.\n",
    "        top_n (int): The number of top models to return.\n",
    "\n",
    "    Returns:\n",
    "        List[Dict[str, Any]]: A list of dictionaries, each representing a model,\n",
    "        sorted by validation accuracy in descending order.\n",
    "        Example: [{'path': 'path/to/model.keras', 'val_accuracy': 0.985}]\n",
    "    \"\"\"\n",
    "    # This function is a great example of the 'separation of concerns' principle.\n",
    "    # Its only job is to interact with the file system. By not loading the heavyweight\n",
    "    # TensorFlow models here, we make the function incredibly fast and memory-efficient.\n",
    "    # This is a crucial distinction between a simple script and a well-designed programme.\n",
    "    all_models: List[Dict[str, Any]] = []\n",
    "    pattern: re.Pattern = re.compile(r\"val_acc-([\\d.]+)\\.keras\")\n",
    "\n",
    "    if not os.path.isdir(parent_folder):\n",
    "        print(f\"Error: Parent directory not found at '{parent_folder}'\")\n",
    "        return []\n",
    "\n",
    "    for dirpath, _, filenames in os.walk(parent_folder):\n",
    "        for filename in filenames:\n",
    "            match: Union[re.Match, None] = pattern.search(filename)\n",
    "            if match:\n",
    "                # This is a great use of regular expressions to parse metadata from a filename.\n",
    "                # For added robustness in a real-world application, we might wrap this float()\n",
    "                # conversion in a try-except block to gracefully handle any malformed filenames.\n",
    "                # This is a concept called 'defensive programming'.\n",
    "                val_accuracy: float = float(match.group(1))\n",
    "                model_path: str = os.path.join(dirpath, filename)\n",
    "                all_models.append({'path': model_path, 'val_accuracy': val_accuracy})\n",
    "    \n",
    "    # Python's built-in `sorted` function is highly efficient. Using a `lambda` function\n",
    "    # as the 'key' is a powerful, concise way to specify a custom sorting rule without\n",
    "    # needing to define a separate, named function.\n",
    "    sorted_models: List[Dict[str, Any]] = sorted(all_models, key=lambda x: x['val_accuracy'], reverse=True)\n",
    "    \n",
    "    # List slicing is a clean and Pythonic way to select a subset of a list.\n",
    "    return sorted_models[:top_n]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b24f712",
   "metadata": {},
   "source": [
    "### Testing the MLP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f0f21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Find the top models. \n",
    "top_3_models_to_analyse: List[Dict[str, Any]] = find_top_models(\n",
    "    parent_folder='MLP_Models', \n",
    "    top_n=3\n",
    ")\n",
    "\n",
    "# Step 2: Pass the list of found models to the processing function.\n",
    "if top_3_models_to_analyse:\n",
    "    final_results: List[Dict[str, Any]] = process_model_list(\n",
    "        models_to_process=top_3_models_to_analyse,\n",
    "        analysis_func=analyse_single_model,\n",
    "        x_test_data=X_test,\n",
    "        y_test_data=Y_test\n",
    "    )\n",
    "    print(f\"\\nCompleted analysis. Processed {len(final_results)} models.\")\n",
    "else:\n",
    "    print(\"No models found to process.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c2e325",
   "metadata": {},
   "source": [
    "### Testing the CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e42a562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Find the top models. \n",
    "top_3_models_to_analyse: List[Dict[str, Any]] = find_top_models(\n",
    "    parent_folder='CNN_Models', \n",
    "    top_n=3\n",
    ")\n",
    "\n",
    "# Step 2: Pass the list of found models to the processing function.\n",
    "if top_3_models_to_analyse:\n",
    "    final_results: List[Dict[str, Any]] = process_model_list(\n",
    "        models_to_process=top_3_models_to_analyse,\n",
    "        analysis_func=analyse_single_model,\n",
    "        x_test_data=X_test,\n",
    "        y_test_data=Y_test\n",
    "    )\n",
    "    print(f\"\\nCompleted analysis. Processed {len(final_results)} models.\")\n",
    "else:\n",
    "    print(\"No models found to process.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9820dc3",
   "metadata": {},
   "source": [
    "# Future to do's (not part of this assessment)\n",
    "- Implement KerasTuner (gridsearch amongst others), to automatically train and test models with a plethora of hyperparamaters, optimisers, loss functions, and select the best performing:\n",
    "\n",
    "We first need to install it first: uv pip install keras-tuner\n",
    "import keras_tuner\n",
    "\n",
    "def build_model(hp):\n",
    "    \"\"\"This is our hypermodel, which defines the search space.\"\"\"\n",
    "    \n",
    "    model = Sequential(name=\"Tuned_MLP\")\n",
    "    model.add(Input(shape=(28, 28)))\n",
    "    model.add(Normalization())\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    # --- Define Hyperparameters to Tune ---\n",
    "    # Tune the number of units in the first Dense layer\n",
    "    hp_units = hp.Int('units', min_value=32, max_value=512, step=32)\n",
    "    model.add(Dense(units=hp_units, activation='relu'))\n",
    "    \n",
    "    # Tune the learning rate\n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "    \n",
    "    # Add the output layer\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "    # --- Compile the model inside the function ---\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "--- Set up the Tuner ---\n",
    "### We'll use RandomSearch, which randomly tries combinations.\n",
    "tuner = keras_tuner.RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=10,  # The total number of model variations to test\n",
    "    executions_per_trial=2, # The number of times to train each model variation\n",
    "    directory='tuning_results',\n",
    "    project_name='MNIST_Tuning'\n",
    ")\n",
    "\n",
    "### --- Start the Search ---\n",
    "### This is like model.fit(), but it runs the whole tuning process.\n",
    "tuner.search(X_train, Y_train, epochs=10, validation_split=0.1)\n",
    "\n",
    "### --- Get the Best Model ---\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "best_hyperparameters = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(\"\\n--- Best Hyperparameters Found ---\")\n",
    "print(best_hyperparameters.values)\n",
    "\n",
    "print(\"\\n--- Evaluating the Best Model Found by the Tuner ---\")\n",
    "best_model.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2428276c",
   "metadata": {},
   "source": [
    "# Testing a new approach (Future to do)\n",
    "### A Base Class for models, with a common interface and allowing for inheriting layers, inheriting behaviour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7275e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # models.py\n",
    "# # This file serves as a centralised factory for creating our neural network models.\n",
    "# # It uses a class-based, inherited structure to keep the codebase organised and extensible.\n",
    "\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.layers import Normalization, Flatten, Dense, Conv2D, MaxPooling2D\n",
    "# from tensorflow.keras.models import Model\n",
    "\n",
    "# class BaseNeuralNetwork(Model):\n",
    "#     \"\"\"\n",
    "#     A base class for all neural networks in this project.\n",
    "#     It encapsulates the common input and preprocessing layers that all models will share.\n",
    "#     \"\"\"\n",
    "#     def __init__(self, **kwargs):\n",
    "#         # We call the parent constructor to ensure correct initialisation of the Keras Model.\n",
    "#         # **kwargs allows us to pass additional arguments like 'name' when creating subclasses.\n",
    "#         super().__init__(**kwargs)\n",
    "#         # These layers are common to all models and are defined here once for efficiency.\n",
    "#         self.normalization_layer = Normalization(name=\"normalization_layer\")\n",
    "#         self.flatten_layer = Flatten(name=\"flatten_layer\")\n",
    "\n",
    "#     def call(self, inputs):\n",
    "#         \"\"\"\n",
    "#         Defines the forward pass for the common preprocessing layers.\n",
    "#         \"\"\"\n",
    "#         # The input data is passed through the normalisation and flattening layers.\n",
    "#         x = self.normalization_layer(inputs)\n",
    "#         x = self.flatten_layer(x)\n",
    "#         return x\n",
    "\n",
    "# class MLPModel(BaseNeuralNetwork):\n",
    "#     \"\"\"\n",
    "#     A standard Multi-Layer Perceptron (MLP) model.\n",
    "#     It inherits the base preprocessing from BaseNeuralNetwork and adds dense layers.\n",
    "#     \"\"\"\n",
    "#     def __init__(self, num_units_1: int = 128, num_units_2: int = 256, num_units_3: int = 64, num_classes: int = 10, **kwargs):\n",
    "#         # We call the parent constructor and provide a specific name for this model.\n",
    "#         super().__init__(name='mlp_model', **kwargs)\n",
    "#         # Define the unique dense layers for this specific model architecture.\n",
    "#         self.dense_1 = Dense(units=num_units_1, activation='relu', name=\"dense_1\")\n",
    "#         self.dense_2 = Dense(units=num_units_2, activation='relu', name=\"dense_2\")\n",
    "#         self.dense_3 = Dense(units=num_units_3, activation='relu', name=\"dense_3\")\n",
    "#         self.output_layer = Dense(units=num_classes, activation='softmax', name=\"output_layer\")\n",
    "\n",
    "#     def call(self, inputs):\n",
    "#         # First, we process the input using the base class's call method.\n",
    "#         x = super().call(inputs)\n",
    "#         # Then, we pass the output through the MLP-specific layers.\n",
    "#         x = self.dense_1(x)\n",
    "#         x = self.dense_2(x)\n",
    "#         x = self.dense_3(x)\n",
    "#         return self.output_layer(x)\n",
    "\n",
    "# class MLP_Wide_Model(BaseNeuralNetwork):\n",
    "#     \"\"\"\n",
    "#     A wider, shallower MLP model. This is a variation for experimentation.\n",
    "#     \"\"\"\n",
    "#     def __init__(self, num_units_1: int = 256, num_units_2: int = 128, num_classes: int = 10, **kwargs):\n",
    "#         super().__init__(name='mlp_wide_model', **kwargs)\n",
    "#         # This model has a different configuration of dense layers.\n",
    "#         self.dense_1 = Dense(units=num_units_1, activation='relu', name=\"dense_1\")\n",
    "#         self.dense_2 = Dense(units=num_units_2, activation='relu', name=\"dense_2\")\n",
    "#         self.output_layer = Dense(units=num_classes, activation='softmax', name=\"output_layer\")\n",
    "\n",
    "#     def call(self, inputs):\n",
    "#         x = super().call(inputs)\n",
    "#         x = self.dense_1(x)\n",
    "#         x = self.dense_2(x)\n",
    "#         return self.output_layer(x)\n",
    "\n",
    "# class SimpleCNN(BaseNeuralNetwork):\n",
    "#     \"\"\"\n",
    "#     A simple Convolutional Neural Network (CNN) model for image classification.\n",
    "#     \"\"\"\n",
    "#     def __init__(self, num_classes: int = 10, **kwargs):\n",
    "#         super().__init__(name='simple_cnn', **kwargs)\n",
    "#         # The convolutional and pooling layers are unique to CNNs.\n",
    "#         self.conv1 = Conv2D(filters=32, kernel_size=(3, 3), activation='relu', name=\"conv1\")\n",
    "#         self.pool1 = MaxPooling2D(pool_size=(2, 2), name=\"pool1\")\n",
    "#         self.conv2 = Conv2D(filters=64, kernel_size=(3, 3), activation='relu', name=\"conv2\")\n",
    "#         self.pool2 = MaxPooling2D(pool_size=(2, 2), name=\"pool2\")\n",
    "#         self.output_layer = Dense(units=num_classes, activation='softmax', name=\"output_layer\")\n",
    "\n",
    "#     def call(self, inputs):\n",
    "#         # We start by using the base class's normalisation.\n",
    "#         x = self.normalization_layer(inputs)\n",
    "        \n",
    "#         # Then, we pass the output through the CNN-specific layers.\n",
    "#         x = self.conv1(x)\n",
    "#         x = self.pool1(x)\n",
    "#         x = self.conv2(x)\n",
    "#         x = self.pool2(x)\n",
    "\n",
    "#         # The flatten layer from the base class is still applied before the output layer.\n",
    "#         x = self.flatten_layer(x)\n",
    "        \n",
    "#         return self.output_layer(x)\n",
    "\n",
    "# class DeepCNN(BaseNeuralNetwork):\n",
    "#     \"\"\"\n",
    "#     A deeper CNN model with more layers for greater representational capacity.\n",
    "#     \"\"\"\n",
    "#     def __init__(self, num_classes: int = 10, **kwargs):\n",
    "#         super().__init__(name='deep_cnn', **kwargs)\n",
    "#         # This model has a more complex arrangement of convolutional layers.\n",
    "#         self.conv1 = Conv2D(32, (3, 3), activation='relu')\n",
    "#         self.conv2 = Conv2D(32, (3, 3), activation='relu')\n",
    "#         self.pool1 = MaxPooling2D(pool_size=(2, 2))\n",
    "#         self.conv3 = Conv2D(64, (3, 3), activation='relu')\n",
    "#         self.conv4 = Conv2D(64, (3, 3), activation='relu')\n",
    "#         self.pool2 = MaxPooling2D(pool_size=(2, 2))\n",
    "#         self.dense1 = Dense(128, activation='relu')\n",
    "#         self.output_layer = Dense(units=num_classes, activation='softmax')\n",
    "\n",
    "#     def call(self, inputs):\n",
    "#         x = self.normalization_layer(inputs)\n",
    "#         x = self.conv1(x)\n",
    "#         x = self.conv2(x)\n",
    "#         x = self.pool1(x)\n",
    "#         x = self.conv3(x)\n",
    "#         x = self.conv4(x)\n",
    "#         x = self.pool2(x)\n",
    "#         x = self.flatten_layer(x)\n",
    "#         x = self.dense1(x)\n",
    "#         return self.output_layer(x)\n",
    "\n",
    "# def create_model_from_class(model_class: type, input_shape, **kwargs):\n",
    "#     \"\"\"\n",
    "#     A helper function to instantiate a model class with a defined input shape.\n",
    "#     It automatically builds the model for you.\n",
    "#     \"\"\"\n",
    "#     model_instance = model_class(**kwargs)\n",
    "#     # The build method ensures that the model's layers are initialised with the correct input shape.\n",
    "#     model_instance.build(input_shape=(None,) + input_shape)\n",
    "#     return model_instance\n",
    "\n",
    "\n",
    "# Just a thought for now. It may or may not simplify creating models, but only by a margin. It does however increase complexity of the code, so that's the trade-off. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70bd9c8a",
   "metadata": {},
   "source": [
    "## A function to pull data from the WandB server, so we can use it locally. \n",
    "\n",
    "Note that this requires the implementation of additional functions to be able to print the model architecture AND the associated training data of the model. \n",
    "This is beyond the scope of this assessment, and is a future to do. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0655d33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # In the cell above, we store all the models, run history, and hyperparameters in the Wandb cloud platform. We save the model architectures + their best W ad b Locally as well.\n",
    "# # The function below can be used to pull all that data from the cloud and store it locally as well. \n",
    "# # This can be useful if we want to analyse it with python. E.g., plotting the accuracy and losses locally, rather than in the cloud with limited customisation tools. \n",
    "\n",
    "# def pull_wandb_data_from_cloud(project_name, local_dir):\n",
    "#     \"\"\"\n",
    "#     Pulls all runs, models, and artifacts from a WandB project\n",
    "#     and stores them locally, using the run's expressive name for the folder.\n",
    "\n",
    "#     Args:\n",
    "#         project_name (str): The name of the WandB project.\n",
    "#         local_dir (str): The local directory to save the data.\n",
    "#     \"\"\"\n",
    "#     print(f\"Connecting to WandB project: {project_name}\")\n",
    "#     api = wandb.Api()\n",
    "    \n",
    "#     try:\n",
    "#         user_name = api.default_entity\n",
    "#     except wandb.errors.CommError:\n",
    "#         print(\"Error: Could not retrieve WandB username. Make sure you are logged in.\")\n",
    "#         return\n",
    "        \n",
    "#     os.makedirs(local_dir, exist_ok=True)\n",
    "    \n",
    "#     runs = api.runs(f\"{user_name}/{project_name}\")\n",
    "    \n",
    "#     print(f\"Found {len(runs)} runs for user: {user_name}. Downloading data...\")\n",
    "\n",
    "#     for run in runs:\n",
    "#         # --- Fix: Create a clean, expressive folder name ---\n",
    "#         # 1. Start with the expressive run name.\n",
    "#         run_folder_name = run.name\n",
    "        \n",
    "#         # 2. Sanitize the name to remove any characters that are invalid for file paths.\n",
    "#         sanitized_run_name = re.sub(r'[\\\\/:*?\"<>|]', '_', run_folder_name)\n",
    "        \n",
    "#         # 3. We no longer append the unique run ID, since the timestamp already ensures uniqueness.\n",
    "#         final_folder_name = sanitized_run_name\n",
    "        \n",
    "#         # Construct the full path for the run's local directory.\n",
    "#         run_dir = os.path.join(local_dir, final_folder_name)\n",
    "#         os.makedirs(run_dir, exist_ok=True)\n",
    "        \n",
    "#         # Download all files associated with the run\n",
    "#         for file in run.files():\n",
    "#             file.download(root=run_dir, exist_ok=True)\n",
    "            \n",
    "#         print(f\"Downloaded files for run: {final_folder_name}\")\n",
    "\n",
    "#         # Download artifacts (including models)\n",
    "#         for artifact in run.logged_artifacts():\n",
    "#             print(f\"Downloading artifact: {artifact.name}\")\n",
    "            \n",
    "#             sanitized_artifact_name = re.sub(r'[\\\\/:*?\"<>|]', '_', artifact.name)\n",
    "            \n",
    "#             artifact_dir = os.path.join(run_dir, \"artifacts\", sanitized_artifact_name)\n",
    "#             os.makedirs(artifact_dir, exist_ok=True)\n",
    "#             artifact.download(root=artifact_dir)\n",
    "\n",
    "#     print(\"Download complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34d25a5",
   "metadata": {},
   "source": [
    "### Below we fetch the data from WandB cloud via its API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1117c3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # We get the current working directory.\n",
    "# current_dir = os.getcwd()\n",
    "# # We create the parent folder path using os.path.join.\n",
    "# parent_folder_name = \"WandB_downloads\"\n",
    "# wandb_folder = os.path.join(current_dir, parent_folder_name)\n",
    "# # We now call the function with the correct local directory.\n",
    "# pull_wandb_data_from_cloud(\"CSE5ML-Assessment2\", wandb_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe03810",
   "metadata": {},
   "source": [
    "### Plotting results using WandB run objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c37ebac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_and_plot_all_runs(parent_folder: str) -> None:\n",
    "#     \"\"\"\n",
    "#     Scans all subdirectories in a parent folder, loads the WandB run data \n",
    "#     (config and history) from each, and then calls the universal plotting function \n",
    "#     for each valid run found. This function is self-contained.\n",
    "\n",
    "#     Args:\n",
    "#         parent_folder (str): The path to the directory containing all downloaded run folders.\n",
    "#     \"\"\"\n",
    "#     print(f\"--- Scanning and plotting all runs in '{parent_folder}' ---\")\n",
    "#     if not os.path.isdir(parent_folder):\n",
    "#         print(f\"Error: Directory not found at '{parent_folder}'\")\n",
    "#         return\n",
    "\n",
    "#     run_folders: List[str] = [d for d in os.listdir(parent_folder) if os.path.isdir(os.path.join(parent_folder, d))]\n",
    "#     print(f\"Found {len(run_folders)} potential run folders to process.\")\n",
    "    \n",
    "#     plotted_count: int = 0\n",
    "\n",
    "#     for run_folder_name in sorted(run_folders):\n",
    "#         run_folder_path: str = os.path.join(parent_folder, run_folder_name)\n",
    "#         print(f\"\\nProcessing folder: {run_folder_name}\")\n",
    "\n",
    "#         # --- Start of Merged Data Loading Logic ---\n",
    "#         config_file: str = os.path.join(run_folder_path, 'config.yaml')\n",
    "        \n",
    "#         # 1. Find and load the config file\n",
    "#         if not os.path.exists(config_file):\n",
    "#             print(f\"  -> Skipping: config.yaml not found.\")\n",
    "#             continue\n",
    "#         try:\n",
    "#             with open(config_file, 'r') as f:\n",
    "#                 config: Any = yaml.safe_load(f)\n",
    "#                 config_data: Dict[str, Any] = {k: v['value'] for k, v in config.items() if isinstance(v, dict) and 'value' in v}\n",
    "#         except Exception as e:\n",
    "#             print(f\"  -> Skipping: Could not load config.yaml. Error: {e}\")\n",
    "#             continue\n",
    "\n",
    "#         # 2. Find and load all history .parquet files\n",
    "#         history_files: List[str] = []\n",
    "#         artifacts_dir: str = os.path.join(run_folder_path, 'artifacts')\n",
    "#         if os.path.isdir(artifacts_dir):\n",
    "#             for root, _, files in os.walk(artifacts_dir):\n",
    "#                 for file in files:\n",
    "#                     if file.endswith('.parquet'):\n",
    "#                         history_files.append(os.path.join(root, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed950f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to your main downloads folder\n",
    "wandb_downloads_path = os.path.join(os.getcwd(), 'WandB_downloads')\n",
    "\n",
    "# Call the new function to process and plot everything inside that folder\n",
    "load_and_plot_all_runs(wandb_downloads_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
