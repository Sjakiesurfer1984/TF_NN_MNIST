{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71a89eb6",
   "metadata": {},
   "source": [
    "# Notes on Sparse Categorical Loss vs. Categorical Loss\n",
    "# Understanding Cross-Entropy Loss\n",
    "\n",
    "At its heart, **cross-entropy** is a concept from information theory that measures how different two probability distributions are. In the context of training a neural network for classification, we use it to measure the \"distance\" between the model's predicted probability distribution and the true probability distribution of the labels. The goal of training is to minimise this distance, effectively making the model's predictions more accurate (Goodfellow et al., 2016).\n",
    "\n",
    "---\n",
    "### Categorical Cross-Entropy (for One-Hot Labels)\n",
    "\n",
    "You use this loss function when your labels are explicitly **one-hot encoded** (e.g., the digit `3` is represented as `[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]`). The formula for a single sample is:\n",
    "\n",
    "$$L = -\\sum_{i=0}^{C-1} y_i \\log(\\hat{y}_i)$$\n",
    "\n",
    "-   $L$ is the final loss value for the sample.\n",
    "-   $C$ is the total number of classes (e.g., 10 for MNIST).\n",
    "-   $y_i$ is the ground truth (it is `1` for the correct class and `0` for all others).\n",
    "-   $\\hat{y}_i$ is the model's predicted probability for class $i$.\n",
    "\n",
    "Because the `y` vector is almost all zeros, the summation simplifies to just the negative logarithm of the probability the model assigned to the single correct class. For a label of `3`, the loss simply becomes $L = -\\log(\\hat{y}_3)$.\n",
    "\n",
    "---\n",
    "### Sparse Categorical Cross-Entropy (for Integer Labels)\n",
    "\n",
    "This is a more computationally and memory-efficient version used when your labels are simple **integers** (e.g., `3`). It arrives at the exact same mathematical result but skips the need for the one-hot encoded vector.\n",
    "\n",
    "The formula is a direct implementation of the simplified logic:\n",
    "\n",
    "$$L = -\\log(\\hat{y}_c)$$\n",
    "\n",
    "-   $L$ is the final loss value for the sample.\n",
    "-   $c$ is the integer representing the correct class (e.g., `c = 3`).\n",
    "-   $\\hat{y}_c$ is the model's predicted probability for that correct class $c$.\n",
    "\n",
    "As Chollet (2021) explains, both formulas compute the exact same value. The choice is purely a practical one based on the format of your labels, not a mathematical one that affects the model's learning.\n",
    "\n",
    "---\n",
    "**References**\n",
    "\n",
    "Chollet, F. (2021). *Deep learning with Python* (2nd ed.). Shelter Island, NY: Manning Publications.\n",
    "\n",
    "Goodfellow, I., Bengio, Y., & Courville, A. (2016). *Deep learning*. Cambridge, MA: MIT Press."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a6d567",
   "metadata": {},
   "source": [
    "# Notes on batch normalisation\n",
    "\n",
    "**Batch normalisation (Batch Norm)** is a layer used in neural networks to **stabilise and accelerate training**.  \n",
    "Unlike normalising only the input data before training, Batch Norm normalises the **inputs to hidden layers** inside the network.  \n",
    "It is usually added **after a Conv2D or Dense layer**, and **before the activation function**.\n",
    "\n",
    "## The problem it solves: *internal covariate shift*\n",
    "\n",
    "In deep networks, as earlier layers update their weights, the **distribution of outputs changes**.  \n",
    "These outputs are inputs for the next layers.  \n",
    "This constant change in data distribution is called **internal covariate shift**.  \n",
    "\n",
    "It is similar to trying to learn to hit a baseball while the pitcher is standing on a moving truck.  \n",
    "This slows training and makes it unstable.\n",
    "\n",
    "## How it works: *normalising inside the network*\n",
    "\n",
    "For each mini-batch during training, Batch Norm does the following:\n",
    "\n",
    "1. **Calculate batch statistics**  \n",
    "   - Mean (μ) and variance (σ²) of the activations in the batch.\n",
    "\n",
    "2. **Normalise**  \n",
    "   - Each activation $x_i$ is normalised:  \n",
    "\n",
    "   $$\n",
    "   \\hat{x}_i = \\frac{x_i - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}}\n",
    "   $$\n",
    "\n",
    "3. **Scale and shift (learnable parameters)**  \n",
    "   - Two new trainable parameters are introduced: γ (gamma) and β (beta).  \n",
    "   - They allow the network to adapt the normalised activations:  \n",
    "\n",
    "   $$\n",
    "   y_i = \\gamma \\hat{x}_i + \\beta\n",
    "   $$\n",
    "\n",
    "## Benefits of batch normalisation\n",
    "\n",
    "- **Faster training** – stabilised inputs allow for higher learning rates.  \n",
    "- **Reduced vanishing gradients** – keeps activations in a healthy range.  \n",
    "- **Acts as a regulariser** – mini-batch statistics add noise, reducing overfitting.  \n",
    "- **Smoother loss landscape** – makes optimisation easier and convergence faster.  \n",
    "\n",
    "## Reference\n",
    "\n",
    "Ioffe, S., & Szegedy, C. (2015). *Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift*.  \n",
    "arXiv preprint: [arXiv:1502.03167](https://arxiv.org/abs/1502.03167)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a43b04",
   "metadata": {},
   "source": [
    "# Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbc1d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "# Imports the MNIST dataset from Keras, a classic collection of 70,000 grayscale images of handwritten digits (0-9).\n",
    "from keras.datasets import mnist \n",
    "\n",
    "# Imports TensorFlow, the core open-source library from Google for building and training machine learning models. We use the alias 'tf' by convention.\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import TensorShape \n",
    "\n",
    "from visualkeras import SpacingDummyLayer # Added for better text spacing\n",
    "\n",
    "# Imports the Adam optimiser. An optimiser is an algorithm that adjusts the model's internal parameters (weights) to minimise the error, and Adam is a popular, efficient choice.\n",
    "from tensorflow.keras.optimizers import Adam, SGD, AdamW, RMSprop\n",
    "\n",
    "# Imports specific performance metrics. Metrics are used to evaluate how well the model is performing.\n",
    "# Precision: Measures the accuracy of positive predictions.\n",
    "# Recall: Measures the model's ability to find all the actual positive instances.\n",
    "# Accuracy: Measures the overall fraction of correct predictions.\n",
    "from tensorflow.keras.metrics import Precision, Recall, Accuracy\n",
    "\n",
    "# Imports the History callback object. A 'callback' is a function that can be executed at different stages of training. The History object automatically records the metrics and loss values from each epoch.\n",
    "from tensorflow.python.keras.callbacks import History\n",
    "\n",
    "# Imports the ModelCheckpoint callback. This callback saves the model to a file during training, typically only when its performance on a validation metric improves.\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard, EarlyStopping\n",
    "\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "# Imports the Pandas library, a powerful tool for data manipulation and analysis. It's mainly used for working with structured data in tables called DataFrames. 'pd' is the standard alias.\n",
    "import pandas as pd\n",
    "\n",
    "# Imports the Sequential model type from Keras. This is the simplest way to build a model, by creating a linear stack of layers.\n",
    "from keras.models import Sequential, Model\n",
    "\n",
    "# Imports different types of layers, which are the fundamental building blocks of a neural network.\n",
    "# Dense: A standard, fully-connected layer where each neuron is connected to every neuron in the previous layer.\n",
    "# Input: A special layer used to define the shape and data type of the model's input.\n",
    "# Flatten: A layer that transforms a multi-dimensional input (like a 2D image) into a one-dimensional vector.\n",
    "# Normalization: A preprocessing layer that scales input data to a standard range (e.g., mean of 0, standard deviation of 1), which helps the model train faster. \n",
    "from tensorflow.keras.layers import Dense, Input, Flatten, Normalization, Input, Conv2D, MaxPooling2D, Dropout, BatchNormalization\n",
    "\n",
    "# Imports a utility function from scikit-learn, a popular library for traditional machine learning. train_test_split is used to split a single dataset into separate training and testing sets.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Imports the pyplot interface from Matplotlib, which is the most widely used library for creating plots and visualisations in Python. 'plt' is the standard alias.\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Imports the NumPy library, which is the foundation for numerical computing in Python. It provides support for large, multi-dimensional arrays and a wide range of mathematical functions. 'np' is the standard alias.\n",
    "import numpy as np\n",
    "\n",
    "# Imports a data scaling tool from scikit-learn. MinMaxScaler scales all data features to a specific range, usually 0 to 1.\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Imports tools for 'type hinting' from Python's typing module. Type hints make code more readable and can be used by external tools to check for errors.\n",
    "# Tuple: Used to hint that a variable or function return is a tuple (an ordered, immutable collection of elements).\n",
    "from typing import Tuple\n",
    "\n",
    "# Imports a specific type hint from NumPy's typing module.\n",
    "# NDArray: Used to hint that a variable is a NumPy n-dimensional array, which is more descriptive than a generic type.\n",
    "from numpy.typing import NDArray\n",
    "\n",
    "# Imports the 'os' module. This library provides a way for Python to interact with the computer's underlying operating system.\n",
    "# We use it for tasks like reading file names from a folder (os.listdir()) and constructing file paths that work correctly on any system, like Windows, Mac, or Linux (os.path.join()).\n",
    "import os \n",
    "\n",
    "# Imports the 're' module, which stands for Regular Expression. This is Python's library for advanced pattern matching in strings.\n",
    "# We use it to find and extract specific pieces of text from a string, like pulling the accuracy score out of a complex filename (e.g., finding '0.9935' in 'model_acc-0.9935.keras').\n",
    "import re\n",
    "\n",
    "# The seaborn dependency is used for plotting confusion matrices. \n",
    "import seaborn as sns\n",
    "\n",
    "# Wandb allows us to log the results of our experiments online at the WandB platform. This platform can be used to visualise plots of our models'\n",
    "# performance (accuracy, loss) per model, or for all models at once. Additionally, this platform keeps track of the models' architecture and hyperparameters used in the trainig process. \n",
    "import wandb\n",
    "\n",
    "# This import allows us to log the metrics generated by our models (Keras)\n",
    "from wandb.integration.keras import WandbMetricsLogger, WandbModelCheckpoint\n",
    "\n",
    "import datetime\n",
    "\n",
    "import visualkeras\n",
    "\n",
    "import warnings\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "from PIL import ImageFont\n",
    "\n",
    "from IPython.display import display # Needed to show plots inside a function\n",
    "\n",
    "import requests\n",
    "\n",
    "import yaml\n",
    "\n",
    "from types import SimpleNamespace\n",
    "\n",
    "import pickle\n",
    "\n",
    "from typing import Callable, Any\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.callbacks import History # Use the public API path\n",
    "from types import SimpleNamespace\n",
    "import pandas as pd\n",
    "from typing import Union, Optional, Dict, Any, List\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978ce86f",
   "metadata": {},
   "source": [
    "# Import and inspect the dataset\n",
    "\n",
    "### Define the training set features (X_train) and target variable (Y_train) as well as the test set features (X_test_) and target variable (Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05b32fe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New shape for X_train for CNN's: (60000, 28, 28, 1)\n",
      "New shape for X_test for CNN's: (10000, 28, 28, 1)\n",
      "Shape of X_train:\t (60000, 28, 28, 1)\n",
      "Shape of X_test:\t (10000, 28, 28, 1)\n",
      "Shape of Y_train:\t (60000,)\n",
      "Shape of Y_test:\t (10000,)\n",
      "X_train data type: float32\n",
      "X_test data type: float32\n",
      "Y_train data type: uint8\n",
      "Y_test data type: uint8\n",
      "\n",
      "X_train data 567-th element (a 28x28 pixel image):\n",
      " [[  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0. 198. 254. 254. 162. 161. 161. 161. 162.  78.  13.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0. 163. 253. 253. 254. 253. 253. 253. 254. 253. 234. 163.  68.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.  26. 181. 253. 230. 230. 154. 205. 230. 242. 253. 253. 241.  95.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  15. 128.   0.   0.   0.   0.   0.  38. 119. 253. 254. 115.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  83. 254. 255. 115.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   9. 174. 253. 216.  19.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. 164. 253. 244. 101.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. 119. 247. 253. 128.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  85. 254. 254. 146.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  81. 222. 254. 210.  21.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  89. 245. 253. 214.  29.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  26. 239. 253. 219.  25.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  51. 254. 254. 214.  25.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   9.  97. 247. 254. 227.  46.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   7.  99. 253. 253. 214.  29.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0. 104. 253. 253. 219.  25.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.  85. 255. 254. 146.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0. 106. 247. 241. 139.   4.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0. 157. 253. 142.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0. 174. 219.  25.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]]\n",
      "\n",
      "And its corresponding label:\t 7\n",
      "\n",
      "--- Train Dataset Distribution ---\n",
      "Digits:\t\t\t [0 1 2 3 4 5 6 7 8 9]\n",
      "Count per digit:\t [5923 6742 5958 6131 5842 5421 5918 6265 5851 5949]\n",
      "Average sample size:\t 6000.00\n",
      "Maximum sample size:\t 6742\n",
      "Minimum sample size:\t 5421\n",
      "\n",
      "--- Test Dataset Distribution ---\n",
      "Digits:\t\t\t [0 1 2 3 4 5 6 7 8 9]\n",
      "Count per digit:\t [ 980 1135 1032 1010  982  892  958 1028  974 1009]\n",
      "Average sample size:\t 1000.00\n",
      "Maximum sample size:\t 1135\n",
      "Minimum sample size:\t 892\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkQAAAHHCAYAAABeLEexAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjUsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvWftoOwAAAAlwSFlzAAAPYQAAD2EBqD+naQAAXfZJREFUeJzt3Qd4FNX6P/B3S5amAekoUoJcmoYgoCIXlSKI6FUBCzauFJELGuAvoFdEwIKCgqgoURH0Cljuz0pQpCig4DWJRDqoQUGRFiChhWyZ//M9y+zObjYhCcmWzPfzPMNmzw6z58ycmXn3nDMzFk3TNCEiIiIyMWukM0BEREQUaQyIiIiIyPQYEBEREZHpMSAiIiIi02NARERERKbHgIiIiIhMjwERERERmR4DIiIiIjI9BkRERERkegyIiCool8sl48aNkwsvvFCsVqvcfPPNEk2++eYbsVgs8t///jds3zlp0iT1naUxf/589X9/++23Un8//j/yEEuaNGki//znP0v1f6+55ho1EcUCBkRUof36668ybNgwSUhIkMqVK0t8fLx07txZZs2aJSdPnpRo8Oqrr6qTbVl76623ZPr06dK/f395++23ZfTo0YXOi5MWTtahppYtW0o00gMUfcL2Pf/886VXr17y0ksvydGjR2N22xU3mCzOZFb5+flqP2/Xrp3a72vUqCFt2rSR+++/X7Zt21bi5e3Zs0cFs5mZmeWSX4o8e6QzQFReUlNT5dZbb5VKlSrJvffeKxdffLE6SH777bcyduxY2bx5s7z++uuRzqY6qdauXbvUv8ILs3LlSrngggtk5syZxZq/YcOGMnXq1ALp1atXl2g2ZcoUadq0qTidTtm7d68KFkaNGiUzZsyQzz77TBITE33zTpgwQR555JFSfc8999wjd9xxh6pPpd12CMLt9rM/7LZq1Ur+85//BKQ9+uijcs4558hjjz0mZWn79u2qhbE0vvrqK4mUfv36yRdffCEDBgyQoUOHqvqBQGjx4sVy5ZVXljjQR0A0efJk1WKWlJRUbvmmyGFARBXSzp071cmrcePGKjBo0KCB77MRI0bIL7/8ogKmimz//v3qV3FxIfC5++67Jdb07t1bOnToEBAYYJvfcMMN8o9//EO2bt0qVapUUZ8hGCltQGKz2dR0NtCKVRbq1atXYFs9++yzKjgraht6PB71o6Ak+TAGgCXlcDgkEtLS0lTg8/TTT8u///3vgM9eeeUVOXLkSETyRdGNXWZUIU2bNk2OHTsmc+fODQiGdBdddJEkJycHjLd58sknpVmzZuoEgF+BOJCeOnWqWGNAgsdZ6N053333nYwZM0bq1Kkj1apVk1tuuUUOHDgQ8P/QUrVq1SpfF8eZxlwcP35c/t//+39qbBDy2qJFC3n++edF0zT1Oca4YDlff/21Wra+XLScnK3ff/9d/vWvf6nvRJBRq1Yt1QoXalwNTjropkMZkU+0QKGl7uDBgwVO0jhx4XOcqLt3764C1rPRrVs3efzxx1V+33333SLHEKHV5qGHHlLBxLnnnquCqD///LPAtg4eQ1SabRe8TD0/KC/qDwJYBKb33XefnDhx4qzWgf59I0eOlAULFqjuImyHL7/8Un2GOoOWEmxDbMv27duHHM9V2rodagyR3tX3wQcfFGubz549W3V3I3+XXXaZrFmzpljjktBVDugeD4agFmU2wvYeNGiQCjSxjrCu0OVszHfHjh3V39g2+vaORHcplR+2EFGF9Pnnn6sDKQ74xTFkyBA1zgbjbRBs/O9//1PdR2hd+Pjjj0udjwcffFDOO+88eeKJJ9SJ9MUXX1QnqPfff199jveYx9jVgYNyYRD04ISNYGfw4MGq6X7p0qWqCxAHdXSP4QSF7hSccBAU6t1g6GYpitvtLhCsAE5GOOHpv7zXrl2rWt9wMkOZXnvtNXWC2rJli1StWlXNh+/t0qWLWn840Vx66aVq2ejC+uOPP1TwYWzZQJfMww8/LDk5OSqYveuuu9Q2OBvo4kJQi24bdJkUBid7nKAx/xVXXKECnD59+pxx+SXddkW57bbbVLcfttWPP/4ob775ptStW1eee+45OVtoLUP5UO+w3hHgAMbXoC5hXaPV6L333lPBLVpWilP+M9XtohRnm6NeYXmoRwis8R24MADfibpXFLQMAwJBBEVFtQru27dPbXc9eMT+g6427F+5ubmq+xX7DrpmJ06cqMYgIU9Q3OMLxQiNqILJyclBU4l20003FWv+zMxMNf+QIUMC0h9++GGVvnLlSl8a3j/xxBMFltG4cWNt4MCBvvfz5s1T8/bo0UPzeDy+9NGjR2s2m007cuSIL61Nmzba1VdfXay8fvLJJ2q5Tz31VEB6//79NYvFov3yyy++NCwTyy4OzIvlhpqGDRvmm+/EiRMF/u+6devUfO+8844vbeLEiSrto48+KjC/vj6+/vprNU+rVq20U6dO+T6fNWuWSt+4cWORedbXcVpaWqHzVK9eXWvXrp3vPbad8bCXkZGh3o8aNSrg//3zn/8ssK3179u5c2epth0EL1PPz6BBgwLmu+WWW7RatWoVe7mF5QXLtlqt2ubNmwvMH7wt8/PztYsvvljr1q1bmdVt5MeYp+Juc3yG8nfs2FFzOp2++ebPn6/mO9M6R770Ol2vXj1twIAB2uzZs7Xff/+9wLyDBw/WGjRooB08eDAg/Y477lD1R19PqGdYHspPFRO7zKjCwa86QPdHcSxZskS9ovnfCC1FcDZjjfBr0thFg1+WaIlBV05pIK9o8kcXT3Becf7DL9vSQsvBsmXLCkz4hazTx+IABqlmZ2er7kd09aBlQ/d///d/0rZtW9WNEiy4ywpdEMaxJvqv76ysLDlbaL0p6mozvfsI3YDBrR/h9MADDwS8xzrAutXr8tm4+uqrpXXr1gXSjdvy8OHDqqUG32vcjuVVt8+0zdPT01X50bJnbN1BKxJaiM4E+ULL6VNPPaXmX7RokRo7iJaj22+/3TeGCPsM6uqNN96o/kYrpj7hakWsk+KuD4p97DKjCgeX2EJxL7vGARzN9zixG9WvX1+d6EsbvECjRo0C3usHc5yASgN5waXlwcGe3h12NnlFt1iPHj2KnAfjbdCtM2/ePNVFp49bApw8jGM4cJVPJNaREbru0PV0pm2P7iqj4LpQ3opaB3p9Lq3gsunQNYaAAZeRG8fKFfdS/bPZbmf6v3o9Dt4OCI70Lr8zwVggdGVi+uuvv1RXKLoJ0X0YFxenxpZhzBOCI1xtWtgVp7g4gcyBARFVODiBIGjYtGlTif7f2dyzBb+MQynsqiRjIBFL0HKCYAitRp06dVIDgLHeMKYIg6NLo7zWEcYqIUgLd3BTGuVZT4wtQToMTsb4oauuukrdOgAXHiBIwLZduHBhuec53PsFyoc6iiAdA6YRFGFAtF5ncWXewIEDQ/5f420bqGJjQEQVEi65xi++devWqRN3UdCMjgPjzz//HDDwGIMt8etRH6Cp/5INvmQXA1LxC7S0ShKIIS/Lly9XrV/GViL9RnPGvJYHXIWEE8cLL7zgS8vLyyuwTnC1XkkD0rKm36cHXR9n2va4TUPz5s196cW9yi1Wb3yIbiJc3YVuJeNl9QiIooFej7EdunbtGnA1KAZXlzZIQdCH/4t9Hd1iGECN/Qg/aM7UOhqr25qKj2OIqELCIyvQBYSrxxDYBEOXDprP4frrr1evuErGCDf2A+MVNzjRr169OmA+BF6FtRAVB/JZ3PuiIK/4LtxLxQhXl+GAjXvylCf8sg/+Ff/yyy8XKD9+if/0008hr9ALR+sYrqzCbRTQXYRxJ4XRgyW0kgSXqay3XTTBdkR9MW43BBqffPKJRAPcVwqXxr/xxhsqCNLhqrHidMkh4Nm1a1eBdGwr/EjCDxsEQ1gPqKsIEEMF8MbbCOhXWsbi9qbiYQsRVUgIXND0jwGUaPUx3qkal41/+OGHvnurYPAvWj0Q2OBgh0GoP/zwg7oMH5f5Gn+hIsDCAFgcRK+99lp10sevbONl5CWF+7/gEmOM50D3Dsa84D46oWDwJ/KDcRE4gSHvuKz8008/Vd1YKHdpoXvJeM8eI/1mf2h5Q8sLusowUBcnF7RYBd/XBbcBQGsSLuPGZfco46FDh9Rl93PmzFH5LisYSI4WMpw4EfwiGMJgcLQy4PuKugkh8oVtiWAYg3j1y+537NhRrFaBkmy7aIIgHwH/ddddJ3feeacaJ4N7/qAMGzZsiHT21IBr3KMJXbRYn7gtAeo7urlQx8+0XbBfolz4gYAB2zVr1lRj3rBP447T2N56tx1uAYDbWFx++eVqEDfqNeoqBlOjbuNvwPdiTCHqL1qVECDh/xQ2RotiUKQvcyMqTzt27NCGDh2qNWnSRHM4HNq5556rde7cWXv55Ze1vLw833y4tHfy5Mla06ZNtbi4OO3CCy/UHn300YB5wO12a+PHj9dq166tVa1aVevVq5e61L2wS5ODLwnXLzvGq27v3r1anz59VN6Kc0nx0aNH1SXO559/vspr8+bNtenTpwdcAl2Wl90bDxOHDx/W7rvvPlX+c845R5V/27ZtBcoP2dnZ2siRI7ULLrhArfuGDRuqefTLm/V18eGHHwb8P1zWXpzLm/V1rE/4jvr162vXXnutuow7Nze3wP8Jvuwejh8/ro0YMUKrWbOmKtPNN9+sbd++Xc337LPPFnnZfUm3XWGX3R84cCBk2YzfVdrL7lG2UObOnavqTqVKlbSWLVuq7wy1fs6mbhd22X1xt/lLL72kvh95vOyyy7TvvvtOa9++vXbdddcVuS727dunth2+G5fU2+127bzzzlO3FPjvf/8bcn6sJ+z32KdQj7p37669/vrrAfN9+umnWuvWrdXyeAl+xWPBP5EOyoiIogmuvMJDQdFiVlSXG4UXxnuhq6tv376qO42oLHEMERGZGm4lEAxdKrgcH1dhUWRgsH7w7/V33nlHdWGd6dEdRKXBMUREZGp4bERGRoYam4X73GBMEibceBDPi6PI+P7779UjOzAODWPUMKYHzybEWECkEZU1dpkRkalhAPbkyZPVs9hwI0fcNBDPNcPA9aKegUXlC4OocUd2XOCAViEMjMZVlhgEXdTNNolKiwERERERmR7HEBEREZHpMSAiIiIi02MHeTEv9cTNvHAzLt6+nYiIKDZgVBAedYTnW+LK0TPNHDG44Vaom8D961//Up+fPHlS/Y0bplWrVk3r27evuhGa0e+//65df/31WpUqVbQ6depoDz/8sLrJnhFuBtauXTt147ZmzZqV+GZau3fvLvKmdZw4ceLEiRMnidoJ5/EziWgLUVpaWsCzdPAsGTwOQb+kEpdcpqamqscs4FEBI0eOVDfk+u6779Tn+L+4BX39+vXV4xjwgE08ogEP8HvmmWfUPHhoI+bB4xbwHJwVK1aoxy/g6cdFPfTRSH+I5u7du9WT1ImIiCj65ebmqttnGB+GHRNXmeFZTIsXL1YP5kMhcEdSPI+qf//+6nM8rwjPpcLzk/DMIdwrBM9WQndWvXr11Dx4zsz48ePVQ/nwPBz8jaDK+OC+O+64Qz2z6ssvvyxWvpAXBGR41hMDIiIiothQkvN31IwhwkM3cZv8MWPGqHE6uFGa0+mUHj16+OZp2bKlukeIHhDh9ZJLLvEFQ4BWn+HDh8vmzZvVrfcxj3EZ+jwIvgpz6tQpNRlXKODhkfqTl9EXiQnjizDp9HS0XhljzcLS9adOG5/orKdD8FPEC0vH/VKwXGM6lov5g/NYWDrLxDKxTCwTy8QyVbQyFVfUBESffPKJarXRn0C+d+9e1cKDpwsbIfjBZ/o8xmBI/1z/rKh5EOTglv1VqlQpkJepU6eqG7UFW79+vXrCMaD1Ck8/RpccWqN0DRs2VBOelo2IVJeQkKBuJoaWKuOjAhDkoYxYtnHDJSYmqvKnp6cH5KFDhw4qeDQ+kRqVp2PHjur70IqmQ9nwVPGDBw9KVlaWLx3RMlra0LL2xx9/+NJZJpaJZWKZWCaWqSKVCTdcLa6o6TJDqw1W8Oeff67eo6vsvvvuC2ipgcsuu0zdYv+5555Tt9b//fffZenSpb7PT5w4oYKWJUuWSO/eveVvf/ubWs6jjz7qmwefYVwR5g0VEIVqIUIfZHZ2tq/JjRE4y8QysUwsE8vEMkV3mQ4fPqzuch4zXWYIapYvXy4fffSRLw0DpRFpotXI2Eq0b98+9Zk+D27rboTP9c/0Vz3NOA9WTKhgCCpVqqSmYNiowbfy11d6ML1iFDe9sEcElCQdFSlUemF5LGk6y8QyFZbOMrFMReWdZWKZLFFWpqi9MeO8efNU0xZabXTt27dXV4vhqjDd9u3bZdeuXdKpUyf1Hq8bN26U/fv3BzyXCMFO69atffMYl6HPoy+DiIiIKOIBEZq+EBANHDgwIHpEP+LgwYPVIOuvv/5aDbJG1xcCGQyohp49e6rABw9i/Omnn1TX2YQJE2TEiBG+Fh5cbo/+yXHjxqn+y1dffVU++OADdUk/ERERUVR0maGrDK0+gwYNKvDZzJkzVdNYv3791JgejDNCQGNsCsNl+riqDIESxg4hsJoyZYpvnqZNm6rL7hEAzZo1Sw2+evPNN4t9DyIiIiKq+KJmUHU0432IiIiIKvb5O+JdZkRERESRxoCIiIiITI8BEREREZkeAyIiIiIyPQZEREREZHoMiIiIiMj0In4fIopduH8UHswXTWrXri2NGjWKdDaIiCjGMCCiUgdDLVq0kry8ExJNKleuKtu3b2VQREREJcKAiEoFLUPeYOhdEWkl0WGr5OXdrfLGgIiIiEqCARGdJQRDl0Y6E0RERGeFg6qJiIjI9BgQERERkekxICIiIiLTY0BEREREpseAiIiIiEyPARERERGZHgMiIiIiMj0GRERERGR6DIiIiIjI9BgQERERkekxICIiIiLTY0BEREREpseAiIiIiEyPARERERGZHgMiIiIiMj0GRERERGR6DIiIiIjI9BgQERERkekxICIiIiLTY0BEREREpseAiIiIiEzPHukMEBER7dq1Sw4ePCjRpHbt2tKoUaNIZ4PChAERERFFPBhq0aKV5OWdkGhSuXJV2b59K4Mik2BAREREEYWWIW8w9K6ItJLosFXy8u5WeWNAZA4MiIiIKEogGLo00pkgk+KgaiIiIjI9BkRERERkegyIiIiIyPQ4hogoRvCyZCKi8sOAiCgG8LJkIqLyxYCIKAbwsmQiogo+hujPP/+Uu+++W2rVqiVVqlSRSy65RNLT032fa5omEydOlAYNGqjPe/ToIT///HPAMg4dOiR33XWXxMfHS40aNWTw4MFy7NixgHk2bNggXbp0kcqVK8uFF14o06ZNC1sZicr+suRomKIlMCMiivGA6PDhw9K5c2eJi4uTL774QrZs2SIvvPCCnHfeeb55ELi89NJLMmfOHPnf//4n1apVk169ekleXp5vHgRDmzdvlmXLlsnixYtl9erVcv/99/s+z83NlZ49e0rjxo0lIyNDpk+fLpMmTZLXX3897GUmIiKi6BPRLrPnnntOtdbMmzfPl9a0adOA1qEXX3xRJkyYIDfddJNKe+edd6RevXryySefyB133CFbt26VL7/8UtLS0qRDhw5qnpdfflmuv/56ef755+X888+XBQsWSH5+vrz11lvicDikTZs2kpmZKTNmzAgInIiIiMicIhoQffbZZ6q159Zbb5VVq1bJBRdcIP/6179k6NCh6vOdO3fK3r17VTeZrnr16nL55ZfLunXrVECEV3ST6cEQYH6r1apalG655RY1z1VXXaWCIR2+FwEZWqmMLVJw6tQpNRlbmMDlcqkJsHxMHo9HTTo93e12q4DuTOk2m00sFotvucZ0wPzFSbfb7Wq5xnQsF/MH57Gw9JKUCZ975xWJi3OLxeIvk8uFZVvE4Qgsk9NpE/x3hyMw7/n5WAfe5QSm28Vq1cRu96drmkUtx2r1iN3uCUr3rh/krSJuJ2/99f7tcllV+bEedG438m0NsT1Qfms5bCd8t3efCmfdi/btxDKVvEx6/XY6NTVf+R0jbGKzedSkw74Ren/CuvLWbeO6N/N2itUyxURAlJWVJa+99pqMGTNG/v3vf6tWnoceekjtGAMHDlTBEKBFyAjv9c/wWrdu3QIrv2bNmgHzGFuejMvEZ8EB0dSpU2Xy5MkF8rt+/XrVZQd16tSRZs2aqaDtwIEDvnkaNmyoph07dkhOTo4vPSEhQeVz06ZNcvLkSV96y5YtVUCHZRs3XGJioloPxvFUgMAPrV0YE6VD5enYsaP6vm3btvnSMeaqbdu2atAr1rUxqGzVqpXs2bNH/vjjD196ScqEV+QxM1Nk0KBNUru2v0yLFrWUrKwakpy8PuDAlpKSKLm5Dhk7NrBM06d3kPj4fBk2bEPAAXD69I7SpEmODBjgL9PBg1UkJaWtJCYelD59/GXKyqouixaJ6oLNzs72rbeKsp1QprFjx4pItmRm7pTU1GbSq9dOSUryl2nNmoayenVD6d9/hyQk+MuUmpogmZl1y2E75Uh+frL3rzDWvWjeTixT6cqE/4P6nZLiltxcdzkeI1pJ5857pEsXf5kyM+sUsj85ZfVq7w9i4zo283aKxTJhKE5xWTRjSBVmWKFYgWvXrvWlISBCYIRWHaTjBIcVg0HVuttuu01Fke+//74888wz8vbbb8v27dsDlo0VgaBm+PDhavwQAqKUlBTf51hJ6DrDK1b+mVqI0LWHkxIGboPZI3B0OaKlzuNJk7i4tlHSQvST2GyXyffffy9JSUkVajv9+OOPal8Q+U48nkujpIUoU0Q6q321Xbt2/EXLMpW6TDieoH47nWtF0y6Nkhain8Tt7qBO+jhxczvFZpnQC4QGEgRL+vk7KluIEOS0bt06IA3Byf/93/+pv+vXr69e9+3bFxAQ4b1+wsM8+/fvD1gGNh6uPNP/P17xf4z09/o8RpUqVVJTMGxUTEb6Sg+mV4zipgcvtzTpqEih0gvLY0nTjXnXKybgIBMKDlbFTUc9DpWOE3bodKvk5xfMI3YK5K2o7RTNNzgsanvgl5f3OgjvPDiIh7ouorDtUfbbCd+dH/a6VxH3J7OXyV+/LWoq32OE94dDsIL7k6XIdRbr2ymaj4PhqHtRFxDhF0Fwyw6avXA1GKBVBwHLihUrfAEQWmswNggtP9CpUyc5cuSIunqsffv2Km3lypXqZI0WDH2exx57TJxOp7qiDXBFWosWLQp0l0VCtFfMioQ3OCQis+NxMAoDotGjR8uVV16pur3QDfbDDz+oS+H1y+ERJY8aNUqeeuopad68uQqQHn/8cXXl2M033+xrUbruuuvUQGxcmo+gZ+TIkWrANeaDO++8U3Wf4f5E48ePV/2ks2bNkpkzZ0qksWKGF29wSBUdf2DRmfA4GIUBEQZaffzxx/Loo4/KlClTVMCDy+xxXyHduHHj5Pjx4+ryeLQE/f3vf1eX2eMGizpcVo8gqHv37qoprV+/fureRcZBWl999ZWMGDFCtSJh58TNHqPhkntWzEjf4JCo4uAPLCoZHgej6tEdN9xwg5oKg1YiBEuYCoMBUwsXLizyezAqfs2aNRK9WDGJ6OzwBxZRDAdERERU1vgDK1zYRVlxMCAionLFEwZVVOyirFgYEBFRueEJgyoydlFWLAyIiKjc8IRB5sAuyoqAARERhQFPGEQU3Qre7pGIiIjIZBgQERERkekxICIiIiLTY0BEREREpseAiIiIiEyPARERERGZHgMiIiIiMj0GRERERGR6DIiIiIjI9BgQERERkekxICIiIiLTY0BEREREpseAiIiIiEyPARERERGZHgMiIiIiMj0GRERERGR6DIiIiIjI9BgQERERkenZI50BIqJotGvXLjl48KBEk9q1a0ujRo0inQ2iCokBERFRiGCoRYtWkpd3QqJJ5cpVZfv2rQyKiMoBAyIioiBoGfIGQ++KSCuJDlslL+9ulTcGRERljwEREVGhEAxdGulMEFEYcFA1ERERmR4DIiIiIjI9BkRERERkegyIiIiIyPQYEBEREZHpMSAiIiIi02NARERERKbHgIiIiIhMjwERERERmR4DIiIiIjI9BkRERERkegyIiIiIyPQYEBEREZHpMSAiIiIi04toQDRp0iSxWCwBU8uWLX2f5+XlyYgRI6RWrVpyzjnnSL9+/WTfvn0By9i1a5f06dNHqlatKnXr1pWxY8eKy+UKmOebb76RSy+9VCpVqiQXXXSRzJ8/P2xlJCIiougX8RaiNm3ayF9//eWbvv32W99no0ePls8//1w+/PBDWbVqlezZs0f69u3r+9ztdqtgKD8/X9auXStvv/22CnYmTpzom2fnzp1qnq5du0pmZqaMGjVKhgwZIkuXLg17WYmIiCg62SOeAbtd6tevXyA9JydH5s6dKwsXLpRu3bqptHnz5kmrVq3k+++/lyuuuEK++uor2bJliyxfvlzq1asnSUlJ8uSTT8r48eNV65PD4ZA5c+ZI06ZN5YUXXlDLwP9H0DVz5kzp1atX2MtLRERE0SfiLUQ///yznH/++ZKQkCB33XWX6gKDjIwMcTqd0qNHD9+86E5r1KiRrFu3Tr3H6yWXXKKCIR2CnNzcXNm8ebNvHuMy9Hn0ZRARERFFtIXo8ssvV11cLVq0UN1lkydPli5dusimTZtk7969qoWnRo0aAf8HwQ8+A7wagyH9c/2zouZB0HTy5EmpUqVKgXydOnVKTTrMCxibpI9PslqtavJ4PGrS6enoztM07Yzp+t8OB5bhH/vkdNoEHzkc7oC85efbxGIRiYsLTreL1aqJ3e5P1zSLWo7V6hG73VMg3WbzqEnn8VjF5bKK3a6J1epQ5UJ5Q+Udn3nL782LxeIvk8tlE4/HIg5H4Fiu8i+TiM1m8+U71HbChHrldmvidofKO+a1hrlMmso7Xo3j3zCmTi+Pnm8R79/e7YRt4F8Hbje2kTWMZcJ3I0/evKN+BOcd6Xq+Ub/PXPfCUSZvvrEe9fpszLsO9cZbx1xnuT+VVZn8+UY9wfoNzrt+LML/j4tzleMxoiRlQp2JC9gvkXfUEf29Xr+dTu10nYnEcS+4TN5jRPB+aTweGvdLl0uL0HEvuEw4RnhP7YWdn4L3y8gc9yxBZfLWb+M5JlTeS3POjYmAqHfv3r6/ExMTVYDUuHFj+eCDD0IGKuEydepUFZwFW79+vVSrVk39XadOHWnWrJkao3TgwAHfPA0bNlTTjh07VLefDi1gGPSNYA+BWLDk5EPicKT73qekJEpurkPGjvWnwfTpHSQ+Pl+GDdsQUAmnT+8oTZrkyIAB23zpBw9WkZSUtpKYeFD69MnypWdlVZdFi1pJ5857pEuXP3zpmZl1JDW1mfTqdUySksZKdna2pKenhywTXrHNMjNFBg3aJLVr+8u0aFFLycqqIcnJ6wN2mPIvk0jnzp19+Q61nZBvDLxfs+aErF4t0r//DklI8G+n1NQEycysG+YyHZOUFG8grucbqlevrrp4MXYOZUK+RbIlM3Pn6e20U5KS/HVvzZqGsnp1wzCWKUfy85N99WHbNn+ZsP+2bdtWlUnPt0h6MepeOMqE5XjrNw6WGIO4YYN/O+FkjalJkyYyYIA332e3P5VVmXCM8O+X2P9wUjPWGWwHpMXHu2XYsPRyPEaUpEw4RgwK2C/R2o8fuzimYhvo+2VKiltyc90ROu4Fl8mpjhH4QWxcx8ZjuXG/XLQoJ0LHveAy5Uhmpnc4SGHnJ5TJuF9G5rhXJahM3v1Sb4DAce+PP/zbqbTnXAyrKTYtynTo0EF75JFHtBUrViDU0w4fPhzweaNGjbQZM2aovx9//HGtbdu2AZ9nZWWp//fjjz+q9126dNGSk5MD5nnrrbe0+Pj4QvOQl5en5eTk+Kbdu3erZWZnZ2tOp1NNbrdbzYtXPc2Y7nK5ipWenp6ulu1wpGkOh9M3WSweTcQTkIYJafisYLqmWa2B6XFxrtPp7pDpNltgut3uVul2e7rmcDi0tLS0QvOOz6wI+SVDLc+4HOQDywnOY/mXKUOz2Wy+fIfaTvgMZbPZsN61EHn3roPwlslbB1AXjHUD61vPu55v1BP/dgpcB9ie4S2TN08ZGRmax+MJmXeUSc938epeOMrkzTfWKfIdnHdMKBPqt3G/LP3+VFZl8ucbeQyVd3yGumSxpJfzMaIkZcIxIi5gv0S+wZhvlA35jtxxL7hMofdL4/HQuF9G7rgXXCYcI+yqDhd2fgreLyNz3HMFlcm7LpG3os6tJT3nHjp0SG1HnMvPJOKDqo2OHTsmv/76q9xzzz3Svn171cy6YsUKdbk9bN++XY0x6tSpk3qP16efflr279+vIkFYtmyZxMfHS+vWrX3zLFmyJOB7MI++jFBweT6mUAPAMRnpzXLB9ObsM6Wj2Rjy87GMgpsDTY3B0CoYKh1Nm6HTraeXH0hv5g7mciFP+ae7DOwh8643XQKaPUMJlZfyL5O7QL71/OoTWgRELEXmPbxlsvjqQnC+jXn35htl9pYbzdyhhgGGr0z47vwi8450f77txah74SiTN99Yp/r+FyrvqN+h9suS709lVSZ/vo35Nf6tH4vQHVG+x4iSlskZcr/U3wful5YIHfeCy1T0fonjYeB+aYngcc9YJrwGDhko7n7pjOix3Fu/9X2ysLyf7Tk3agdVP/zww+py+t9++01dNn/LLbeozA8YMEB1FwwePFjGjBkjX3/9tRpkfd9996lABleYQc+ePVXggwDqp59+UpfST5gwQd27SA9oHnjgAcnKypJx48apJv1XX31Vdcnhkn4iIiIiiGgLEfoHEfygHxb9g3//+9/VJfX4G3BpPCJBtBBhLAKuDkNAo0PwtHjxYhk+fLgKlDC+Z+DAgTJlyhTfPLjkPjU1VQVAs2bNUn2Nb775Ji+5JyIiougIiN57770iP69cubLMnj1bTYXBIOzgLrFg11xzjRq8R0RERBSV9yEiIiIiijQGRERERGR6DIiIiIjI9BgQERERkekxICIiIiLTY0BEREREpseAiIiIiEyPARERERGZHgMiIiIiMj0GRERERGR6DIiIiIjI9BgQERERkekxICIiIiLTY0BEREREpseAiIiIiEyPARERERGZHgMiIiIiMj0GRERERGR6DIiIiIjI9BgQERERkekxICIiIiLTY0BEREREpseAiIiIiEyPARERERGZHgMiIiIiMj0GRERERGR6DIiIiIjI9BgQERERkekxICIiIiLTY0BEREREpseAiIiIiEyPARERERGZHgMiIiIiMj0GRERERGR6DIiIiIjI9BgQERERkekxICIiIiLTY0BEREREpseAiIiIiEyPARERERGZHgMiIiIiMj0GRERERGR6URMQPfvss2KxWGTUqFG+tLy8PBkxYoTUqlVLzjnnHOnXr5/s27cv4P/t2rVL+vTpI1WrVpW6devK2LFjxeVyBczzzTffyKWXXiqVKlWSiy66SObPnx+2chEREVH0i4qAKC0tTVJSUiQxMTEgffTo0fL555/Lhx9+KKtWrZI9e/ZI3759fZ+73W4VDOXn58vatWvl7bffVsHOxIkTffPs3LlTzdO1a1fJzMxUAdeQIUNk6dKlYS0jERERRa+IB0THjh2Tu+66S9544w0577zzfOk5OTkyd+5cmTFjhnTr1k3at28v8+bNU4HP999/r+b56quvZMuWLfLuu+9KUlKS9O7dW5588kmZPXu2CpJgzpw50rRpU3nhhRekVatWMnLkSOnfv7/MnDkzYmUmIiKi6GKPdAbQJYYWnB49eshTTz3lS8/IyBCn06nSdS1btpRGjRrJunXr5IorrlCvl1xyidSrV883T69evWT48OGyefNmadeunZrHuAx9HmPXXLBTp06pSZebm6te0RWnd8dZrVY1eTweNen0dLReaZp2xnT9b4cDy/B39TmdNsFHDoc7IG/5+TaxWETi4oLT7WK1amK3+9M1zaKWY7V6xG73FEi32Txq0nk8VnG5rGK3a2K1OlS5UN5Qecdn3vJ782Kx+MvkctnE47GIwxHYdVn+ZRKx2Wy+fIfaTpgcDoe43Zq43aHyjnmtYS6TpvKOV2N3L7qQ9fLo+Rbx/u3dTtgG/nXgdmMbWcNYJnw38uTNO+pHcN6Rrucb9fvMdS8cZfLmG+tRr8/GvOtQb7x1zHWW+1NZlcmfb9QTrN/gvOvHIvz/uDhXOR4jSlIm1Jm4gP0SeUcd0d/r9dvp1E7XmUgc94LL5D1GBO+XxuOhcb90ubQIHfeCy4RjhPfUXtj5KXi/jMxxzxJUJm/9Np5jQuW9NOfcmAiI3nvvPfnxxx9Vl1mwvXv3qg1Wo0aNgHQEP/hMn8cYDOmf658VNQ+CnJMnT0qVKlUKfPfUqVNl8uTJBdLXr18v1apVU3/XqVNHmjVrprrkDhw44JunYcOGatqxY4dq5dIlJCSoMU6bNm1S3xssOfmQOBzpvvcpKYmSm+uQsWP9aTB9egeJj8+XYcM2BFTC6dM7SpMmOTJgwDZf+sGDVSQlpa0kJh6UPn2yfOlZWdVl0aJW0rnzHunS5Q9femZmHUlNbSa9eh2TpKSxkp2dLenp6SHLhFd0cWZmigwatElq1/aXadGilpKVVUOSk9cH7DDlXyaRzp07+/Idajsh3xhntmbNCVm9WqR//x2SkODfTqmpCZKZWTfMZTomKSneQFzPN1SvXl21aqKrGGVCvkWyJTNz5+nttFOSkvx1b82ahrJ6dcMwlilH8vOTffVh2zZ/mbBftW3bVpVJz7dIejHqXjjKhOV46zcOlmhN3rDBv51wssbUpEkTGTDAm++z25/Kqkw4Rvj3S+x/OEYa6wy2A9Li490ybFh6OR4jSlImHCMGBeyX+HGLYzuOqdgG+n6ZkuKW3Fx3hI57wWVyqmMEzhXGdWw8lhv3y0WLciJ03AsuU45kZvZSfxV2fkKZjPtlZI57VYLK5N0v9QYIHPf++MO/nUp7zkUvUrFpEbJr1y6tbt262k8//eRLu/rqq7Xk5GT194IFCzSHw1Hg/3Xs2FEbN26c+nvo0KFaz549Az4/fvw4QkRtyZIl6n3z5s21Z555JmCe1NRUNc+JEydC5i0vL0/LycnxTbt371bzZ2dna06nU01ut1vNi1c9zZjucrmKlZ6enq6W7XCkaQ6H0zdZLB5NxBOQhglp+KxguqZZrYHpcXGu0+nukOk2W2C63e5W6XZ7ulr3aWlpheYdn1kR8kuGWp5xOcgHlhOcx/IvU4Zms9l8+Q61nfAZymazYb1rIfLuXQfhLZO3DqAuGOsG1reedz3fqCf+7RS4DrA9w1smb54yMjI0j8cTMu8ok57v4tW9cJTJm2+sU+Q7OO+YUCbUb+N+Wfr9qazK5M838hgq7/gMdcliSS/nY0RJyoRjRFzAfol8gzHfKBvyHbnjXnCZQu+XxuOhcb+M3HEvuEw4RthVHS7s/BS8X0bmuOcKKpN3XSJvRZ1bS3rOPXTokNqOOJefScRaiNAltn//fnX1lw6/FFavXi2vvPKKGvSMX25HjhwJaCXCVWb169dXf+P1hx9+CFiufhWacZ7gK9PwPj4+PmTrEOBqNEzB7Ha7moz0ZrlgenP2mdLRbAz5+VhGwc2BpsZgaBUMlY6mzdDp1tPLD6Q3cwdzuZCn/NNdBvaQedebLgHNnqGEykv5l8ldIN96fvXJO77MUmTew1smi68uBOfbmHdvvlFmb7nRzB1qGGD4yoTvzi8y70j359tejLoXjjJ58411qu9/ofKO+h1qvyz5/lRWZfLn25hf49/6sQjdEeV7jChpmZwh90v9feB+aYnQcS+4TEXvlzgeBu6Xlgge94xlwmvgkIHi7pfOiB7LvfVb3ycLy/vZnnOjclB19+7dZePGjerKL33q0KGDGmCt/41+5xUrVvj+z/bt29Vl9p06dVLv8YplILDSLVu2TAU7rVu39s1jXIY+j74MIiIiooi1EJ177rly8cUXB6RhfA7uOaSnDx48WMaMGSM1a9ZUQc6DDz6oAhkMqIaePXuqwOeee+6RadOmqfFCEyZMUAO19RaeBx54QLU4jRs3TvVjr1y5Uj744ANJTU2NQKmJiIgoGkX8KrOi4NJ4NI3hhowYnImrw1599dWAprDFixerq8oQKCGgGjhwoEyZMsU3Dy65R/CDexrNmjVLDb5688031bKIiIiISh0QYfQ2rgxDa44RxvtgTFBWln8UfEngjtJGlStXVvcUwlSYxo0by5IlS4pc7jXXXKOuZiAiIiIqszFEv/32W8hr+9GK8+eff5ZmkURERESx0UL02Wef+f7GVWC4T4oOARIGL+PeHUREREQVNiC6+eab1Ssui8NYHSNcEYZgCI/IICIiIqqwAZF+3xkMVMYYotq1a5dXvoiIiIiie1A1bp1NREREJGa/7B7jhTDhpojGB63BW2+9VRZ5IyIiIoregAgPPsW9fnA36QYNGvhutU1ERERkmoBozpw5Mn/+fHWHaCIiIiJT3ocID4W78soryz43RERERLESEA0ZMkQWLlxY9rkhIiIiipUus7y8PHn99ddl+fLlkpiYqO5BZDRjxoyyyh8RERFRdAZEGzZskKSkJPX3pk2bAj7jAGsiIiIyRUD09ddfl31OiIiIiGJpDBERERGRmL2FqGvXrkV2ja1cufJs8kREREQU/QGRPn5I53Q6JTMzU40nCn7oKxEREVGFDIhmzpwZMn3SpEly7Nixs80TERERUeyOIbr77rv5HDMiIiIyd0C0bt06qVy5clkukoiIiCg6u8z69u0b8F7TNPnrr78kPT1dHn/88bLKGxEREVH0BkTVq1cPeG+1WqVFixYyZcoU6dmzZ1nljYiIiCh6A6J58+aVfU6IiIiIYikg0mVkZMjWrVvV323atJF27dqVVb6IiIiIojsg2r9/v9xxxx3yzTffSI0aNVTakSNH1A0b33vvPalTp05Z55OIiIgouq4ye/DBB+Xo0aOyefNmOXTokJpwU8bc3Fx56KGHyj6XRERERNHWQvTll1/K8uXLpVWrVr601q1by+zZszmomoiIiMzRQuTxeCQuLq5AOtLwGREREVGFD4i6desmycnJsmfPHl/an3/+KaNHj5bu3buXZf6IiIiIojMgeuWVV9R4oSZNmkizZs3U1LRpU5X28ssvl30uiYiIiKJtDNGFF14oP/74oxpHtG3bNpWG8UQ9evQo6/wRERERRVcL0cqVK9XgabQEWSwWufbaa9UVZ5g6duyo7kW0Zs2a8sstERERUaQDohdffFGGDh0q8fHxIR/nMWzYMJkxY0ZZ5o+IiIgougKin376Sa677rpCP8cl97h7NREREVGFDYj27dsX8nJ7nd1ulwMHDpRFvoiIiIiiMyC64IIL1B2pC7NhwwZp0KBBWeSLiIiIKDoDouuvv14ef/xxycvLK/DZyZMn5YknnpAbbrihLPNHREREFF2X3U+YMEE++ugj+dvf/iYjR46UFi1aqHRceo/HdrjdbnnsscfKK69EREREkQ+I6tWrJ2vXrpXhw4fLo48+KpqmqXRcgt+rVy8VFGEeIiIiogp9Y8bGjRvLkiVL5PDhw/LLL7+ooKh58+Zy3nnnlU8OiYiIiKLxTtWAAAg3YyQiIiIy5bPMiIiIiCqSiAZEr732miQmJqo7X2Pq1KmTfPHFF77PcTXbiBEjpFatWnLOOedIv3791L2QjHbt2iV9+vSRqlWrSt26dWXs2LHicrkC5vnmm2/k0ksvlUqVKslFF10k8+fPD1sZiYiIKPpFNCBq2LChPPvss+ru1unp6dKtWze56aabZPPmzerz0aNHy+effy4ffvihrFq1Svbs2SN9+/b1/X9c1YZgKD8/Xw32fvvtt1WwM3HiRN88O3fuVPN07dpVMjMzZdSoUTJkyBBZunRpRMpMREREFWgMUVm48cYbA94//fTTqtXo+++/V8HS3LlzZeHChSpQgnnz5kmrVq3U51dccYV89dVXsmXLFlm+fLm6ui0pKUmefPJJGT9+vEyaNEkcDofMmTNHmjZtKi+88IJaBv7/t99+KzNnzlRXxhERERFFNCAyQmsPWoKOHz+uus7QauR0OqVHjx6+eVq2bCmNGjWSdevWqYAIr5dccknApf4IcnBbALQytWvXTs1jXIY+D1qKCnPq1Ck16XJzc9UruuL07jir1aomj8ejJp2ejvLotyUoKl3/2+HAMvxdfU6nTfCRw+EOyFt+vk0sFpG4uOB0u1itmtjt/nRNs6jlWK0esds9BdJtNo+adB6PVVwuq9jtmlitDlUulDdU3vGZt/zevFgs/jK5XDbxeCzicAR2XZZ/mURsNpsv36G2EyYEym63Jm53qLxjXmuYy6SpvOPV2N2L21no5dHzLeL927udsA3868DtxjayhrFM+G7kyZt31I/gvCNdzzfq95nrXjjK5M031qNen41516HeeOuY6yz3p7Iqkz/fqCdYv8F5149F+P9xca5yPEaUpEyoM3EB+yXyjjqiv9frt9Opna4zkTjuBZfJe4wI3i+Nx0PjfulyaRE67gWXCccI76m9sPNT8H4ZmeOeJahM3vptPMeEyntpzrkxExBt3LhRBUAYL4RxQh9//LG0bt1adW9hg9WoUSNgfgQ/e/fuVX/jNfi+R/r7M82DIAd3165SpUqBPE2dOlUmT55cIH39+vVSrVo19XedOnWkWbNmqkvO+Pw2tGxh2rFjh+Tk5PjSExIS1BgnPPoE3xssOfmQOBzpvvcpKYmSm+uQsWP9aTB9egeJj8+XYcM2BFTC6dM7SpMmOTJgwDZf+sGDVSQlpa0kJh6UPn2yfOlZWdVl0aJW0rnzHunS5Q9femZmHUlNbSa9eh2TpKSxkp2drboyQ5UJrxj/lZkpMmjQJqld21+mRYtaSlZWDUlOXh+ww5R/mUQ6d+7sy3eo7YR8Y5zZmjUnZPVqkf79d0hCgn87paYmSGZm3TCX6ZikpHgDcT3fUL16ddWiia5ilAn5FsmWzMydp7fTTklK8te9NWsayurVDcNYphzJz0/21QfcoFWH/apt27aqTHq+RdKLUffCUSYsx1u/cbBElzseO6TDyRpTkyZNZMAAb77Pbn8qqzLhGOHfL7H/4RhprDPYDkiLj3fLsGHp5XiMKEmZcIwYFLBf4sctju04pmIb6PtlSopbcnPdETruBZfJqY4ROFcY17HxWG7cLxctyonQcS+4TDmSment/Sjs/IQyGffLyBz3qgSVybtf6g0QOO798Yd/O5X2nItepGLTIuzUqVPazz//rKWnp2uPPPKIVrt2bW3z5s3aggULNIfDUWD+jh07auPGjVN/Dx06VOvZs2fA58ePH0eIqC1ZskS9b968ufbMM88EzJOamqrmOXHiRMg85eXlaTk5Ob5p9+7dav7s7GzN6XSqye12q3nxqqcZ010uV7HSUW4s2+FI0xwOp2+yWDyaiCcgDRPS8FnBdE2zWgPT4+Jcp9PdIdNttsB0u92t0u32dLXu09LSCs07PrMi5JcMtTzjcpAPLCc4j+VfpgzNZrP58h1qO+EzlM1mw3rXQuTduw7CWyZvHUBdMNYNrG8973q+UU/82ylwHWB7hrdM3jxlZGRoHo8nZN5RJj3fxat74SiTN99Yp8h3cN4xoUyo38b9svT7U1mVyZ9v5DFU3vEZ6pLFkl7Ox4iSlAnHiLiA/RL5BmO+UTbkO3LHveAyhd4vjcdD434ZueNecJlwjLCrOlzY+Sl4v4zMcc8VVCbvukTeijq3lvSce+jQIbUdcS4/k4i3EOHXDK78gvbt20taWprMmjVLbr/9dvXL7ciRIwGtRLjKrH79+upvvP7www8By9OvQjPOE3xlGt7jqrZQrUOAq9EwBbPb7Woy0pvlgunN2WdKR7Mx5OdjGQU3B5oag6FVMFQ6mjZDp1tPLz+Q3swdzOVCnvJPdxnYQ+Zdb7oENHuGEiov5V8md4F86/nVJ9QrEUuReQ9vmSy+uhCcb2PevflGmb3lRjN3qOsiwlcmfHd+kXlHuj/f9mLUvXCUyZtvrFN9/wuVd9TvUPtlyfensiqTP9/G/Br/1o9F6I4o32NEScvkDLlf6u8D90tLhI57wWUqer/E8TBwv7RE8LhnLBNeA4cMFHe/dEb0WO6t3/o+WVjez/acG1P3IcJBCM3sCI7Q77xixQrfZ9u3b1eX2aOLDfCKLrf9+/f75lm2bJkKdtDtps9jXIY+j74MIiIiooi2EOF5aL1791YDpY8ePaquKMM9g3BJPMZPDB48WMaMGSM1a9ZUQc6DDz6oAhkMqIaePXuqwOeee+6RadOmqfFCeAAt7l2kt/A88MAD8sorr8i4ceNUP/bKlSvlgw8+kNTU1EgWnYiIiKJIRAMitOzce++98tdff6kACIMEEQxde+216nNcGo+mMdyQEa1GuDrs1VdfDWgKW7x4sbqqDIESBjwPHDhQpkyZ4psHl9wj+ME9jdAVh8FXb775Ji+5JyIiougIiHCfoaJUrlxZZs+eraYzPWy2KNdcc426moGIiIgoJsYQEREREYUbAyIiIiIyPQZEREREZHoMiIiIiMj0GBARERGR6TEgIiIiItNjQERERESmx4CIiIiITI8BEREREZkeAyIiIiIyPQZEREREZHoMiIiIiMj0GBARERGR6TEgIiIiItNjQERERESmx4CIiIiITI8BEREREZkeAyIiIiIyPQZEREREZHoMiIiIiMj0GBARERGR6TEgIiIiItNjQERERESmx4CIiIiITI8BEREREZkeAyIiIiIyPQZEREREZHoMiIiIiMj0GBARERGR6TEgIiIiItNjQERERESmx4CIiIiITI8BEREREZkeAyIiIiIyPQZEREREZHoMiIiIiMj0GBARERGR6TEgIiIiItNjQERERESmx4CIiIiITI8BEREREZleRAOiqVOnSseOHeXcc8+VunXrys033yzbt28PmCcvL09GjBghtWrVknPOOUf69esn+/btC5hn165d0qdPH6latapaztixY8XlcgXM880338ill14qlSpVkosuukjmz58fljISERFR9ItoQLRq1SoV7Hz//feybNkycTqd0rNnTzl+/LhvntGjR8vnn38uH374oZp/z5490rdvX9/nbrdbBUP5+fmydu1aefvtt1WwM3HiRN88O3fuVPN07dpVMjMzZdSoUTJkyBBZunRp2MtMRERE0cceyS//8ssvA94jkEELT0ZGhlx11VWSk5Mjc+fOlYULF0q3bt3UPPPmzZNWrVqpIOqKK66Qr776SrZs2SLLly+XevXqSVJSkjz55JMyfvx4mTRpkjgcDpkzZ440bdpUXnjhBbUM/P9vv/1WZs6cKb169YpI2YmIiCh6RDQgCoYACGrWrKleERih1ahHjx6+eVq2bCmNGjWSdevWqYAIr5dccokKhnQIcoYPHy6bN2+Wdu3aqXmMy9DnQUtRKKdOnVKTLjc3V72iG07virNarWryeDxq0unpaLnSNO2M6frfDgeW4e/mczptgo8cDndA3vLzbWKxiMTFBafbxWrVxG73p2uaRS3HavWI3e4pkG6zedSk83is4nJZxW7XxGp1qHKhvKHyjs+85ffmxWLxl8nlsonHYxGHI7DbsvzLJGKz2Xz5DrWdMCFIdrs1cbtD5R3zWsNcJk3lHa/Grl6LxeIrj55vEe/f3u2EbeBfB243tpE1jGXCdyNP3ryjfgTnHel6vlG/z1z3wlEmb76xHvX6bMy7DvXGW8dcZ7k/lVWZ/PlGPcH6Dc67fizC/4+Lc5XjMaIkZUKdiQvYL5F31BH9vV6/nU7tdJ2JxHEvuEzeY0Twfmk8Hhr3S5dLi9BxL7hMOEZ4T+2FnZ+C98vIHPcsQWXy1m/jOSZU3ktzzo25gAgFRIDSuXNnufjii1Xa3r171UarUaNGwLwIfvCZPo8xGNI/1z8rah4EOidPnpQqVaoUGNs0efLkAnlcv369VKtWTf1dp04dadasmeqOO3DggG+ehg0bqmnHjh2+AA8SEhJU69emTZvUdwZLTj4kDke6731KSqLk5jpk7Fh/Gkyf3kHi4/Nl2LANAZVw+vSO0qRJjgwYsM2XfvBgFUlJaSuJiQelT58sX3pWVnVZtKiVdO68R7p0+cOXnplZR1JTm0mvXsckKWmsZGdnS3p6esgy4TUxMVEyM0UGDdoktWv7y7RoUUvJyqohycnrA3aY8i+TqPqj5zvUdkK+McZszZoTsnq1SP/+OyQhwb+dUlMTJDOzbpjLdExSUryBuJ5vqF69umrNRDcxyoR8i2RLZubO09tppyQl+evemjUNZfXqhmEsU47k5yf76sO2bf4yYZ9q27atKpOeb5H0YtS9cJQJy/HWbxws0d2+YYN/O+FkjalJkyYyYIA332e3P5VVmXCM8O+X2P9wfDTWGWwHpMXHu2XYsPRyPEaUpEw4RgwK2C/xwxbHdRxTsQ30/TIlxS25ue4IHfeCy+RUxwicJ4zr2HgsN+6XixblROi4F1ymHMnM9PZ8FHZ+QpmM+2VkjntVgsrk3S/1Bggc9/74w7+dSnvORQ9SsWlR4oEHHtAaN26s7d6925e2YMECzeFwFJi3Y8eO2rhx49TfQ4cO1Xr27Bnw+fHjxxEmakuWLFHvmzdvrj3zzDMB86Smpqp5Tpw4UWD5eXl5Wk5Ojm9CnjBvdna25nQ61eR2u9W8eNXTjOkul6tY6enp6WrZDkea5nA4fZPF4tFEPAFpmJCGzwqma5rVGpgeF+c6ne4OmW6zBabb7W6Vbrenq/WelpZWaN7xmRUhv2So5RmXg3xgOcF5LP8yZWg2m82X71DbCZ+hbDYb1rsWIu/edRDeMnnrAOqCsW5gfet51/ONeuLfToHrANszvGXy5ikjI0PzeDwh844y6fkuXt0LR5m8+cY6Rb6D844JZUL9Nu6Xpd+fyqpM/nwjj6Hyjs9QlyyW9HI+RpSkTDhGxAXsl8g3GPONsiHfkTvuBZcp9H5pPB4a98vIHfeCy4RjhF3V4cLOT8H7ZWSOe66gMnnXJfJW1Lm1pOfcQ4cOqe2Ic/mZREUL0ciRI2Xx4sWyevVqFenp6tevr369HTlyJKCVCFeZ4TN9nh9++CFgefpVaMZ5gq9Mw/v4+PgCrUOAK9EwBbPb7Woy0pvlgunN2WdKR7Mx5OdjGQU3B5oag6FVMFQ6mjZDp1tPLz+Q3swdzOVCnvJPdxnYQ+Zdb7oENHuGEiov5V8md4F86/nVJ9QpEUuReQ9vmSy+uhCcb2PevflGmb3lRjN3qOsiwlcmfHd+kXlHuj/f9mLUvXCUyZtvrFN9/wuVd9TvUPtlyfensiqTP9/G/Br/1o9F6I4o32NEScvkDLlf6u8D90tLhI57wWUqer/E8TBwv7RE8LhnLBNeA4cMFHe/dEb0WO6t3/o+WVjez/acG7VXmaG/D8HQxx9/LCtXrlQDn43at2+v+p5XrFjhS8Nl+bjMvlOnTuo9Xjdu3Cj79+/3zYMr1hDstG7d2jePcRn6PPoyiIiIyNwi2kKES+5xBdmnn36q7kWkj/nB2Am03OB18ODBMmbMGDXQGkHOgw8+qAIZDKgGXKaPwOeee+6RadOmqWVMmDBBLVtv5XnggQfklVdekXHjxqm+bARfH3zwgaSmpkay+ERERBQlItpC9Nprr6lBUNdcc400aNDAN73//vu+eXBp/A033KBuyIhL8dH99dFHHwU0h6G7Da8IlO6++2659957ZcqUKb550PKE4AetQhjoicvv33zzTV5yT0RERJFvITJeIleYypUry+zZs9VUmMaNG8uSJUuKXA6CLlzRQERERBSMzzIjIiIi02NARERERKbHgIiIiIhMjwERERERmR4DIiIiIjI9BkRERERkegyIiIiIyPQYEBEREZHpMSAiIiIi02NARERERKbHgIiIiIhMjwERERERmR4DIiIiIjI9BkRERERkegyIiIiIyPQYEBEREZHpMSAiIiIi02NARERERKbHgIiIiIhMjwERERERmR4DIiIiIjI9BkRERERkegyIiIiIyPQYEBEREZHpMSAiIiIi02NARERERKbHgIiIiIhMjwERERERmR4DIiIiIjI9BkRERERkegyIiIiIyPQYEBEREZHpMSAiIiIi02NARERERKbHgIiIiIhMjwERERERmR4DIiIiIjI9BkRERERkegyIiIiIyPQYEBEREZHpMSAiIiIi04toQLR69Wq58cYb5fzzzxeLxSKffPJJwOeapsnEiROlQYMGUqVKFenRo4f8/PPPAfMcOnRI7rrrLomPj5caNWrI4MGD5dixYwHzbNiwQbp06SKVK1eWCy+8UKZNmxaW8hEREVFsiGhAdPz4cWnbtq3Mnj075OcIXF566SWZM2eO/O9//5Nq1apJr169JC8vzzcPgqHNmzfLsmXLZPHixSrIuv/++32f5+bmSs+ePaVx48aSkZEh06dPl0mTJsnrr78eljISERFR9LNH8st79+6tplDQOvTiiy/KhAkT5KabblJp77zzjtSrV0+1JN1xxx2ydetW+fLLLyUtLU06dOig5nn55Zfl+uuvl+eff161PC1YsEDy8/PlrbfeEofDIW3atJHMzEyZMWNGQOBERERE5hW1Y4h27twpe/fuVd1kuurVq8vll18u69atU+/xim4yPRgCzG+1WlWLkj7PVVddpYIhHVqZtm/fLocPHw5rmYiIiCg6RbSFqCgIhgAtQkZ4r3+G17p16wZ8brfbpWbNmgHzNG3atMAy9M/OO++8At996tQpNRm73cDlcqkJEHRh8ng8atLp6W63W7VynSld/9vhwDK8ywan0yb4yOFwB+QtP98mFotIXFxwul2sVk3sdn+6plnUcqxWj9jtngLpNptHTTqPxyoul1Xsdk2sVocqF8obKu/4zFt+b14sFn+ZXC6beDwWcTj85QlPmURsNpsv36G2EyYEx263Jm53qLxjXmuYy6SpvONVzzdgXJ1eHj3fIt6/vdsJ28C/DtxubCNrGMuE7/b+0EDeUT+C8450Pd+o32eue+EokzffWI96fTbmXYd6461jrrPcn8qqTP58o55g/QbnXT8W4f/HxbnK8RhRkjKhzsQF7JfIO+qI/l6v306ndrrOROK4F1wm7zEieL80Hg+N+6XLpUXouBdcJhwjvKf2ws5PwftlZI57lqAyeeu38RwTKu+lOefGfEAUSVOnTpXJkycXSF+/fr0axwR16tSRZs2aqZasAwcO+OZp2LChmnbs2CE5OTm+9ISEBBW8bdq0SU6ePFlg2cnJh8ThSPe9T0lJlNxch4wd60+D6dM7SHx8vgwbtiGgEk6f3lGaNMmRAQO2+dIPHqwiKSltJTHxoPTpk+VLz8qqLosWtZLOnfdIly5/+NIzM+tIamoz6dXrmCQljZXs7GxJT08PWSa8JiYmSmamyKBBm6R2bX+ZFi1qKVlZNSQ5eX3ADlP+ZRLp3LmzL9+hthPyPXbsWFmz5oSsXi3Sv/8OSUjwb6fU1ATJzKwb5jIdk5QUbyCu51tvEW3VqpXs2bNHlQn5FsmWzMydp7fTTklK8te9NWsayurVDcNYphzJz0/21Ydt2/xlwkUQGB+IMun5FkkvRt0LR5mwHG/9xsESXeq48EKHkzWmJk2ayIAB3nyf3f5UVmXCMcK/X2L/w0nNWGewHZAWH++WYcPSy/EYUZIy4RgxKGC/bNmypWrdxzEV20DfL1NS3JKb647QcS+4TE51jMAPYuM6Nh7LjfvlokU5ETruBZcpRzIze6m/Cjs/oUzG/TIyx70qQWXy7pd6AwSOe3/84d9OpT3nbtmyRYpNixLIyscff+x7/+uvv6q09evXB8x31VVXaQ899JD6e+7cuVqNGjUCPnc6nZrNZtM++ugj9f6ee+7RbrrppoB5Vq5cqZZ96NChkHnJy8vTcnJyfNPu3bvV/NnZ2Wr5mNxut5oXr3qaMd3lchUrPT09XS3b4UjTHA6nb7JYPJqIJyANE9LwWcF0TbNaA9Pj4lyn090h0222wHS73a3S7fZ0zeFwaGlpaYXmHZ9ZEfJLhlqecTnIB5YTnMfyL1OG2vZ6vkNtJ3yGstlsWO9aiLx710F4y+StA6gLxrqB9a3nXc836ol/OwWuA2zP8JbJm6eMjAzN4/GEzDvKpOe7eHUvHGXy5hvrFPkOzjsmlAn127hfln5/Kqsy+fONPIbKOz5DXbJY0sv5GFGSMuEYERewXyLf+vHauF8i35E77gWXKfR+aTweGvfLyB33gsuEY4Rd1eHCzk/B+2VkjnuuoDJ51yXyVtS5taTnXJznsR1xLj+TqG0hQjdX/fr1ZcWKFZKUlKTSEDlibNDw4cPV+06dOsmRI0fU1WPt27dXaStXrlTNaRhrpM/z2GOPidPpVM22gCvSWrRoEbK7DCpVqqSmYOiOw2SkN8sF05uzz5SOZmPIz8cyCm4ONDUGQ/gYKh1Nm6HTraeXH0hv5g7mciFP+ae7DOwh8643XQKaPUMJlZfyL5O7QL71/OoTWgRELEXmPbxlsvjqQnC+jXn35htl9pYbzdyhhgGGr0z47vwi8450f77txah74SiTN99Yp/r+FyrvqN+h9suS709lVSZ/vo35Nf6tH4vQHVG+x4iSlskZcr/U3wful5YIHfeCy1T0fonjYeB+aYngcc9YJrwGDhko7n7pjOix3Fu/9X2ysLyf7Tk3agdV435BuOILE6ApDH/v2rVLrZRRo0bJU089JZ999pls3LhR7r33XnXl2M0336zmR3fCddddJ0OHDpUffvhBvvvuOxk5cqS6Ag3zwZ133qmakHF/Ilye//7778usWbNkzJgxkSw6ERERRZGIthChX7Zr166+93qQMnDgQJk/f76MGzdO3asIl8ejJejvf/+7usweN1jU4bJ6BEHdu3dXUWO/fv3UvYuM4zC++uorGTFihGpFql27trrZIy+5JyIioqgIiK655pqAUeHB0Eo0ZcoUNRUGV5QtXLiwyO/B4MM1a9acVV6JiIio4ora+xARERERhQsDIiIiIjI9BkRERERkegyIiIiIyPQYEBEREZHpMSAiIiIi02NARERERKbHgIiIiIhMjwERERERmR4DIiIiIjI9BkRERERkegyIiIiIyPQYEBEREZHpMSAiIiIi02NARERERKbHgIiIiIhMjwERERERmR4DIiIiIjI9BkRERERkegyIiIiIyPQYEBEREZHpMSAiIiIi02NARERERKbHgIiIiIhMjwERERERmR4DIiIiIjI9BkRERERkegyIiIiIyPQYEBEREZHpMSAiIiIi02NARERERKbHgIiIiIhMjwERERERmR4DIiIiIjI9BkRERERkegyIiIiIyPQYEBEREZHpMSAiIiIi02NARERERKbHgIiIiIhMjwERERERmZ6pAqLZs2dLkyZNpHLlynL55ZfLDz/8EOksERERURQwTUD0/vvvy5gxY+SJJ56QH3/8Udq2bSu9evWS/fv3RzprREREFGGmCYhmzJghQ4cOlfvuu09at24tc+bMkapVq8pbb70V6awRERFRhJkiIMrPz5eMjAzp0aOHL81qtar369ati2jeiIiIKPLsYgIHDx4Ut9st9erVC0jH+23bthWY/9SpU2rS5eTkqNdDhw6Jy+XyBVSYPB6PmnR6Or5P07Qzph89elS9xsVliEiuL93ptJxO989bdLpVLBZN7HZ/uqZZxOWyFJputWpis/nTPR6LuN0Wsdm2i9UaJ7m5uarMofKOzywWi2hahtjtR9V36LBsfEdcnH+9hKdMO1Q+9XyH2k74LC4uTtzuDPF4jqllhMp7YenlU6afBdUKdUHPN2D92my2gHyLIN9HT28nTW1DHdKwDcNXpp/xTo4dO6b2EdSP4LyjTHq+Ub/PXPfCUSZvvrFO9X3bmHdAmVAGu92/X5Z+fyqrMm335Rv1BOs3OO/4zCtD4uJyy/EYUZIy7RC73R6wXyLvWL/68VSv304n1vfRCB33gsv0s+DQHrxfGo+Hxv3S5ToaoeNecJmQb5uqw0eOHAl5fgreLyNz3LMElcm7XyJvWK+FnVtLes49fPjw6e8LzFNImgn8+eefWBPa2rVrA9LHjh2rXXbZZQXmf+KJJ9T8nDhx4sSJEyeJ+Wn37t1njBVM0UJUu3Zt9Ytk3759Ael4X79+/QLzP/roo2oAtg7RKH4h1KpVS/2qiUaIqC+88ELZvXu3xMfHSyzp2LGjpKWlSSzh+g4vru/w4voOL67v8oOWIbQ6nX/++Wec1xQBkcPhkPbt28uKFSvk5ptv9gU5eD9y5MgC81eqVElNRjVq1JBYgJ0p1nYoBKuxlmcd13d4cX2HF9d3eHF9l4/q1asXaz5TBESAFp+BAwdKhw4d5LLLLpMXX3xRjh8/rq46o8gaMWJEpLNgKlzf4cX1HV5c3+E1ogKtbwv6zcQkXnnlFZk+fbrs3btXkpKS5KWXXlI3aKwI0OSKKBiDRKM9Wq8IuL7Di+s7vLi+w4vrOzqYpoUI0D0WqousIkAXH246GdzVR+WD6zu8uL7Di+s7vLi+o4OpWoiIiIiITHtjRiIiIqKiMCCiiFi9erXceOON6lJI3Mrgk08+iXSWKrTXXntNEhMTfVexdOrUSb744otIZ6vCmjRpkqrXxqlly5aRzlaFhYd2B69vTBVpwG+0OXr0qIwaNUoaN24sVapUkSuvvDKqL78vDgZEFBG4wg8P2J09e3aks2IKDRs2lGeffVY9wiY9PV26desmN910k2zevDnSWauw2rRpI3/99Zdv+vbbbyOdpQoLJ2Ljul62bJlKv/XWWyOdtQpryJAhaj3/5z//kY0bN0rPnj3V47D+/PNPiVUcQ0QRh19yH3/8se8eURQeNWvWVFddDh48ONJZqZAtRGj1zMzMjHRWTAktF4sXL5aff/45am+mG8tOnjwp5557rnz66afSp08fXzru99e7d2956qmnJBaxhaiCQEsLmo0rV66sbiXwww8/RDpLFdLUqVPVnVlxMKhbt64K4rZvxzOmYgee+fPee++pVjp0ncUStHLhBIcTXrTDyRhdwgkJCXLXXXfJrl27JBbgF/7dd9+t7syPrpBLLrlEtSrG0sO83333XRk0aFDUB0PYFx9//HFp2rSpWtfNmjWTJ598snjP3Yogl8ul8o7zjRHKEMstoQyIKoD3339f3XgSl23++OOPqiuqV69esn///khnrcJZtWqVGpfw/fffq+Zip9OpmooRXEQ7NGufc8456tLeBx54QLXKtW7dWmKpWyQlJUWNhYp2+FEyf/58+fLLL9X4rZ07d0qXLl18D3OOVngQZufOndWDPzHGbMuWLfLCCy/IeeedJ7ECLXN4qOk///lPiXbPPfecqh+4R97WrVvV+2nTpsnLL78s0ezcc89VP6YQvO3Zs0cFRwhC161bp7osY1bZPUKVIgUPqB0xYoTvvdvt1s4//3xt6tSpWixANfz444+1WLR//36V/1WrVmnR7tSpU9rPP/+spaena4888ohWu3ZtbfPmzVosOHr0qNa8eXNt2bJl2tVXX60lJydrseTw4cNafHy89uabb2rRbPz48drf//53LZb17NlTu+GGG7RY0KdPH23QoEEBaX379tXuuusuLdr98ssv2lVXXaWOfzabTevYsaPKd8uWLbVYxRaiGIfmYQyUxWA2ndVqVe8RrVP5wp1l9fE4sfBMv4suukj186PrDy2Js2bNkliAVjmMVTDW81iCZyH+7W9/k19++UWi2WeffaYeb4TByOgSbteunbzxxhsSK37//XdZvny5GvAbC3BlFp6puWPHDvX+p59+Ul1OGIcT7Zo1a6ZazI8dO6YeSothGmgxRxdxrGJAFOMOHjyomivr1asXkI73eEQJlR88IBhjWdDFcPHFF0ss5v/UqVMS7TDeCV3BCOJiFU4av/76qzRo0ECiWVZWlurCad68uSxdulSGDx8uDz30kLz99tsSC+bNm6cCOeNA32j2yCOPyB133KFuyYBuSgSgOKZgzFmsqFatmqrX6G5FncHVq7HKVI/uoOg6QRh/LWOMBa7IQUtLo0aNJFZaLTZt2hQTgwgfffRR9asT6xbjWBYuXCjffPONOoBFM/zyTE5OVuO1ggdwRrOHH35Y3WcL92jBGAuM78NTwQcMGCDRHiSjheiZZ55R73GCRh2fM2eOejh2tOcdARHyabfHxqntgw8+kAULFqj9EbdpwDEQAREG40f7+l66dKka/N2iRQt1LB87dqwK7GL6gemR7rOjsx8Xgv7b4DE49957r/aPf/xDi1Zff/216nsOngYOHKjFAozZatiwoZaVlaXFAoxTaNy4seZwOLQ6depo3bt317766ist2qFe62MU9AnvLRaL+tvlcmnR6Pbbb9caNGig1vcFF1yg3mPMRbRr1KiRNnjw4IC0V199VY1JjHZLly5VdWP79u1arMAx5JVXXglIe/LJJ7UWLVpo0e7999/XEhISVB2vX7++OiYeOXJEi2WxEUZTkeNCMCYE/dD6fXzwSwnvo/lBttdcc03UX1oaCvL84IMPqiu00MKCy2Vjwdy5cyUWde/eXV0dZ4RfoPglOn78eNXqEq3dfLEI3b/Bt5HA+Ba0dEU7XO0Za8eUEydOqDGfRqjTOIZHu9tuu01NFQkDogoAl9yjeRVN3Zdddpm8+OKL6jLwmG66jOJuMjRv44ZkuPRUH6dVvXp1dQ8OKltYx8HjszBmAffIicVxW9Fu9OjRaqAvusxwssNA2ddff11NVPbQrfr000+rrmx0ma1fv15mzJih7qFE4cc7VVcQuI8F7jqME3RSUpK89NJL6l4oVLYKu9Ebxi7Ewn1PKgK0LqKOI/Cnsoc7PGPMGW4siRZQ/OAaOnRopLNVIWE8H27MiBZn3DcOY4cwzmzixImq9Z/CiwERERERmR4vuyciIiLTY0BEREREpseAiIiIiEyPARERERGZHgMiIiIiMj0GRERERGR6DIiIiIjI9BgQEZEpbqj5ySefFHt+PJYF/+fIkSPlmi8iih4MiIgoZuHu4AhcMMXFxUm9evXk2muvlbfeeivgeVB//fWX9O7du9jLxeMr8H/wSBaYP3++1KhRo1zKQETRgQEREcW06667TgUvv/32m3zxxRfStWtXSU5OlhtuuEFcLpeap379+lKpUqViLxOPTcD/KexRLURU8TAgIqKYhkAHwcsFF1wgl156qfz73/9WD99FcISWnVBdZmvXrlXPQ6tcubJ6KDI+wzyZmZkFuszwNx6UnJOT42uNmjRpkprv1VdflebNm6vloHWqf//+EVoLRHS2+LR7IqpwunXrJm3btpWPPvpIhgwZEvBZbm6uesr49ddfLwsXLpTff/9dRo0aVWT3GR4kiwdubt++XaWdc845kp6eLg899JD85z//UfMcOnRI1qxZU+5lI6LywYCIiCqkli1byoYNGwqkIwhCK88bb7yhWnZat24tf/75Z6FPdEf3GcYS4f+gJUq3a9cuqVatmuqaO/fcc6Vx48bSrl27ci0TEZUfdpkRUYWkaVrIMUBo5UlMTFTBkO6yyy4r8fIxeBtBUEJCgtxzzz2yYMECOXHixFnnm4gigwEREVVIW7dulaZNm5bb8tEq9OOPP8qiRYukQYMGqksN3XS8VJ8oNjEgIqIKZ+XKlbJx40bp169fgc9atGihPjt16pQvLS0trcjlodvM7XYXSLfb7dKjRw+ZNm2a6p7DlW74biKKPQyIiCimIbDZu3evGgeEFptnnnlGbrrpJjW259577y0w/5133qnuUXT//ferVqSlS5fK888/rz4r7DL7Jk2ayLFjx2TFihVy8OBB1TW2ePFieemll9SVaRiY/c4776jlIuAiotjDgIiIYtqXX36puqwQtOCeRF9//bUKVHDpvc1mKzB/fHy8fP755yqQwaX3jz32mOruAuO4IiNcRfbAAw/I7bffLnXq1FEtQrhRI65iwxVtrVq1kjlz5qjuszZt2pR7mYmo7Fk0jDwkIjIxDIjW7zVUpUqVSGeHiCKAl90TkemgewtXh+Fmjj/99JOMHz9ebrvtNgZDRCbGgIiITAdjjtBNhld0t916663y9NNPRzpbRBRB7DIjIiIi0+OgaiIiIjI9BkRERERkegyIiIiIyPQYEBEREZHpMSAiIiIi02NARERERKbHgIiIiIhMjwERERERmR4DIiIiIhKz+//1BlHkmlfjxQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the MNIST dataset, which is a large database of handwritten digits.\n",
    "# The function returns two tuples: one for training data and one for testing data.\n",
    "# Recalling, a Tuple is a collection of objects that are ordered and immutable.\n",
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
    "\n",
    "# First we convert the data to float32, which helps with numerical stability. A float32 provides sufficient precision, while also being memory efficient.\n",
    "# Most modern CPU's and GPU's are optimized for float32 operations, making computations faster.\n",
    "X_train = X_train.astype(\"float32\")\n",
    "X_test = X_test.astype(\"float32\")\n",
    "\n",
    "# Then we convert the training data to have a channel dimension, which is required for CNNs.\n",
    "X_train = X_train[..., tf.newaxis] # Add the channel dimension\n",
    "X_test = X_test[..., tf.newaxis] # Add the channel dimension\n",
    "print(f\"New shape for X_train for CNN's: {X_train.shape}\")\n",
    "print(f\"New shape for X_test for CNN's: {X_test.shape}\")\n",
    "\n",
    "# Declare the types of the loaded data for clarity.\n",
    "X_train: NDArray[np.float32]\n",
    "Y_train: NDArray[np.uint8]\n",
    "X_test: NDArray[np.float32]\n",
    "Y_test: NDArray[np.uint8]\n",
    "\n",
    "# We set the line width to a large value to avoid line breaks when printing the array.\n",
    "with np.printoptions(linewidth=10000):\n",
    "    # Print the shapes of the datasets to understand their dimensions.\n",
    "    print(\"Shape of X_train:\\t\", X_train.shape)\n",
    "    print(\"Shape of X_test:\\t\", X_test.shape)\n",
    "    print(\"Shape of Y_train:\\t\", Y_train.shape)\n",
    "    print(\"Shape of Y_test:\\t\", Y_test.shape)\n",
    "    print(f\"X_train data type: {X_train.dtype}\")\n",
    "    print(f\"X_test data type: {X_test.dtype}\")\n",
    "    print(f\"Y_train data type: {Y_train.dtype}\")\n",
    "    print(f\"Y_test data type: {Y_test.dtype}\")\n",
    "\n",
    "    # Inspect a single data sample to see what it looks like.\n",
    "    n: int = 567\n",
    "    print(f\"\\nX_train data {n}-th element (a 28x28 pixel image):\\n\", np.squeeze(X_train[n]))\n",
    "    print(\"\\nAnd its corresponding label:\\t\", Y_train[n])\n",
    "\n",
    "    # To do: For using this data in a neural network,\n",
    "    # Tensorflow/Keras expects the input data to be in a 1D or 2D array format where each row represents a single sample and each column represents a feature. The general format for the input shape is: (batch_size, feature_1, feature_2, ...)\n",
    "    # However, we can use the tf.keras.layers.Flatten layer as the first layer in our sequential model.\n",
    "    # This layer automatically flattens the input shape without the need for manual reshaping of our data.\n",
    "    # For a Dense (fully connected) network: We must flatten each 28x28 image into a single 1D array of 784 pixels. The input shape for the first layer of our model would then be (None, 784), where None represents a variable batch size.\n",
    "    # For a Convolutional Neural Network (CNN): We must add a channel dimension. Since the images are grayscale, there is only one channel. We would reshape the data to (number_of_images, 28, 28, 1). The input shape for the first layer (typically a Conv2D layer) would be (28, 28, 1). The batch size is handled automatically by Keras.\n",
    "    # Scaling can also be performd in the model using a tf.keras.layers.Rescaling or keras.layers.Normalization layer as the first layer in our sequential model.\n",
    "    # The advantage of using these layers is that they integrate seamlessly into the model architecture, ensuring that the data is preprocessed consistently during both training and inference.\n",
    "    # This approach also simplifies the code by reducing the need for separate preprocessing steps outside the model definition.\n",
    "    # And, it ensures that inference data is processed in the same way as training data, which is crucial for maintaining model performance.\n",
    "\n",
    "    # Analyze the distribution of the digits in the training set.\n",
    "    # `np.unique` finds the unique digit labels and `return_counts=True` counts their occurrences.\n",
    "    dataset_train_distribution: Tuple[np.ndarray, np.ndarray] = np.unique(Y_train, return_counts=True)\n",
    "    digits_train: np.ndarray = dataset_train_distribution[0]\n",
    "    counts_train: np.ndarray = dataset_train_distribution[1]\n",
    "    \n",
    "    print(\"\\n--- Train Dataset Distribution ---\")\n",
    "    print(\"Digits:\\t\\t\\t\", digits_train)\n",
    "    print(\"Count per digit:\\t\", counts_train)\n",
    "    \n",
    "    # Calculate basic statistics on the distribution.\n",
    "    avg: float = np.mean(counts_train)\n",
    "    print(f\"Average sample size:\\t {avg:.2f}\")\n",
    "    \n",
    "    max_count_train: np.int64 = np.max(counts_train)\n",
    "    min_count_train: np.int64 = np.min(counts_train)\n",
    "    print(f\"Maximum sample size:\\t {max_count_train}\")\n",
    "    print(f\"Minimum sample size:\\t {min_count_train}\")\n",
    "\n",
    "\n",
    "    dataset_test_distribution: Tuple[np.ndarray, np.ndarray] = np.unique(Y_test, return_counts=True)\n",
    "    digits_test: np.ndarray = dataset_test_distribution[0]\n",
    "    counts_test: np.ndarray = dataset_test_distribution[1]\n",
    "    \n",
    "    print(\"\\n--- Test Dataset Distribution ---\")\n",
    "    print(\"Digits:\\t\\t\\t\", digits_test)\n",
    "    print(\"Count per digit:\\t\", counts_test)\n",
    "    \n",
    "    # Calculate basic statistics on the distribution.\n",
    "    avg: float = np.mean(counts_test)\n",
    "    print(f\"Average sample size:\\t {avg:.2f}\")\n",
    "    \n",
    "    max_count_test: np.int64 = np.max(counts_test)\n",
    "    min_count_test: np.int64 = np.min(counts_test)\n",
    "    print(f\"Maximum sample size:\\t {max_count_test}\")\n",
    "    print(f\"Minimum sample size:\\t {min_count_test}\")\n",
    "\n",
    "# Create a bar chart from the counts and digits to visualize the distribution.\n",
    "plt.bar(digits_train, counts_train, color='blue', edgecolor='black')\n",
    "\n",
    "# Set the title and labels for clarity.\n",
    "plt.title('Count of Each Digit in Training Set')\n",
    "plt.xlabel('Digits')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "# Set x-ticks to be at the center of each bar and label them with the digit.\n",
    "# Minor ticks are used here to place the labels directly under the bars.\n",
    "# Ticks specify the positions on the x-axis where the labels should be placed.\n",
    "# By setting ticks=digits_train, we ensure that each digit (0-9) is labeled correctly under its corresponding bar.\n",
    "plt.xticks(ticks=digits_train, minor=True, labels=digits_train)\n",
    "\n",
    "# Add a grid for better readability.\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Display the plot.\n",
    "plt.show()\n",
    "\n",
    "# And the same bar chart for the test set.\n",
    "\n",
    "# Create a bar chart from the counts and digits to visualize the distribution.\n",
    "plt.bar(digits_test, counts_test, color='red', edgecolor='black')\n",
    "\n",
    "# Set the title and labels for clarity.\n",
    "plt.title('Count of Each Digit in Test Set')\n",
    "plt.xlabel('Digits')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "# Set x-ticks to be at the center of each bar and label them with the digit.\n",
    "plt.xticks(ticks=digits_test, minor=True, labels=digits_test)\n",
    "\n",
    "# Add a grid for better readability.\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899d4acf",
   "metadata": {},
   "source": [
    "## Dataset Analysis\n",
    "\n",
    "The content and size of the training and testing datasets align with the description on the Kaggle MNIST dataset page, Hojjat, F. (2017). MNIST: The Most Famous Dataset in the World. Kaggle. Retrieved August 28, 2025, from https://www.kaggle.com/datasets/hojjatk/mnist-dataset. The plot of digit distribution shows a fairly homogeneous representation across all classes (digits 0 through 9). While the digit '1' is slightly oversampled and the digit '5' is slightly undersampled, the class imbalance is not significant enough to warrant further action for this assessment.\n",
    "\n",
    "In a scenario where the distribution were to be significantly imbalanced and we needed to make it more homogeneous, we would use a technique called **resampling**. Resampling involves adjusting the distribution of the training data to be more balanced. There are two primary types:\n",
    "\n",
    "- **Oversampling** involves duplicating samples from the underrepresented classes to increase their frequency.\n",
    "\n",
    "- **Undersampling** involves removing samples from the overrepresented classes to reduce their frequency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f1599d",
   "metadata": {},
   "source": [
    "### Define the training loop\n",
    "\n",
    "The 'run_experiment' function is our main pipeline for training a neural network.\n",
    "It encapsulates all the necessary steps: callking the model creation function, setting default hyperparameters,\n",
    "configuring callbacks for logging and monitoring, and finally training the model and return a History object for analysis.\n",
    "This makes our code highly modular and reusable, allowing us to easily run different\n",
    "experiments by simply changing the input parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "119c86a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The 'run_experiment' function is our main pipeline for training a neural network.\n",
    "# It encapsulates all the necessary steps: creating the model, setting hyperparameters,\n",
    "# configuring callbacks for logging and monitoring, and finally training the model.\n",
    "# This makes our code highly modular and reusable, allowing us to easily run different\n",
    "# experiments by simply changing the input parameters.\n",
    "\n",
    "def run_experiment(\n",
    "    model_creation_func: Callable[[], Model],\n",
    "    hyperparameters: dict[str, int | float | str],\n",
    "    parent_folder: str,\n",
    "    X_train: np.ndarray | tf.Tensor,\n",
    "    Y_train: np.ndarray | tf.Tensor\n",
    ") -> History:\n",
    "    \"\"\"\n",
    "    Runs a full training experiment for a given model architecture and hyperparameter set.\n",
    "\n",
    "    Args:\n",
    "        model_creation_func: A callable (function) that builds and returns a compiled Keras Model.\n",
    "        hyperparameters: Dictionary containing experiment settings (learning_rate, batch_size, optimiser, epochs, etc).\n",
    "        parent_folder: Path to the directory where models and results will be saved.\n",
    "        X_train: Training input data (features) as a NumPy array or TensorFlow tensor.\n",
    "        Y_train: Training labels (targets) as a NumPy array or TensorFlow tensor.\n",
    "\n",
    "    Returns:\n",
    "        History: A Keras History object with details of the training process.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create the model by calling the function provided\n",
    "    model: Model = model_creation_func()\n",
    "\n",
    "    # Set some default hyperparameter values (these will be used if not specified in the dictionary)\n",
    "    default_lr: float = 0.001\n",
    "    default_optimiser: str = \"adam\"\n",
    "    default_batch_size: int = 64\n",
    "    default_epochs: int = 10\n",
    "\n",
    "    # Give the model a name if it doesn’t have one already\n",
    "    if model.name is None or model.name == \"\":\n",
    "        model.name = model_creation_func.__name__\n",
    "\n",
    "    # Many models begin with a \"normalisation\" layer.\n",
    "    # Here we adapt (fit) that layer to the training data so it knows the data’s mean and variance.\n",
    "    print(\"\\nAdapting the normalisation layer...\")\n",
    "    model.layers[0].adapt(X_train)  # type: ignore[attr-defined]\n",
    "    print(\"Adaptation complete.\\n\")\n",
    "\n",
    "    # Display a summary of what is about to happen\n",
    "    print(f\"\\n--- Starting Experiment: {model.name} ---\")\n",
    "    print(\"\\n--- Model Architecture ---\")\n",
    "    model.summary()\n",
    "    print(\"\\n--- Hyperparameters ---\")\n",
    "    for key, value in hyperparameters.items():\n",
    "        print(f\"{key:<20}: {value}\")\n",
    "\n",
    "    # Generate a unique run name so we can distinguish between different experiments.\n",
    "    run_name: str = (\n",
    "        f\"{model.name}-lr_{hyperparameters.get('learning_rate', default_lr)}\"\n",
    "        f\"-bs_{hyperparameters.get('batch_size', default_batch_size)}-\"\n",
    "        f\"{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    )\n",
    "\n",
    "    # Start a new run in Weights & Biases (wandb) for logging metrics and results.\n",
    "    run: Any = wandb.init(\n",
    "        project=\"CSE5ML-Assessment2\",\n",
    "        name=run_name,\n",
    "        config=hyperparameters,\n",
    "    )\n",
    "\n",
    "    # This callback sends training metrics (loss, accuracy, etc.) to wandb in real time.\n",
    "    wandb_metrics_logger: Callback = WandbMetricsLogger()\n",
    "\n",
    "    # Choose the optimiser and learning rate based on the hyperparameters provided.\n",
    "    optimiser_name: str = hyperparameters.get(\"optimiser\", default_optimiser).lower()  # type: ignore[union-attr]\n",
    "    learning_rate: float = float(hyperparameters.get(\"learning_rate\", default_lr))\n",
    "\n",
    "    if optimiser_name == \"adam\":\n",
    "        optimiser: Adam | SGD | str = Adam(learning_rate=learning_rate)\n",
    "    elif optimiser_name == \"sgd\":\n",
    "        optimiser = SGD(learning_rate=learning_rate)\n",
    "    elif optimiser_name == 'adamw': \n",
    "        optimiser = AdamW(learning_rate=learning_rate)\n",
    "    elif optimiser_name == 'rmsprop': \n",
    "        optimiser = RMSprop(learning_rate=learning_rate)\n",
    "    else:\n",
    "            # It's good practice to have a fallback in case of a typo\n",
    "            print(f\"Warning: Unknown optimiser '{optimiser_name}'. Defaulting to Adam.\")\n",
    "            optimiser = Adam(learning_rate=learning_rate)\n",
    "\n",
    "    # Create a folder to save this model’s results on disk.\n",
    "    model_specific_folder: str = os.path.join(parent_folder, run_name)\n",
    "    os.makedirs(model_specific_folder, exist_ok=True)\n",
    "\n",
    "    # Define a filename pattern for saving the best models during training.\n",
    "    filepath: str = os.path.join(\n",
    "        model_specific_folder,\n",
    "        \"best_model_epoch-{epoch:02d}_val_acc-{val_accuracy:.4f}.keras\"\n",
    "    )\n",
    "\n",
    "    # This callback saves the best model (on the local hard drive).\n",
    "    checkpoint: ModelCheckpoint = ModelCheckpoint(\n",
    "        filepath=filepath,\n",
    "        monitor=\"val_accuracy\",   # we monitor validation accuracy\n",
    "        mode=\"max\",               # we want the maximum value\n",
    "        save_best_only=True,      # only save when performance improves\n",
    "        verbose=1,\n",
    "    )\n",
    "\n",
    "    # This callback saves the best model (to wandb’s cloud storage).\n",
    "    wandb_checkpoint: WandbModelCheckpoint = WandbModelCheckpoint(\n",
    "        filepath=f\"wandb_models/{run_name}/best_model.keras\",\n",
    "        monitor=\"val_accuracy\",\n",
    "        mode=\"max\",\n",
    "        save_best_only=True,\n",
    "        verbose=1,\n",
    "    )\n",
    "\n",
    "    # This callback stops training early if validation accuracy does not improve for a while.\n",
    "    early_stopping: EarlyStopping = EarlyStopping(\n",
    "        monitor=\"val_accuracy\",\n",
    "        patience=10,              # number of epochs to wait\n",
    "        restore_best_weights=True # restore the best model when stopping\n",
    "    )\n",
    "\n",
    "    # Now we compile the model, telling it what optimiser, loss function, and metrics to use.\n",
    "    model.compile(\n",
    "        optimizer=optimiser,\n",
    "        loss=hyperparameters.get(\"loss_function\", \"sparse_categorical_crossentropy\"),  # type: ignore[arg-type]\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "\n",
    "    # Train the model on the provided data.\n",
    "    # We use 10% of the training set as a validation set.\n",
    "    history: History = model.fit(\n",
    "        X_train,\n",
    "        Y_train,\n",
    "        epochs=int(hyperparameters.get(\"epochs\", default_epochs)),\n",
    "        batch_size=int(hyperparameters.get(\"batch_size\", default_batch_size)),\n",
    "        validation_split=0.1,\n",
    "        callbacks=[checkpoint, wandb_metrics_logger, wandb_checkpoint, early_stopping],\n",
    "        verbose=1,\n",
    "    )\n",
    "\n",
    "    # Save the training history (loss/accuracy curves) to disk for later analysis.\n",
    "    history.hyperparameters = hyperparameters  # type: ignore[attr-defined]\n",
    "    history_filepath: str = os.path.join(model_specific_folder, \"training_history.pkl\")\n",
    "    with open(history_filepath, \"wb\") as f:\n",
    "        pickle.dump(history.history, f)\n",
    "    print(f\"\\nTraining history saved to: {history_filepath}\")\n",
    "\n",
    "    # Summarise peak performance at the end of training.\n",
    "    val_accuracies: list[float] = history.history[\"val_accuracy\"]\n",
    "    best_validation_accuracy: float = max(val_accuracies)\n",
    "    best_epoch: int = val_accuracies.index(best_validation_accuracy) + 1\n",
    "    associated_train_acc: float = history.history[\"accuracy\"][best_epoch - 1]\n",
    "\n",
    "    print(\"\\n--- Peak Performance Summary ---\")\n",
    "    print(f\"{'Best validation accuracy:':<35} {best_validation_accuracy:.4f}\")\n",
    "    print(f\"{'Associated training accuracy:':<35} {associated_train_acc:.4f}\")\n",
    "    print(f\"{'Occurred at epoch:':<35} {best_epoch}\")\n",
    "\n",
    "    # Close the wandb run so it is properly logged.\n",
    "    run.finish()\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5568184",
   "metadata": {},
   "source": [
    "## Part 1, Task 1: Creating a simple Multilayer Perceptron (MLP) neural network\n",
    "\n",
    "The code below defines our base model.\n",
    "\n",
    "To experiment with different architectures or tune its hyperparameters, we simply copy this entire cell and make our changes.\n",
    "\n",
    "We need to make sure to give each new model a unique name. This ensures that when the ModelCheckpoint callback saves the best-performing version during training, the filename will be clear and identifiable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b401b847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Set Seeds for Reproducibility ---\n",
    "\n",
    "# This sets the global random seed for all TensorFlow operations.\n",
    "# It ensures that things like model weight initialisation are the same every time.\n",
    "# `tf.random.set_seed()` is the modern way to do this in TensorFlow 2.\n",
    "tf.random.set_seed(1)\n",
    "\n",
    "# This sets the random seed for all NumPy operations.\n",
    "# This is important if we are creating our data using NumPy or using any\n",
    "# NumPy functions that involve randomness.\n",
    "np.random.seed(23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "765eac04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Task 1**\n",
    "\n",
    "# Build a neural network without convolutional layers to do the classification task (hint: you will need the use of dense layers). \n",
    "# Then you can change the model structure (i.e. number of dense layers, number of neurons in dense layers or activation functions) to be able to improve network performance.\n",
    "\n",
    "def create_mlp_model_base() -> Sequential:\n",
    "    \"\"\"\n",
    "    Defines and returns the (base) MLP model architecture.\n",
    "    \"\"\"\n",
    "    # Note: we can change the model architecture here. However, it is more prudent to save the model parameters first, and then change it. \n",
    "    model = Sequential([\n",
    "        # We use the implicit input_shape here for a cleaner look.\n",
    "        Input(shape=(28, 28, 1)),\n",
    "        Normalization(),\n",
    "        Flatten(),\n",
    "        Dense(units=32, activation='relu'), # Relu is the goto activation function. We could also use LeakyRelu, tanh, sigmoid, etc.\n",
    "        Dense(units=64, activation='relu'),\n",
    "        Dense(units=32, activation='relu'),\n",
    "        Dense(units=10, activation='softmax')\n",
    "    ], name = \"Base_MLP_Model\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83172f9",
   "metadata": {},
   "source": [
    "## Creating variations of the MLP base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1216af8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mlp_model_2() -> Sequential:\n",
    "    \"\"\"\n",
    "    Defines and returns the (base) MLP model architecture.\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        # We use the implicit input_shape here for a cleaner look.\n",
    "        Input(shape=(28, 28, 1)),\n",
    "        Normalization(),\n",
    "        Flatten(),\n",
    "        Dense(units=64, activation='relu'), # Relu is the goto activation function. We could also use LeakyRelu, tanh, sigmoid, etc.\n",
    "        Dense(units=128, activation='relu'),\n",
    "        Dense(units=64, activation='relu'),\n",
    "        Dense(units=10, activation='softmax')\n",
    "    ], name = \"Base_MLP_Model_2\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "361f3d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is our first \"base\" model we created. Repeated here, because it yielded very good accuracies on both validate and test data, as well as loss. \n",
    "def create_mlp_model_3() -> Sequential:\n",
    "    \"\"\"\n",
    "    Defines and returns the (base) MLP model architecture.\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        # We use the implicit input_shape here for a cleaner look.\n",
    "        Input(shape=(28, 28, 1)),\n",
    "        Normalization(),\n",
    "        Flatten(),\n",
    "        Dense(units=128, activation='relu'), # Relu is the goto activation function. We could also use LeakyRelu, \n",
    "        Dense(units=256, activation='relu'),\n",
    "        Dense(units=64, activation='relu'),\n",
    "        Dense(units=10, activation='softmax')\n",
    "    ], name = \"Base_MLP_Model_3\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "954def66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mlp_model_wide() -> Sequential:\n",
    "    \"\"\"\n",
    "    Defines and returns the (base) MLP model architecture.\n",
    "    \"\"\"\n",
    "    # Note: we can change the model architecture here. However, it is more prudent to save the model parameters first, and then change it. \n",
    "    model = Sequential([\n",
    "        # We use the implicit input_shape here for a cleaner look.\n",
    "        Input(shape=(28, 28, 1)),\n",
    "        Normalization(),\n",
    "        Flatten(),\n",
    "        Dense(units=512, activation='relu'),\n",
    "        Dense(units=1024, activation='relu'),\n",
    "        Dense(units=256, activation='relu'),\n",
    "        Dense(units=10, activation='softmax')\n",
    "    ], name = \"Wide_MLP_Model\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6283beed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mlp_model_deep() -> Sequential:\n",
    "    \"\"\"\n",
    "    Defines and returns the (base) MLP model architecture.\n",
    "    \"\"\"\n",
    "    # Note: we can change the model architecture here. However, it is more prudent to save the model parameters first, and then change it. \n",
    "    model = Sequential([\n",
    "        # We use the implicit input_shape here for a cleaner look.\n",
    "        Input(shape=(28, 28, 1)),\n",
    "        Normalization(),\n",
    "        Flatten(),\n",
    "        Dense(units=128, activation='relu'),\n",
    "        Dense(units=256, activation='relu'),\n",
    "        Dense(units=256, activation='relu'),\n",
    "        Dense(units=256, activation='relu'),\n",
    "        Dense(units=256, activation='relu'),\n",
    "        Dense(units=64, activation='relu'),\n",
    "        Dense(units=32, activation='relu'),\n",
    "        Dense(units=10, activation='softmax')\n",
    "    ], name=\"Deep_MLP_Model\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a924894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All MLP models:\n",
    "mlp_model_functions: List[Callable] = [\n",
    "    create_mlp_model_base,\n",
    "    create_mlp_model_2,\n",
    "    create_mlp_model_3,\n",
    "    create_mlp_model_wide,\n",
    "    create_mlp_model_deep,\n",
    "]\n",
    "\n",
    "# # # Only a single model:\n",
    "# # mlp_model_functions = [\n",
    "# #     create_mlp_model_3,\n",
    "# # ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c83321",
   "metadata": {},
   "source": [
    "### Define sets of hyperparameters for the MLP models\n",
    "\n",
    "The cell below allow us to set a plethora of hyperparameters, such as the learning rate, batch_size, but also the optimiser and the number of epochs the training loop is run for. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f8d05a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define Hyperparameter Sets ---\n",
    "# Part 1 Task 3 only requires one experiment with an MLP, so we will just define one hyperparameter set for the MLP here.\n",
    "mlp_epochs: int = 100\n",
    "mlp_batch_size: int = 64\n",
    "# Experiment 1: Our baseline run\n",
    "# We choose SGD as our optimiser because it is expected that the loss landscape of our simple MLP model is largely convex. \n",
    "mlp_exp_1_config: Dict[str, Union[str, float, int]] = {\n",
    "    \"optimiser\": \"adamw\",\n",
    "    \"learning_rate\": 0.001, # The optimal, or at least a satisfactory LR will have to be identified by experimenting. We start with LR = 0.01. This does not lead to convergence of Loss and Accuracy. So LR=0.001 is used. \n",
    "    \"epochs\": mlp_epochs,\n",
    "    \"batch_size\": mlp_batch_size\n",
    "}\n",
    "\n",
    "# Experiment 2: Same as the MLP_Baseline but with a higher learning rate\n",
    "mlp_exp_2_config = {\n",
    "    \"optimiser\": \"adamw\",\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"epochs\": mlp_epochs,\n",
    "    \"batch_size\": mlp_batch_size,\n",
    "}\n",
    "\n",
    "# Experiment 3: Same as the MLP_Baseline but with a different optimizer (SGD)\n",
    "mlp_exp_3_config = {\n",
    "    \"optimiser\": \"SGD\",\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"epochs\": mlp_epochs,\n",
    "    \"batch_size\": mlp_batch_size,\n",
    "}\n",
    "\n",
    "# Experiment 4: Same as the MLP_Baseline but with a different optimizer (SGD) and a higher learning rate\n",
    "mlp_exp_4_config = {\n",
    "    \"optimiser\": \"SGD\",\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"epochs\": mlp_epochs,\n",
    "    \"batch_size\": mlp_batch_size,\n",
    "}\n",
    "\n",
    "mlp_config: List[Dict[str, Union[str, float, int]]] = [mlp_exp_1_config, mlp_exp_2_config, mlp_exp_3_config, mlp_exp_4_config]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b41f409",
   "metadata": {},
   "source": [
    "## Training the MLP models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c67af808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Adapting the normalisation layer...\n",
      "Adaptation complete.\n",
      "\n",
      "\n",
      "--- Starting Experiment: Base_MLP_Model ---\n",
      "\n",
      "--- Model Architecture ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"Base_MLP_Model\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"Base_MLP_Model\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ normalization (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Normalization</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">25,120</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,112</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">330</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ normalization (\u001b[38;5;33mNormalization\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m1\u001b[0m)      │             \u001b[38;5;34m3\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m784\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │        \u001b[38;5;34m25,120\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m2,112\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m2,080\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │           \u001b[38;5;34m330\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">29,645</span> (115.80 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m29,645\u001b[0m (115.80 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">29,642</span> (115.79 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m29,642\u001b[0m (115.79 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3</span> (16.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m3\u001b[0m (16.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Hyperparameters ---\n",
      "optimiser           : adamw\n",
      "learning_rate       : 0.001\n",
      "epochs              : 100\n",
      "batch_size          : 64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtim-vos-nl\u001b[0m (\u001b[33mtim-vos-nl-mine\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "creating run (0.0s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\TimVos\\VSC Projects\\CSE5ML\\Assessment 2\\wandb\\run-20250917_074815-1n7j83hu</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/1n7j83hu' target=\"_blank\">Base_MLP_Model-lr_0.001-bs_64-20250917-074814</a></strong> to <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2' target=\"_blank\">https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/1n7j83hu' target=\"_blank\">https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/1n7j83hu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m When using `save_best_only`, ensure that the `filepath` argument contains formatting placeholders like `{epoch:02d}` or `{batch:02d}`. This ensures correct interpretation of the logged artifacts.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m820/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7956 - loss: 0.6571\n",
      "Epoch 1: val_accuracy improved from None to 0.95367, saving model to MLP_Models\\Base_MLP_Model-lr_0.001-bs_64-20250917-074814\\best_model_epoch-01_val_acc-0.9537.keras\n",
      "\n",
      "Epoch 1: val_accuracy improved from None to 0.95367, saving model to wandb_models/Base_MLP_Model-lr_0.001-bs_64-20250917-074814/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.8899 - loss: 0.3595 - val_accuracy: 0.9537 - val_loss: 0.1532\n",
      "Epoch 2/100\n",
      "\u001b[1m814/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9472 - loss: 0.1760\n",
      "Epoch 2: val_accuracy improved from 0.95367 to 0.96533, saving model to MLP_Models\\Base_MLP_Model-lr_0.001-bs_64-20250917-074814\\best_model_epoch-02_val_acc-0.9653.keras\n",
      "\n",
      "Epoch 2: val_accuracy improved from 0.95367 to 0.96533, saving model to wandb_models/Base_MLP_Model-lr_0.001-bs_64-20250917-074814/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9513 - loss: 0.1610 - val_accuracy: 0.9653 - val_loss: 0.1169\n",
      "Epoch 3/100\n",
      "\u001b[1m828/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9606 - loss: 0.1320\n",
      "Epoch 3: val_accuracy improved from 0.96533 to 0.97033, saving model to MLP_Models\\Base_MLP_Model-lr_0.001-bs_64-20250917-074814\\best_model_epoch-03_val_acc-0.9703.keras\n",
      "\n",
      "Epoch 3: val_accuracy improved from 0.96533 to 0.97033, saving model to wandb_models/Base_MLP_Model-lr_0.001-bs_64-20250917-074814/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9627 - loss: 0.1231 - val_accuracy: 0.9703 - val_loss: 0.1036\n",
      "Epoch 4/100\n",
      "\u001b[1m818/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9682 - loss: 0.1064\n",
      "Epoch 4: val_accuracy improved from 0.97033 to 0.97050, saving model to MLP_Models\\Base_MLP_Model-lr_0.001-bs_64-20250917-074814\\best_model_epoch-04_val_acc-0.9705.keras\n",
      "\n",
      "Epoch 4: val_accuracy improved from 0.97033 to 0.97050, saving model to wandb_models/Base_MLP_Model-lr_0.001-bs_64-20250917-074814/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9695 - loss: 0.1005 - val_accuracy: 0.9705 - val_loss: 0.0991\n",
      "Epoch 5/100\n",
      "\u001b[1m836/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9741 - loss: 0.0890\n",
      "Epoch 5: val_accuracy did not improve from 0.97050\n",
      "\n",
      "Epoch 5: val_accuracy did not improve from 0.97050\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9742 - loss: 0.0856 - val_accuracy: 0.9695 - val_loss: 0.1034\n",
      "Epoch 6/100\n",
      "\u001b[1m822/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9766 - loss: 0.0790\n",
      "Epoch 6: val_accuracy improved from 0.97050 to 0.97217, saving model to MLP_Models\\Base_MLP_Model-lr_0.001-bs_64-20250917-074814\\best_model_epoch-06_val_acc-0.9722.keras\n",
      "\n",
      "Epoch 6: val_accuracy improved from 0.97050 to 0.97217, saving model to wandb_models/Base_MLP_Model-lr_0.001-bs_64-20250917-074814/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9771 - loss: 0.0759 - val_accuracy: 0.9722 - val_loss: 0.1002\n",
      "Epoch 7/100\n",
      "\u001b[1m807/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9796 - loss: 0.0692\n",
      "Epoch 7: val_accuracy did not improve from 0.97217\n",
      "\n",
      "Epoch 7: val_accuracy did not improve from 0.97217\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9800 - loss: 0.0665 - val_accuracy: 0.9702 - val_loss: 0.1082\n",
      "Epoch 8/100\n",
      "\u001b[1m826/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9810 - loss: 0.0641\n",
      "Epoch 8: val_accuracy did not improve from 0.97217\n",
      "\n",
      "Epoch 8: val_accuracy did not improve from 0.97217\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9816 - loss: 0.0609 - val_accuracy: 0.9695 - val_loss: 0.1186\n",
      "Epoch 9/100\n",
      "\u001b[1m829/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9825 - loss: 0.0565\n",
      "Epoch 9: val_accuracy did not improve from 0.97217\n",
      "\n",
      "Epoch 9: val_accuracy did not improve from 0.97217\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9831 - loss: 0.0547 - val_accuracy: 0.9675 - val_loss: 0.1244\n",
      "Epoch 10/100\n",
      "\u001b[1m821/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9842 - loss: 0.0530\n",
      "Epoch 10: val_accuracy did not improve from 0.97217\n",
      "\n",
      "Epoch 10: val_accuracy did not improve from 0.97217\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9850 - loss: 0.0498 - val_accuracy: 0.9708 - val_loss: 0.1197\n",
      "Epoch 11/100\n",
      "\u001b[1m827/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9838 - loss: 0.0495\n",
      "Epoch 11: val_accuracy did not improve from 0.97217\n",
      "\n",
      "Epoch 11: val_accuracy did not improve from 0.97217\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9839 - loss: 0.0487 - val_accuracy: 0.9693 - val_loss: 0.1206\n",
      "Epoch 12/100\n",
      "\u001b[1m839/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9848 - loss: 0.0483\n",
      "Epoch 12: val_accuracy did not improve from 0.97217\n",
      "\n",
      "Epoch 12: val_accuracy did not improve from 0.97217\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9855 - loss: 0.0456 - val_accuracy: 0.9690 - val_loss: 0.1225\n",
      "Epoch 13/100\n",
      "\u001b[1m834/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9866 - loss: 0.0426\n",
      "Epoch 13: val_accuracy did not improve from 0.97217\n",
      "\n",
      "Epoch 13: val_accuracy did not improve from 0.97217\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9870 - loss: 0.0416 - val_accuracy: 0.9688 - val_loss: 0.1337\n",
      "Epoch 14/100\n",
      "\u001b[1m835/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9862 - loss: 0.0407\n",
      "Epoch 14: val_accuracy did not improve from 0.97217\n",
      "\n",
      "Epoch 14: val_accuracy did not improve from 0.97217\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9869 - loss: 0.0388 - val_accuracy: 0.9713 - val_loss: 0.1253\n",
      "Epoch 15/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9862 - loss: 0.0414\n",
      "Epoch 15: val_accuracy did not improve from 0.97217\n",
      "\n",
      "Epoch 15: val_accuracy did not improve from 0.97217\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9867 - loss: 0.0384 - val_accuracy: 0.9705 - val_loss: 0.1338\n",
      "Epoch 16/100\n",
      "\u001b[1m818/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9879 - loss: 0.0366\n",
      "Epoch 16: val_accuracy did not improve from 0.97217\n",
      "\n",
      "Epoch 16: val_accuracy did not improve from 0.97217\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9884 - loss: 0.0358 - val_accuracy: 0.9713 - val_loss: 0.1424\n",
      "\n",
      "Training history saved to: MLP_Models\\Base_MLP_Model-lr_0.001-bs_64-20250917-074814\\training_history.pkl\n",
      "\n",
      "--- Peak Performance Summary ---\n",
      "Best validation accuracy:           0.9722\n",
      "Associated training accuracy:       0.9771\n",
      "Occurred at epoch:                  6\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch/accuracy</td><td>▁▅▆▇▇▇▇█████████</td></tr><tr><td>epoch/epoch</td><td>▁▁▂▂▃▃▄▄▅▅▆▆▇▇██</td></tr><tr><td>epoch/learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/loss</td><td>█▄▃▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/val_accuracy</td><td>▁▅▇▇▇█▇▇▆▇▇▇▇█▇█</td></tr><tr><td>epoch/val_loss</td><td>█▃▂▁▂▁▂▄▄▄▄▄▅▄▅▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch/accuracy</td><td>0.98841</td></tr><tr><td>epoch/epoch</td><td>15</td></tr><tr><td>epoch/learning_rate</td><td>0.001</td></tr><tr><td>epoch/loss</td><td>0.03577</td></tr><tr><td>epoch/val_accuracy</td><td>0.97133</td></tr><tr><td>epoch/val_loss</td><td>0.14243</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Base_MLP_Model-lr_0.001-bs_64-20250917-074814</strong> at: <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/1n7j83hu' target=\"_blank\">https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/1n7j83hu</a><br> View project at: <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2' target=\"_blank\">https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2</a><br>Synced 5 W&B file(s), 0 media file(s), 10 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250917_074815-1n7j83hu\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Adapting the normalisation layer...\n",
      "Adaptation complete.\n",
      "\n",
      "\n",
      "--- Starting Experiment: Base_MLP_Model ---\n",
      "\n",
      "--- Model Architecture ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"Base_MLP_Model\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"Base_MLP_Model\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ normalization_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Normalization</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">25,120</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,112</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">330</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ normalization_1 (\u001b[38;5;33mNormalization\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m1\u001b[0m)      │             \u001b[38;5;34m3\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_1 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m784\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │        \u001b[38;5;34m25,120\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m2,112\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m2,080\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │           \u001b[38;5;34m330\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">29,645</span> (115.80 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m29,645\u001b[0m (115.80 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">29,642</span> (115.79 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m29,642\u001b[0m (115.79 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3</span> (16.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m3\u001b[0m (16.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Hyperparameters ---\n",
      "optimiser           : adamw\n",
      "learning_rate       : 0.01\n",
      "epochs              : 100\n",
      "batch_size          : 64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for wandb.init()..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\TimVos\\VSC Projects\\CSE5ML\\Assessment 2\\wandb\\run-20250917_074912-97nr7na9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/97nr7na9' target=\"_blank\">Base_MLP_Model-lr_0.01-bs_64-20250917-074912</a></strong> to <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2' target=\"_blank\">https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/97nr7na9' target=\"_blank\">https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/97nr7na9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m812/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8248 - loss: 0.5539\n",
      "Epoch 1: val_accuracy improved from None to 0.93300, saving model to MLP_Models\\Base_MLP_Model-lr_0.01-bs_64-20250917-074912\\best_model_epoch-01_val_acc-0.9330.keras\n",
      "\n",
      "Epoch 1: val_accuracy improved from None to 0.93300, saving model to wandb_models/Base_MLP_Model-lr_0.01-bs_64-20250917-074912/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.8875 - loss: 0.3720 - val_accuracy: 0.9330 - val_loss: 0.2340\n",
      "Epoch 2/100\n",
      "\u001b[1m832/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9222 - loss: 0.2680\n",
      "Epoch 2: val_accuracy improved from 0.93300 to 0.95117, saving model to MLP_Models\\Base_MLP_Model-lr_0.01-bs_64-20250917-074912\\best_model_epoch-02_val_acc-0.9512.keras\n",
      "\n",
      "Epoch 2: val_accuracy improved from 0.93300 to 0.95117, saving model to wandb_models/Base_MLP_Model-lr_0.01-bs_64-20250917-074912/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9254 - loss: 0.2600 - val_accuracy: 0.9512 - val_loss: 0.1678\n",
      "Epoch 3/100\n",
      "\u001b[1m815/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9317 - loss: 0.2413\n",
      "Epoch 3: val_accuracy did not improve from 0.95117\n",
      "\n",
      "Epoch 3: val_accuracy did not improve from 0.95117\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9323 - loss: 0.2402 - val_accuracy: 0.9467 - val_loss: 0.1901\n",
      "Epoch 4/100\n",
      "\u001b[1m811/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9382 - loss: 0.2273\n",
      "Epoch 4: val_accuracy improved from 0.95117 to 0.95233, saving model to MLP_Models\\Base_MLP_Model-lr_0.01-bs_64-20250917-074912\\best_model_epoch-04_val_acc-0.9523.keras\n",
      "\n",
      "Epoch 4: val_accuracy improved from 0.95117 to 0.95233, saving model to wandb_models/Base_MLP_Model-lr_0.01-bs_64-20250917-074912/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9376 - loss: 0.2273 - val_accuracy: 0.9523 - val_loss: 0.1892\n",
      "Epoch 5/100\n",
      "\u001b[1m835/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9408 - loss: 0.2192\n",
      "Epoch 5: val_accuracy improved from 0.95233 to 0.95633, saving model to MLP_Models\\Base_MLP_Model-lr_0.01-bs_64-20250917-074912\\best_model_epoch-05_val_acc-0.9563.keras\n",
      "\n",
      "Epoch 5: val_accuracy improved from 0.95233 to 0.95633, saving model to wandb_models/Base_MLP_Model-lr_0.01-bs_64-20250917-074912/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9395 - loss: 0.2235 - val_accuracy: 0.9563 - val_loss: 0.1724\n",
      "Epoch 6/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9426 - loss: 0.2153\n",
      "Epoch 6: val_accuracy did not improve from 0.95633\n",
      "\n",
      "Epoch 6: val_accuracy did not improve from 0.95633\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9429 - loss: 0.2148 - val_accuracy: 0.9535 - val_loss: 0.1808\n",
      "Epoch 7/100\n",
      "\u001b[1m829/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9480 - loss: 0.1981\n",
      "Epoch 7: val_accuracy did not improve from 0.95633\n",
      "\n",
      "Epoch 7: val_accuracy did not improve from 0.95633\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9466 - loss: 0.2020 - val_accuracy: 0.9548 - val_loss: 0.1760\n",
      "Epoch 8/100\n",
      "\u001b[1m839/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9449 - loss: 0.2057\n",
      "Epoch 8: val_accuracy improved from 0.95633 to 0.95967, saving model to MLP_Models\\Base_MLP_Model-lr_0.01-bs_64-20250917-074912\\best_model_epoch-08_val_acc-0.9597.keras\n",
      "\n",
      "Epoch 8: val_accuracy improved from 0.95633 to 0.95967, saving model to wandb_models/Base_MLP_Model-lr_0.01-bs_64-20250917-074912/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9460 - loss: 0.2040 - val_accuracy: 0.9597 - val_loss: 0.1549\n",
      "Epoch 9/100\n",
      "\u001b[1m810/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9491 - loss: 0.1881\n",
      "Epoch 9: val_accuracy did not improve from 0.95967\n",
      "\n",
      "Epoch 9: val_accuracy did not improve from 0.95967\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9494 - loss: 0.1893 - val_accuracy: 0.9567 - val_loss: 0.1666\n",
      "Epoch 10/100\n",
      "\u001b[1m818/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9503 - loss: 0.1949\n",
      "Epoch 10: val_accuracy did not improve from 0.95967\n",
      "\n",
      "Epoch 10: val_accuracy did not improve from 0.95967\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9488 - loss: 0.1961 - val_accuracy: 0.9503 - val_loss: 0.1940\n",
      "Epoch 11/100\n",
      "\u001b[1m835/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9497 - loss: 0.1948\n",
      "Epoch 11: val_accuracy did not improve from 0.95967\n",
      "\n",
      "Epoch 11: val_accuracy did not improve from 0.95967\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9511 - loss: 0.1876 - val_accuracy: 0.9555 - val_loss: 0.1664\n",
      "Epoch 12/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9525 - loss: 0.1803\n",
      "Epoch 12: val_accuracy improved from 0.95967 to 0.96167, saving model to MLP_Models\\Base_MLP_Model-lr_0.01-bs_64-20250917-074912\\best_model_epoch-12_val_acc-0.9617.keras\n",
      "\n",
      "Epoch 12: val_accuracy improved from 0.95967 to 0.96167, saving model to wandb_models/Base_MLP_Model-lr_0.01-bs_64-20250917-074912/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9523 - loss: 0.1826 - val_accuracy: 0.9617 - val_loss: 0.1582\n",
      "Epoch 13/100\n",
      "\u001b[1m812/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9498 - loss: 0.1932\n",
      "Epoch 13: val_accuracy did not improve from 0.96167\n",
      "\n",
      "Epoch 13: val_accuracy did not improve from 0.96167\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9507 - loss: 0.1911 - val_accuracy: 0.9617 - val_loss: 0.1663\n",
      "Epoch 14/100\n",
      "\u001b[1m838/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9528 - loss: 0.1850\n",
      "Epoch 14: val_accuracy did not improve from 0.96167\n",
      "\n",
      "Epoch 14: val_accuracy did not improve from 0.96167\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9516 - loss: 0.1885 - val_accuracy: 0.9592 - val_loss: 0.1655\n",
      "Epoch 15/100\n",
      "\u001b[1m823/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9541 - loss: 0.1777\n",
      "Epoch 15: val_accuracy did not improve from 0.96167\n",
      "\n",
      "Epoch 15: val_accuracy did not improve from 0.96167\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9540 - loss: 0.1755 - val_accuracy: 0.9532 - val_loss: 0.1753\n",
      "Epoch 16/100\n",
      "\u001b[1m836/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9554 - loss: 0.1720\n",
      "Epoch 16: val_accuracy did not improve from 0.96167\n",
      "\n",
      "Epoch 16: val_accuracy did not improve from 0.96167\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9556 - loss: 0.1700 - val_accuracy: 0.9560 - val_loss: 0.1763\n",
      "Epoch 17/100\n",
      "\u001b[1m838/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9548 - loss: 0.1726\n",
      "Epoch 17: val_accuracy did not improve from 0.96167\n",
      "\n",
      "Epoch 17: val_accuracy did not improve from 0.96167\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9545 - loss: 0.1718 - val_accuracy: 0.9538 - val_loss: 0.1884\n",
      "Epoch 18/100\n",
      "\u001b[1m826/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9551 - loss: 0.1750\n",
      "Epoch 18: val_accuracy did not improve from 0.96167\n",
      "\n",
      "Epoch 18: val_accuracy did not improve from 0.96167\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9550 - loss: 0.1720 - val_accuracy: 0.9540 - val_loss: 0.1836\n",
      "Epoch 19/100\n",
      "\u001b[1m813/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9546 - loss: 0.1702\n",
      "Epoch 19: val_accuracy did not improve from 0.96167\n",
      "\n",
      "Epoch 19: val_accuracy did not improve from 0.96167\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9541 - loss: 0.1721 - val_accuracy: 0.9603 - val_loss: 0.1623\n",
      "Epoch 20/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9554 - loss: 0.1765\n",
      "Epoch 20: val_accuracy did not improve from 0.96167\n",
      "\n",
      "Epoch 20: val_accuracy did not improve from 0.96167\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9547 - loss: 0.1759 - val_accuracy: 0.9575 - val_loss: 0.1811\n",
      "Epoch 21/100\n",
      "\u001b[1m835/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9560 - loss: 0.1704\n",
      "Epoch 21: val_accuracy did not improve from 0.96167\n",
      "\n",
      "Epoch 21: val_accuracy did not improve from 0.96167\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9564 - loss: 0.1689 - val_accuracy: 0.9548 - val_loss: 0.1811\n",
      "Epoch 22/100\n",
      "\u001b[1m824/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9578 - loss: 0.1645\n",
      "Epoch 22: val_accuracy did not improve from 0.96167\n",
      "\n",
      "Epoch 22: val_accuracy did not improve from 0.96167\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9570 - loss: 0.1672 - val_accuracy: 0.9465 - val_loss: 0.2125\n",
      "\n",
      "Training history saved to: MLP_Models\\Base_MLP_Model-lr_0.01-bs_64-20250917-074912\\training_history.pkl\n",
      "\n",
      "--- Peak Performance Summary ---\n",
      "Best validation accuracy:           0.9617\n",
      "Associated training accuracy:       0.9523\n",
      "Occurred at epoch:                  12\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch/accuracy</td><td>▁▅▆▆▆▇▇▇▇▇▇█▇▇████████</td></tr><tr><td>epoch/epoch</td><td>▁▁▂▂▂▃▃▃▄▄▄▅▅▅▆▆▆▇▇▇██</td></tr><tr><td>epoch/learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/loss</td><td>█▄▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/val_accuracy</td><td>▁▅▄▆▇▆▆█▇▅▆██▇▆▇▆▆█▇▆▄</td></tr><tr><td>epoch/val_loss</td><td>█▂▄▄▃▃▃▁▂▄▂▁▂▂▃▃▄▄▂▃▃▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch/accuracy</td><td>0.95696</td></tr><tr><td>epoch/epoch</td><td>21</td></tr><tr><td>epoch/learning_rate</td><td>0.01</td></tr><tr><td>epoch/loss</td><td>0.16722</td></tr><tr><td>epoch/val_accuracy</td><td>0.9465</td></tr><tr><td>epoch/val_loss</td><td>0.21253</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Base_MLP_Model-lr_0.01-bs_64-20250917-074912</strong> at: <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/97nr7na9' target=\"_blank\">https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/97nr7na9</a><br> View project at: <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2' target=\"_blank\">https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2</a><br>Synced 5 W&B file(s), 0 media file(s), 12 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250917_074912-97nr7na9\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Adapting the normalisation layer...\n",
      "Adaptation complete.\n",
      "\n",
      "\n",
      "--- Starting Experiment: Base_MLP_Model ---\n",
      "\n",
      "--- Model Architecture ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"Base_MLP_Model\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"Base_MLP_Model\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ normalization_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Normalization</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">25,120</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,112</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">330</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ normalization_2 (\u001b[38;5;33mNormalization\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m1\u001b[0m)      │             \u001b[38;5;34m3\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_2 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m784\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_8 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │        \u001b[38;5;34m25,120\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_9 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m2,112\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_10 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m2,080\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_11 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │           \u001b[38;5;34m330\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">29,645</span> (115.80 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m29,645\u001b[0m (115.80 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">29,642</span> (115.79 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m29,642\u001b[0m (115.79 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3</span> (16.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m3\u001b[0m (16.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Hyperparameters ---\n",
      "optimiser           : SGD\n",
      "learning_rate       : 0.001\n",
      "epochs              : 100\n",
      "batch_size          : 64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "creating run (0.0s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\TimVos\\VSC Projects\\CSE5ML\\Assessment 2\\wandb\\run-20250917_075025-si261zoy</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/si261zoy' target=\"_blank\">Base_MLP_Model-lr_0.001-bs_64-20250917-075025</a></strong> to <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2' target=\"_blank\">https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/si261zoy' target=\"_blank\">https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/si261zoy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m813/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.1944 - loss: 2.1785\n",
      "Epoch 1: val_accuracy improved from None to 0.64217, saving model to MLP_Models\\Base_MLP_Model-lr_0.001-bs_64-20250917-075025\\best_model_epoch-01_val_acc-0.6422.keras\n",
      "\n",
      "Epoch 1: val_accuracy improved from None to 0.64217, saving model to wandb_models/Base_MLP_Model-lr_0.001-bs_64-20250917-075025/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.3303 - loss: 1.9432 - val_accuracy: 0.6422 - val_loss: 1.4364\n",
      "Epoch 2/100\n",
      "\u001b[1m813/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.6791 - loss: 1.2516\n",
      "Epoch 2: val_accuracy improved from 0.64217 to 0.83050, saving model to MLP_Models\\Base_MLP_Model-lr_0.001-bs_64-20250917-075025\\best_model_epoch-02_val_acc-0.8305.keras\n",
      "\n",
      "Epoch 2: val_accuracy improved from 0.64217 to 0.83050, saving model to wandb_models/Base_MLP_Model-lr_0.001-bs_64-20250917-075025/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.7291 - loss: 1.0544 - val_accuracy: 0.8305 - val_loss: 0.6905\n",
      "Epoch 3/100\n",
      "\u001b[1m819/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8117 - loss: 0.6977\n",
      "Epoch 3: val_accuracy improved from 0.83050 to 0.87183, saving model to MLP_Models\\Base_MLP_Model-lr_0.001-bs_64-20250917-075025\\best_model_epoch-03_val_acc-0.8718.keras\n",
      "\n",
      "Epoch 3: val_accuracy improved from 0.83050 to 0.87183, saving model to wandb_models/Base_MLP_Model-lr_0.001-bs_64-20250917-075025/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.8253 - loss: 0.6412 - val_accuracy: 0.8718 - val_loss: 0.4782\n",
      "Epoch 4/100\n",
      "\u001b[1m832/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8495 - loss: 0.5301\n",
      "Epoch 4: val_accuracy improved from 0.87183 to 0.88983, saving model to MLP_Models\\Base_MLP_Model-lr_0.001-bs_64-20250917-075025\\best_model_epoch-04_val_acc-0.8898.keras\n",
      "\n",
      "Epoch 4: val_accuracy improved from 0.87183 to 0.88983, saving model to wandb_models/Base_MLP_Model-lr_0.001-bs_64-20250917-075025/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.8556 - loss: 0.5083 - val_accuracy: 0.8898 - val_loss: 0.3966\n",
      "Epoch 5/100\n",
      "\u001b[1m840/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8682 - loss: 0.4565\n",
      "Epoch 5: val_accuracy improved from 0.88983 to 0.89950, saving model to MLP_Models\\Base_MLP_Model-lr_0.001-bs_64-20250917-075025\\best_model_epoch-05_val_acc-0.8995.keras\n",
      "\n",
      "Epoch 5: val_accuracy improved from 0.88983 to 0.89950, saving model to wandb_models/Base_MLP_Model-lr_0.001-bs_64-20250917-075025/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.8716 - loss: 0.4453 - val_accuracy: 0.8995 - val_loss: 0.3525\n",
      "Epoch 6/100\n",
      "\u001b[1m811/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8796 - loss: 0.4134\n",
      "Epoch 6: val_accuracy improved from 0.89950 to 0.90650, saving model to MLP_Models\\Base_MLP_Model-lr_0.001-bs_64-20250917-075025\\best_model_epoch-06_val_acc-0.9065.keras\n",
      "\n",
      "Epoch 6: val_accuracy improved from 0.89950 to 0.90650, saving model to wandb_models/Base_MLP_Model-lr_0.001-bs_64-20250917-075025/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.8818 - loss: 0.4067 - val_accuracy: 0.9065 - val_loss: 0.3238\n",
      "Epoch 7/100\n",
      "\u001b[1m823/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8876 - loss: 0.3836\n",
      "Epoch 7: val_accuracy improved from 0.90650 to 0.91200, saving model to MLP_Models\\Base_MLP_Model-lr_0.001-bs_64-20250917-075025\\best_model_epoch-07_val_acc-0.9120.keras\n",
      "\n",
      "Epoch 7: val_accuracy improved from 0.90650 to 0.91200, saving model to wandb_models/Base_MLP_Model-lr_0.001-bs_64-20250917-075025/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.8892 - loss: 0.3796 - val_accuracy: 0.9120 - val_loss: 0.3030\n",
      "Epoch 8/100\n",
      "\u001b[1m817/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8937 - loss: 0.3614\n",
      "Epoch 8: val_accuracy improved from 0.91200 to 0.91683, saving model to MLP_Models\\Base_MLP_Model-lr_0.001-bs_64-20250917-075025\\best_model_epoch-08_val_acc-0.9168.keras\n",
      "\n",
      "Epoch 8: val_accuracy improved from 0.91200 to 0.91683, saving model to wandb_models/Base_MLP_Model-lr_0.001-bs_64-20250917-075025/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.8950 - loss: 0.3590 - val_accuracy: 0.9168 - val_loss: 0.2869\n",
      "Epoch 9/100\n",
      "\u001b[1m822/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8987 - loss: 0.3439\n",
      "Epoch 9: val_accuracy improved from 0.91683 to 0.92033, saving model to MLP_Models\\Base_MLP_Model-lr_0.001-bs_64-20250917-075025\\best_model_epoch-09_val_acc-0.9203.keras\n",
      "\n",
      "Epoch 9: val_accuracy improved from 0.91683 to 0.92033, saving model to wandb_models/Base_MLP_Model-lr_0.001-bs_64-20250917-075025/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.8998 - loss: 0.3425 - val_accuracy: 0.9203 - val_loss: 0.2739\n",
      "Epoch 10/100\n",
      "\u001b[1m828/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9025 - loss: 0.3295\n",
      "Epoch 10: val_accuracy improved from 0.92033 to 0.92167, saving model to MLP_Models\\Base_MLP_Model-lr_0.001-bs_64-20250917-075025\\best_model_epoch-10_val_acc-0.9217.keras\n",
      "\n",
      "Epoch 10: val_accuracy improved from 0.92033 to 0.92167, saving model to wandb_models/Base_MLP_Model-lr_0.001-bs_64-20250917-075025/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9034 - loss: 0.3289 - val_accuracy: 0.9217 - val_loss: 0.2632\n",
      "Epoch 11/100\n",
      "\u001b[1m818/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9061 - loss: 0.3174\n",
      "Epoch 11: val_accuracy improved from 0.92167 to 0.92350, saving model to MLP_Models\\Base_MLP_Model-lr_0.001-bs_64-20250917-075025\\best_model_epoch-11_val_acc-0.9235.keras\n",
      "\n",
      "Epoch 11: val_accuracy improved from 0.92167 to 0.92350, saving model to wandb_models/Base_MLP_Model-lr_0.001-bs_64-20250917-075025/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9066 - loss: 0.3171 - val_accuracy: 0.9235 - val_loss: 0.2540\n",
      "Epoch 12/100\n",
      "\u001b[1m837/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9101 - loss: 0.3067\n",
      "Epoch 12: val_accuracy improved from 0.92350 to 0.92567, saving model to MLP_Models\\Base_MLP_Model-lr_0.001-bs_64-20250917-075025\\best_model_epoch-12_val_acc-0.9257.keras\n",
      "\n",
      "Epoch 12: val_accuracy improved from 0.92350 to 0.92567, saving model to wandb_models/Base_MLP_Model-lr_0.001-bs_64-20250917-075025/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9097 - loss: 0.3068 - val_accuracy: 0.9257 - val_loss: 0.2460\n",
      "Epoch 13/100\n",
      "\u001b[1m830/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9127 - loss: 0.2974\n",
      "Epoch 13: val_accuracy improved from 0.92567 to 0.92900, saving model to MLP_Models\\Base_MLP_Model-lr_0.001-bs_64-20250917-075025\\best_model_epoch-13_val_acc-0.9290.keras\n",
      "\n",
      "Epoch 13: val_accuracy improved from 0.92567 to 0.92900, saving model to wandb_models/Base_MLP_Model-lr_0.001-bs_64-20250917-075025/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9123 - loss: 0.2977 - val_accuracy: 0.9290 - val_loss: 0.2390\n",
      "Epoch 14/100\n",
      "\u001b[1m806/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9151 - loss: 0.2890\n",
      "Epoch 14: val_accuracy improved from 0.92900 to 0.93033, saving model to MLP_Models\\Base_MLP_Model-lr_0.001-bs_64-20250917-075025\\best_model_epoch-14_val_acc-0.9303.keras\n",
      "\n",
      "Epoch 14: val_accuracy improved from 0.92900 to 0.93033, saving model to wandb_models/Base_MLP_Model-lr_0.001-bs_64-20250917-075025/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9147 - loss: 0.2894 - val_accuracy: 0.9303 - val_loss: 0.2327\n",
      "Epoch 15/100\n",
      "\u001b[1m818/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9174 - loss: 0.2815\n",
      "Epoch 15: val_accuracy improved from 0.93033 to 0.93233, saving model to MLP_Models\\Base_MLP_Model-lr_0.001-bs_64-20250917-075025\\best_model_epoch-15_val_acc-0.9323.keras\n",
      "\n",
      "Epoch 15: val_accuracy improved from 0.93033 to 0.93233, saving model to wandb_models/Base_MLP_Model-lr_0.001-bs_64-20250917-075025/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9170 - loss: 0.2820 - val_accuracy: 0.9323 - val_loss: 0.2271\n",
      "Epoch 16/100\n",
      "\u001b[1m826/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9197 - loss: 0.2746\n",
      "Epoch 16: val_accuracy improved from 0.93233 to 0.93483, saving model to MLP_Models\\Base_MLP_Model-lr_0.001-bs_64-20250917-075025\\best_model_epoch-16_val_acc-0.9348.keras\n",
      "\n",
      "Epoch 16: val_accuracy improved from 0.93233 to 0.93483, saving model to wandb_models/Base_MLP_Model-lr_0.001-bs_64-20250917-075025/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9191 - loss: 0.2751 - val_accuracy: 0.9348 - val_loss: 0.2219\n",
      "Epoch 17/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9215 - loss: 0.2683\n",
      "Epoch 17: val_accuracy improved from 0.93483 to 0.93583, saving model to MLP_Models\\Base_MLP_Model-lr_0.001-bs_64-20250917-075025\\best_model_epoch-17_val_acc-0.9358.keras\n",
      "\n",
      "Epoch 17: val_accuracy improved from 0.93483 to 0.93583, saving model to wandb_models/Base_MLP_Model-lr_0.001-bs_64-20250917-075025/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9206 - loss: 0.2689 - val_accuracy: 0.9358 - val_loss: 0.2170\n",
      "Epoch 18/100\n",
      "\u001b[1m828/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9229 - loss: 0.2624\n",
      "Epoch 18: val_accuracy improved from 0.93583 to 0.93733, saving model to MLP_Models\\Base_MLP_Model-lr_0.001-bs_64-20250917-075025\\best_model_epoch-18_val_acc-0.9373.keras\n",
      "\n",
      "Epoch 18: val_accuracy improved from 0.93583 to 0.93733, saving model to wandb_models/Base_MLP_Model-lr_0.001-bs_64-20250917-075025/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9220 - loss: 0.2630 - val_accuracy: 0.9373 - val_loss: 0.2126\n",
      "Epoch 19/100\n",
      "\u001b[1m837/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9249 - loss: 0.2570\n",
      "Epoch 19: val_accuracy improved from 0.93733 to 0.93867, saving model to MLP_Models\\Base_MLP_Model-lr_0.001-bs_64-20250917-075025\\best_model_epoch-19_val_acc-0.9387.keras\n",
      "\n",
      "Epoch 19: val_accuracy improved from 0.93733 to 0.93867, saving model to wandb_models/Base_MLP_Model-lr_0.001-bs_64-20250917-075025/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9236 - loss: 0.2576 - val_accuracy: 0.9387 - val_loss: 0.2085\n",
      "Epoch 20/100\n",
      "\u001b[1m820/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9261 - loss: 0.2519\n",
      "Epoch 20: val_accuracy improved from 0.93867 to 0.94017, saving model to MLP_Models\\Base_MLP_Model-lr_0.001-bs_64-20250917-075025\\best_model_epoch-20_val_acc-0.9402.keras\n",
      "\n",
      "Epoch 20: val_accuracy improved from 0.93867 to 0.94017, saving model to wandb_models/Base_MLP_Model-lr_0.001-bs_64-20250917-075025/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9248 - loss: 0.2525 - val_accuracy: 0.9402 - val_loss: 0.2047\n",
      "Epoch 21/100\n",
      "\u001b[1m835/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9276 - loss: 0.2471\n",
      "Epoch 21: val_accuracy improved from 0.94017 to 0.94183, saving model to MLP_Models\\Base_MLP_Model-lr_0.001-bs_64-20250917-075025\\best_model_epoch-21_val_acc-0.9418.keras\n",
      "\n",
      "Epoch 21: val_accuracy improved from 0.94017 to 0.94183, saving model to wandb_models/Base_MLP_Model-lr_0.001-bs_64-20250917-075025/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9263 - loss: 0.2477 - val_accuracy: 0.9418 - val_loss: 0.2012\n",
      "Epoch 22/100\n",
      "\u001b[1m813/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9291 - loss: 0.2427\n",
      "Epoch 22: val_accuracy improved from 0.94183 to 0.94350, saving model to MLP_Models\\Base_MLP_Model-lr_0.001-bs_64-20250917-075025\\best_model_epoch-22_val_acc-0.9435.keras\n",
      "\n",
      "Epoch 22: val_accuracy improved from 0.94183 to 0.94350, saving model to wandb_models/Base_MLP_Model-lr_0.001-bs_64-20250917-075025/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9276 - loss: 0.2432 - val_accuracy: 0.9435 - val_loss: 0.1979\n",
      "Epoch 23/100\n",
      "\u001b[1m823/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9306 - loss: 0.2384\n",
      "Epoch 23: val_accuracy improved from 0.94350 to 0.94433, saving model to MLP_Models\\Base_MLP_Model-lr_0.001-bs_64-20250917-075025\\best_model_epoch-23_val_acc-0.9443.keras\n",
      "\n",
      "Epoch 23: val_accuracy improved from 0.94350 to 0.94433, saving model to wandb_models/Base_MLP_Model-lr_0.001-bs_64-20250917-075025/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9290 - loss: 0.2390 - val_accuracy: 0.9443 - val_loss: 0.1948\n",
      "Epoch 24/100\n",
      "\u001b[1m820/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9319 - loss: 0.2344\n",
      "Epoch 24: val_accuracy improved from 0.94433 to 0.94450, saving model to MLP_Models\\Base_MLP_Model-lr_0.001-bs_64-20250917-075025\\best_model_epoch-24_val_acc-0.9445.keras\n",
      "\n",
      "Epoch 24: val_accuracy improved from 0.94433 to 0.94450, saving model to wandb_models/Base_MLP_Model-lr_0.001-bs_64-20250917-075025/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9304 - loss: 0.2350 - val_accuracy: 0.9445 - val_loss: 0.1918\n",
      "Epoch 25/100\n",
      "\u001b[1m830/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9329 - loss: 0.2306\n",
      "Epoch 25: val_accuracy improved from 0.94450 to 0.94550, saving model to MLP_Models\\Base_MLP_Model-lr_0.001-bs_64-20250917-075025\\best_model_epoch-25_val_acc-0.9455.keras\n",
      "\n",
      "Epoch 25: val_accuracy improved from 0.94450 to 0.94550, saving model to wandb_models/Base_MLP_Model-lr_0.001-bs_64-20250917-075025/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9317 - loss: 0.2312 - val_accuracy: 0.9455 - val_loss: 0.1890\n",
      "Epoch 26/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9339 - loss: 0.2270\n",
      "Epoch 26: val_accuracy improved from 0.94550 to 0.94617, saving model to MLP_Models\\Base_MLP_Model-lr_0.001-bs_64-20250917-075025\\best_model_epoch-26_val_acc-0.9462.keras\n",
      "\n",
      "Epoch 26: val_accuracy improved from 0.94550 to 0.94617, saving model to wandb_models/Base_MLP_Model-lr_0.001-bs_64-20250917-075025/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9330 - loss: 0.2276 - val_accuracy: 0.9462 - val_loss: 0.1863\n",
      "Epoch 27/100\n",
      "\u001b[1m839/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9354 - loss: 0.2235\n",
      "Epoch 27: val_accuracy improved from 0.94617 to 0.94750, saving model to MLP_Models\\Base_MLP_Model-lr_0.001-bs_64-20250917-075025\\best_model_epoch-27_val_acc-0.9475.keras\n",
      "\n",
      "Epoch 27: val_accuracy improved from 0.94617 to 0.94750, saving model to wandb_models/Base_MLP_Model-lr_0.001-bs_64-20250917-075025/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9345 - loss: 0.2241 - val_accuracy: 0.9475 - val_loss: 0.1838\n",
      "Epoch 28/100\n",
      "\u001b[1m829/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9368 - loss: 0.2202\n",
      "Epoch 28: val_accuracy improved from 0.94750 to 0.94933, saving model to MLP_Models\\Base_MLP_Model-lr_0.001-bs_64-20250917-075025\\best_model_epoch-28_val_acc-0.9493.keras\n",
      "\n",
      "Epoch 28: val_accuracy improved from 0.94750 to 0.94933, saving model to wandb_models/Base_MLP_Model-lr_0.001-bs_64-20250917-075025/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9357 - loss: 0.2208 - val_accuracy: 0.9493 - val_loss: 0.1813\n",
      "Epoch 29/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9376 - loss: 0.2170\n",
      "Epoch 29: val_accuracy improved from 0.94933 to 0.95033, saving model to MLP_Models\\Base_MLP_Model-lr_0.001-bs_64-20250917-075025\\best_model_epoch-29_val_acc-0.9503.keras\n",
      "\n",
      "Epoch 29: val_accuracy improved from 0.94933 to 0.95033, saving model to wandb_models/Base_MLP_Model-lr_0.001-bs_64-20250917-075025/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9365 - loss: 0.2176 - val_accuracy: 0.9503 - val_loss: 0.1790\n",
      "Epoch 30/100\n",
      "\u001b[1m824/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9383 - loss: 0.2140\n",
      "Epoch 30: val_accuracy improved from 0.95033 to 0.95167, saving model to MLP_Models\\Base_MLP_Model-lr_0.001-bs_64-20250917-075025\\best_model_epoch-30_val_acc-0.9517.keras\n",
      "\n",
      "Epoch 30: val_accuracy improved from 0.95033 to 0.95167, saving model to wandb_models/Base_MLP_Model-lr_0.001-bs_64-20250917-075025/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9371 - loss: 0.2145 - val_accuracy: 0.9517 - val_loss: 0.1768\n",
      "Epoch 31/100\n",
      "\u001b[1m839/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9388 - loss: 0.2111\n",
      "Epoch 31: val_accuracy improved from 0.95167 to 0.95233, saving model to MLP_Models\\Base_MLP_Model-lr_0.001-bs_64-20250917-075025\\best_model_epoch-31_val_acc-0.9523.keras\n",
      "\n",
      "Epoch 31: val_accuracy improved from 0.95167 to 0.95233, saving model to wandb_models/Base_MLP_Model-lr_0.001-bs_64-20250917-075025/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9377 - loss: 0.2116 - val_accuracy: 0.9523 - val_loss: 0.1746\n",
      "Epoch 32/100\n",
      "\u001b[1m823/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9399 - loss: 0.2082\n",
      "Epoch 32: val_accuracy improved from 0.95233 to 0.95300, saving model to MLP_Models\\Base_MLP_Model-lr_0.001-bs_64-20250917-075025\\best_model_epoch-32_val_acc-0.9530.keras\n",
      "\n",
      "Epoch 32: val_accuracy improved from 0.95233 to 0.95300, saving model to wandb_models/Base_MLP_Model-lr_0.001-bs_64-20250917-075025/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9387 - loss: 0.2088 - val_accuracy: 0.9530 - val_loss: 0.1726\n",
      "Epoch 33/100\n",
      "\u001b[1m823/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9408 - loss: 0.2055\n",
      "Epoch 33: val_accuracy improved from 0.95300 to 0.95317, saving model to MLP_Models\\Base_MLP_Model-lr_0.001-bs_64-20250917-075025\\best_model_epoch-33_val_acc-0.9532.keras\n",
      "\n",
      "Epoch 33: val_accuracy improved from 0.95300 to 0.95317, saving model to wandb_models/Base_MLP_Model-lr_0.001-bs_64-20250917-075025/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.9396 - loss: 0.2061 - val_accuracy: 0.9532 - val_loss: 0.1706\n",
      "Epoch 34/100\n",
      "\u001b[1m838/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9416 - loss: 0.2029\n",
      "Epoch 34: val_accuracy improved from 0.95317 to 0.95400, saving model to MLP_Models\\Base_MLP_Model-lr_0.001-bs_64-20250917-075025\\best_model_epoch-34_val_acc-0.9540.keras\n",
      "\n",
      "Epoch 34: val_accuracy improved from 0.95317 to 0.95400, saving model to wandb_models/Base_MLP_Model-lr_0.001-bs_64-20250917-075025/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9403 - loss: 0.2034 - val_accuracy: 0.9540 - val_loss: 0.1687\n",
      "Epoch 35/100\n",
      "\u001b[1m815/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9423 - loss: 0.2004\n",
      "Epoch 35: val_accuracy improved from 0.95400 to 0.95433, saving model to MLP_Models\\Base_MLP_Model-lr_0.001-bs_64-20250917-075025\\best_model_epoch-35_val_acc-0.9543.keras\n",
      "\n",
      "Epoch 35: val_accuracy improved from 0.95400 to 0.95433, saving model to wandb_models/Base_MLP_Model-lr_0.001-bs_64-20250917-075025/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9410 - loss: 0.2009 - val_accuracy: 0.9543 - val_loss: 0.1669\n",
      "Epoch 36/100\n",
      "\u001b[1m816/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9428 - loss: 0.1979\n",
      "Epoch 36: val_accuracy improved from 0.95433 to 0.95500, saving model to MLP_Models\\Base_MLP_Model-lr_0.001-bs_64-20250917-075025\\best_model_epoch-36_val_acc-0.9550.keras\n",
      "\n",
      "Epoch 36: val_accuracy improved from 0.95433 to 0.95500, saving model to wandb_models/Base_MLP_Model-lr_0.001-bs_64-20250917-075025/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9417 - loss: 0.1984 - val_accuracy: 0.9550 - val_loss: 0.1652\n",
      "Epoch 37/100\n",
      "\u001b[1m837/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9433 - loss: 0.1955\n",
      "Epoch 37: val_accuracy improved from 0.95500 to 0.95517, saving model to MLP_Models\\Base_MLP_Model-lr_0.001-bs_64-20250917-075025\\best_model_epoch-37_val_acc-0.9552.keras\n",
      "\n",
      "Epoch 37: val_accuracy improved from 0.95500 to 0.95517, saving model to wandb_models/Base_MLP_Model-lr_0.001-bs_64-20250917-075025/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9423 - loss: 0.1960 - val_accuracy: 0.9552 - val_loss: 0.1635\n",
      "Epoch 38/100\n",
      "\u001b[1m811/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9441 - loss: 0.1932\n",
      "Epoch 38: val_accuracy improved from 0.95517 to 0.95567, saving model to MLP_Models\\Base_MLP_Model-lr_0.001-bs_64-20250917-075025\\best_model_epoch-38_val_acc-0.9557.keras\n",
      "\n",
      "Epoch 38: val_accuracy improved from 0.95517 to 0.95567, saving model to wandb_models/Base_MLP_Model-lr_0.001-bs_64-20250917-075025/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9430 - loss: 0.1937 - val_accuracy: 0.9557 - val_loss: 0.1619\n",
      "Epoch 39/100\n",
      "\u001b[1m814/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 993us/step - accuracy: 0.9446 - loss: 0.1909\n",
      "Epoch 39: val_accuracy did not improve from 0.95567\n",
      "\n",
      "Epoch 39: val_accuracy did not improve from 0.95567\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9438 - loss: 0.1914 - val_accuracy: 0.9555 - val_loss: 0.1603\n",
      "Epoch 40/100\n",
      "\u001b[1m821/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9453 - loss: 0.1888\n",
      "Epoch 40: val_accuracy did not improve from 0.95567\n",
      "\n",
      "Epoch 40: val_accuracy did not improve from 0.95567\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9445 - loss: 0.1892 - val_accuracy: 0.9552 - val_loss: 0.1588\n",
      "Epoch 41/100\n",
      "\u001b[1m829/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9461 - loss: 0.1867\n",
      "Epoch 41: val_accuracy did not improve from 0.95567\n",
      "\n",
      "Epoch 41: val_accuracy did not improve from 0.95567\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9454 - loss: 0.1870 - val_accuracy: 0.9555 - val_loss: 0.1573\n",
      "Epoch 42/100\n",
      "\u001b[1m825/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9467 - loss: 0.1846\n",
      "Epoch 42: val_accuracy improved from 0.95567 to 0.95617, saving model to MLP_Models\\Base_MLP_Model-lr_0.001-bs_64-20250917-075025\\best_model_epoch-42_val_acc-0.9562.keras\n",
      "\n",
      "Epoch 42: val_accuracy improved from 0.95567 to 0.95617, saving model to wandb_models/Base_MLP_Model-lr_0.001-bs_64-20250917-075025/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9460 - loss: 0.1850 - val_accuracy: 0.9562 - val_loss: 0.1559\n",
      "Epoch 43/100\n",
      "\u001b[1m826/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9473 - loss: 0.1826\n",
      "Epoch 43: val_accuracy improved from 0.95617 to 0.95633, saving model to MLP_Models\\Base_MLP_Model-lr_0.001-bs_64-20250917-075025\\best_model_epoch-43_val_acc-0.9563.keras\n",
      "\n",
      "Epoch 43: val_accuracy improved from 0.95617 to 0.95633, saving model to wandb_models/Base_MLP_Model-lr_0.001-bs_64-20250917-075025/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9467 - loss: 0.1829 - val_accuracy: 0.9563 - val_loss: 0.1545\n",
      "Epoch 44/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9478 - loss: 0.1807\n",
      "Epoch 44: val_accuracy improved from 0.95633 to 0.95683, saving model to MLP_Models\\Base_MLP_Model-lr_0.001-bs_64-20250917-075025\\best_model_epoch-44_val_acc-0.9568.keras\n",
      "\n",
      "Epoch 44: val_accuracy improved from 0.95633 to 0.95683, saving model to wandb_models/Base_MLP_Model-lr_0.001-bs_64-20250917-075025/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9473 - loss: 0.1810 - val_accuracy: 0.9568 - val_loss: 0.1532\n",
      "Epoch 45/100\n",
      "\u001b[1m820/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9483 - loss: 0.1788\n",
      "Epoch 45: val_accuracy improved from 0.95683 to 0.95700, saving model to MLP_Models\\Base_MLP_Model-lr_0.001-bs_64-20250917-075025\\best_model_epoch-45_val_acc-0.9570.keras\n",
      "\n",
      "Epoch 45: val_accuracy improved from 0.95683 to 0.95700, saving model to wandb_models/Base_MLP_Model-lr_0.001-bs_64-20250917-075025/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9479 - loss: 0.1791 - val_accuracy: 0.9570 - val_loss: 0.1519\n",
      "Epoch 46/100\n",
      "\u001b[1m832/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9490 - loss: 0.1769\n",
      "Epoch 46: val_accuracy improved from 0.95700 to 0.95733, saving model to MLP_Models\\Base_MLP_Model-lr_0.001-bs_64-20250917-075025\\best_model_epoch-46_val_acc-0.9573.keras\n",
      "\n",
      "Epoch 46: val_accuracy improved from 0.95700 to 0.95733, saving model to wandb_models/Base_MLP_Model-lr_0.001-bs_64-20250917-075025/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9486 - loss: 0.1772 - val_accuracy: 0.9573 - val_loss: 0.1507\n",
      "Epoch 47/100\n",
      "\u001b[1m827/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9494 - loss: 0.1751\n",
      "Epoch 47: val_accuracy improved from 0.95733 to 0.95767, saving model to MLP_Models\\Base_MLP_Model-lr_0.001-bs_64-20250917-075025\\best_model_epoch-47_val_acc-0.9577.keras\n",
      "\n",
      "Epoch 47: val_accuracy improved from 0.95733 to 0.95767, saving model to wandb_models/Base_MLP_Model-lr_0.001-bs_64-20250917-075025/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9491 - loss: 0.1754 - val_accuracy: 0.9577 - val_loss: 0.1495\n",
      "Epoch 48/100\n",
      "\u001b[1m813/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9501 - loss: 0.1733\n",
      "Epoch 48: val_accuracy improved from 0.95767 to 0.95800, saving model to MLP_Models\\Base_MLP_Model-lr_0.001-bs_64-20250917-075025\\best_model_epoch-48_val_acc-0.9580.keras\n",
      "\n",
      "Epoch 48: val_accuracy improved from 0.95767 to 0.95800, saving model to wandb_models/Base_MLP_Model-lr_0.001-bs_64-20250917-075025/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9497 - loss: 0.1736 - val_accuracy: 0.9580 - val_loss: 0.1483\n",
      "Epoch 49/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9507 - loss: 0.1716\n",
      "Epoch 49: val_accuracy improved from 0.95800 to 0.95917, saving model to MLP_Models\\Base_MLP_Model-lr_0.001-bs_64-20250917-075025\\best_model_epoch-49_val_acc-0.9592.keras\n",
      "\n",
      "Epoch 49: val_accuracy improved from 0.95800 to 0.95917, saving model to wandb_models/Base_MLP_Model-lr_0.001-bs_64-20250917-075025/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9504 - loss: 0.1718 - val_accuracy: 0.9592 - val_loss: 0.1471\n",
      "Epoch 50/100\n",
      "\u001b[1m838/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 964us/step - accuracy: 0.9510 - loss: 0.1700\n",
      "Epoch 50: val_accuracy did not improve from 0.95917\n",
      "\n",
      "Epoch 50: val_accuracy did not improve from 0.95917\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9506 - loss: 0.1701 - val_accuracy: 0.9592 - val_loss: 0.1461\n",
      "Epoch 51/100\n",
      "\u001b[1m824/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9513 - loss: 0.1683\n",
      "Epoch 51: val_accuracy improved from 0.95917 to 0.95983, saving model to MLP_Models\\Base_MLP_Model-lr_0.001-bs_64-20250917-075025\\best_model_epoch-51_val_acc-0.9598.keras\n",
      "\n",
      "Epoch 51: val_accuracy improved from 0.95917 to 0.95983, saving model to wandb_models/Base_MLP_Model-lr_0.001-bs_64-20250917-075025/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9511 - loss: 0.1685 - val_accuracy: 0.9598 - val_loss: 0.1450\n",
      "Epoch 52/100\n",
      "\u001b[1m833/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 973us/step - accuracy: 0.9519 - loss: 0.1667\n",
      "Epoch 52: val_accuracy improved from 0.95983 to 0.96050, saving model to MLP_Models\\Base_MLP_Model-lr_0.001-bs_64-20250917-075025\\best_model_epoch-52_val_acc-0.9605.keras\n",
      "\n",
      "Epoch 52: val_accuracy improved from 0.95983 to 0.96050, saving model to wandb_models/Base_MLP_Model-lr_0.001-bs_64-20250917-075025/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9516 - loss: 0.1669 - val_accuracy: 0.9605 - val_loss: 0.1439\n",
      "Epoch 53/100\n",
      "\u001b[1m835/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9522 - loss: 0.1651\n",
      "Epoch 53: val_accuracy improved from 0.96050 to 0.96100, saving model to MLP_Models\\Base_MLP_Model-lr_0.001-bs_64-20250917-075025\\best_model_epoch-53_val_acc-0.9610.keras\n",
      "\n",
      "Epoch 53: val_accuracy improved from 0.96050 to 0.96100, saving model to wandb_models/Base_MLP_Model-lr_0.001-bs_64-20250917-075025/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9521 - loss: 0.1653 - val_accuracy: 0.9610 - val_loss: 0.1429\n",
      "Epoch 54/100\n",
      "\u001b[1m812/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9525 - loss: 0.1636\n",
      "Epoch 54: val_accuracy improved from 0.96100 to 0.96150, saving model to MLP_Models\\Base_MLP_Model-lr_0.001-bs_64-20250917-075025\\best_model_epoch-54_val_acc-0.9615.keras\n",
      "\n",
      "Epoch 54: val_accuracy improved from 0.96100 to 0.96150, saving model to wandb_models/Base_MLP_Model-lr_0.001-bs_64-20250917-075025/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9524 - loss: 0.1637 - val_accuracy: 0.9615 - val_loss: 0.1418\n",
      "Epoch 55/100\n",
      "\u001b[1m833/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9527 - loss: 0.1620\n",
      "Epoch 55: val_accuracy did not improve from 0.96150\n",
      "\n",
      "Epoch 55: val_accuracy did not improve from 0.96150\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9526 - loss: 0.1622 - val_accuracy: 0.9615 - val_loss: 0.1408\n",
      "Epoch 56/100\n",
      "\u001b[1m802/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9535 - loss: 0.1606\n",
      "Epoch 56: val_accuracy did not improve from 0.96150\n",
      "\n",
      "Epoch 56: val_accuracy did not improve from 0.96150\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9531 - loss: 0.1607 - val_accuracy: 0.9615 - val_loss: 0.1399\n",
      "Epoch 57/100\n",
      "\u001b[1m838/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9538 - loss: 0.1591\n",
      "Epoch 57: val_accuracy did not improve from 0.96150\n",
      "\n",
      "Epoch 57: val_accuracy did not improve from 0.96150\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9535 - loss: 0.1593 - val_accuracy: 0.9615 - val_loss: 0.1390\n",
      "Epoch 58/100\n",
      "\u001b[1m807/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9542 - loss: 0.1577\n",
      "Epoch 58: val_accuracy improved from 0.96150 to 0.96183, saving model to MLP_Models\\Base_MLP_Model-lr_0.001-bs_64-20250917-075025\\best_model_epoch-58_val_acc-0.9618.keras\n",
      "\n",
      "Epoch 58: val_accuracy improved from 0.96150 to 0.96183, saving model to wandb_models/Base_MLP_Model-lr_0.001-bs_64-20250917-075025/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9539 - loss: 0.1578 - val_accuracy: 0.9618 - val_loss: 0.1380\n",
      "Epoch 59/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9545 - loss: 0.1563\n",
      "Epoch 59: val_accuracy did not improve from 0.96183\n",
      "\n",
      "Epoch 59: val_accuracy did not improve from 0.96183\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9544 - loss: 0.1564 - val_accuracy: 0.9617 - val_loss: 0.1371\n",
      "Epoch 60/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 957us/step - accuracy: 0.9550 - loss: 0.1549\n",
      "Epoch 60: val_accuracy improved from 0.96183 to 0.96217, saving model to MLP_Models\\Base_MLP_Model-lr_0.001-bs_64-20250917-075025\\best_model_epoch-60_val_acc-0.9622.keras\n",
      "\n",
      "Epoch 60: val_accuracy improved from 0.96183 to 0.96217, saving model to wandb_models/Base_MLP_Model-lr_0.001-bs_64-20250917-075025/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9546 - loss: 0.1551 - val_accuracy: 0.9622 - val_loss: 0.1363\n",
      "Epoch 61/100\n",
      "\u001b[1m825/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9554 - loss: 0.1536\n",
      "Epoch 61: val_accuracy improved from 0.96217 to 0.96233, saving model to MLP_Models\\Base_MLP_Model-lr_0.001-bs_64-20250917-075025\\best_model_epoch-61_val_acc-0.9623.keras\n",
      "\n",
      "Epoch 61: val_accuracy improved from 0.96217 to 0.96233, saving model to wandb_models/Base_MLP_Model-lr_0.001-bs_64-20250917-075025/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9550 - loss: 0.1537 - val_accuracy: 0.9623 - val_loss: 0.1354\n",
      "Epoch 62/100\n",
      "\u001b[1m822/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 988us/step - accuracy: 0.9560 - loss: 0.1523\n",
      "Epoch 62: val_accuracy improved from 0.96233 to 0.96283, saving model to MLP_Models\\Base_MLP_Model-lr_0.001-bs_64-20250917-075025\\best_model_epoch-62_val_acc-0.9628.keras\n",
      "\n",
      "Epoch 62: val_accuracy improved from 0.96233 to 0.96283, saving model to wandb_models/Base_MLP_Model-lr_0.001-bs_64-20250917-075025/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9556 - loss: 0.1524 - val_accuracy: 0.9628 - val_loss: 0.1346\n",
      "Epoch 63/100\n",
      "\u001b[1m837/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 967us/step - accuracy: 0.9565 - loss: 0.1510\n",
      "Epoch 63: val_accuracy did not improve from 0.96283\n",
      "\n",
      "Epoch 63: val_accuracy did not improve from 0.96283\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9560 - loss: 0.1511 - val_accuracy: 0.9628 - val_loss: 0.1338\n",
      "Epoch 64/100\n",
      "\u001b[1m814/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 995us/step - accuracy: 0.9570 - loss: 0.1497\n",
      "Epoch 64: val_accuracy improved from 0.96283 to 0.96317, saving model to MLP_Models\\Base_MLP_Model-lr_0.001-bs_64-20250917-075025\\best_model_epoch-64_val_acc-0.9632.keras\n",
      "\n",
      "Epoch 64: val_accuracy improved from 0.96283 to 0.96317, saving model to wandb_models/Base_MLP_Model-lr_0.001-bs_64-20250917-075025/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9565 - loss: 0.1499 - val_accuracy: 0.9632 - val_loss: 0.1330\n",
      "Epoch 65/100\n",
      "\u001b[1m825/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 980us/step - accuracy: 0.9574 - loss: 0.1485\n",
      "Epoch 65: val_accuracy improved from 0.96317 to 0.96333, saving model to MLP_Models\\Base_MLP_Model-lr_0.001-bs_64-20250917-075025\\best_model_epoch-65_val_acc-0.9633.keras\n",
      "\n",
      "Epoch 65: val_accuracy improved from 0.96317 to 0.96333, saving model to wandb_models/Base_MLP_Model-lr_0.001-bs_64-20250917-075025/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9569 - loss: 0.1486 - val_accuracy: 0.9633 - val_loss: 0.1323\n",
      "Epoch 66/100\n",
      "\u001b[1m799/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 947us/step - accuracy: 0.9578 - loss: 0.1472\n",
      "Epoch 66: val_accuracy improved from 0.96333 to 0.96383, saving model to MLP_Models\\Base_MLP_Model-lr_0.001-bs_64-20250917-075025\\best_model_epoch-66_val_acc-0.9638.keras\n",
      "\n",
      "Epoch 66: val_accuracy improved from 0.96333 to 0.96383, saving model to wandb_models/Base_MLP_Model-lr_0.001-bs_64-20250917-075025/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9572 - loss: 0.1474 - val_accuracy: 0.9638 - val_loss: 0.1315\n",
      "Epoch 67/100\n",
      "\u001b[1m811/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9580 - loss: 0.1460\n",
      "Epoch 67: val_accuracy improved from 0.96383 to 0.96450, saving model to MLP_Models\\Base_MLP_Model-lr_0.001-bs_64-20250917-075025\\best_model_epoch-67_val_acc-0.9645.keras\n",
      "\n",
      "Epoch 67: val_accuracy improved from 0.96383 to 0.96450, saving model to wandb_models/Base_MLP_Model-lr_0.001-bs_64-20250917-075025/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9575 - loss: 0.1462 - val_accuracy: 0.9645 - val_loss: 0.1308\n",
      "Epoch 68/100\n",
      "\u001b[1m826/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 979us/step - accuracy: 0.9583 - loss: 0.1449\n",
      "Epoch 68: val_accuracy improved from 0.96450 to 0.96483, saving model to MLP_Models\\Base_MLP_Model-lr_0.001-bs_64-20250917-075025\\best_model_epoch-68_val_acc-0.9648.keras\n",
      "\n",
      "Epoch 68: val_accuracy improved from 0.96450 to 0.96483, saving model to wandb_models/Base_MLP_Model-lr_0.001-bs_64-20250917-075025/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9579 - loss: 0.1450 - val_accuracy: 0.9648 - val_loss: 0.1301\n",
      "Epoch 69/100\n",
      "\u001b[1m806/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 939us/step - accuracy: 0.9585 - loss: 0.1437\n",
      "Epoch 69: val_accuracy improved from 0.96483 to 0.96550, saving model to MLP_Models\\Base_MLP_Model-lr_0.001-bs_64-20250917-075025\\best_model_epoch-69_val_acc-0.9655.keras\n",
      "\n",
      "Epoch 69: val_accuracy improved from 0.96483 to 0.96550, saving model to wandb_models/Base_MLP_Model-lr_0.001-bs_64-20250917-075025/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9581 - loss: 0.1438 - val_accuracy: 0.9655 - val_loss: 0.1295\n",
      "Epoch 70/100\n",
      "\u001b[1m828/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 979us/step - accuracy: 0.9588 - loss: 0.1425\n",
      "Epoch 70: val_accuracy did not improve from 0.96550\n",
      "\n",
      "Epoch 70: val_accuracy did not improve from 0.96550\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9585 - loss: 0.1427 - val_accuracy: 0.9655 - val_loss: 0.1288\n",
      "Epoch 71/100\n",
      "\u001b[1m824/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9590 - loss: 0.1414\n",
      "Epoch 71: val_accuracy did not improve from 0.96550\n",
      "\n",
      "Epoch 71: val_accuracy did not improve from 0.96550\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9588 - loss: 0.1415 - val_accuracy: 0.9655 - val_loss: 0.1281\n",
      "Epoch 72/100\n",
      "\u001b[1m815/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 993us/step - accuracy: 0.9594 - loss: 0.1402\n",
      "Epoch 72: val_accuracy did not improve from 0.96550\n",
      "\n",
      "Epoch 72: val_accuracy did not improve from 0.96550\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9592 - loss: 0.1404 - val_accuracy: 0.9653 - val_loss: 0.1275\n",
      "Epoch 73/100\n",
      "\u001b[1m830/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9598 - loss: 0.1391\n",
      "Epoch 73: val_accuracy did not improve from 0.96550\n",
      "\n",
      "Epoch 73: val_accuracy did not improve from 0.96550\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9595 - loss: 0.1393 - val_accuracy: 0.9653 - val_loss: 0.1269\n",
      "Epoch 74/100\n",
      "\u001b[1m798/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9598 - loss: 0.1380\n",
      "Epoch 74: val_accuracy did not improve from 0.96550\n",
      "\n",
      "Epoch 74: val_accuracy did not improve from 0.96550\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9595 - loss: 0.1382 - val_accuracy: 0.9653 - val_loss: 0.1263\n",
      "Epoch 75/100\n",
      "\u001b[1m796/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9599 - loss: 0.1370\n",
      "Epoch 75: val_accuracy did not improve from 0.96550\n",
      "\n",
      "Epoch 75: val_accuracy did not improve from 0.96550\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9597 - loss: 0.1371 - val_accuracy: 0.9653 - val_loss: 0.1257\n",
      "Epoch 76/100\n",
      "\u001b[1m826/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9603 - loss: 0.1359\n",
      "Epoch 76: val_accuracy did not improve from 0.96550\n",
      "\n",
      "Epoch 76: val_accuracy did not improve from 0.96550\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9602 - loss: 0.1361 - val_accuracy: 0.9653 - val_loss: 0.1251\n",
      "Epoch 77/100\n",
      "\u001b[1m829/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9603 - loss: 0.1349\n",
      "Epoch 77: val_accuracy did not improve from 0.96550\n",
      "\n",
      "Epoch 77: val_accuracy did not improve from 0.96550\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9604 - loss: 0.1350 - val_accuracy: 0.9655 - val_loss: 0.1245\n",
      "Epoch 78/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9608 - loss: 0.1338\n",
      "Epoch 78: val_accuracy improved from 0.96550 to 0.96583, saving model to MLP_Models\\Base_MLP_Model-lr_0.001-bs_64-20250917-075025\\best_model_epoch-78_val_acc-0.9658.keras\n",
      "\n",
      "Epoch 78: val_accuracy improved from 0.96550 to 0.96583, saving model to wandb_models/Base_MLP_Model-lr_0.001-bs_64-20250917-075025/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9608 - loss: 0.1340 - val_accuracy: 0.9658 - val_loss: 0.1239\n",
      "Epoch 79/100\n",
      "\u001b[1m817/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9611 - loss: 0.1328\n",
      "Epoch 79: val_accuracy did not improve from 0.96583\n",
      "\n",
      "Epoch 79: val_accuracy did not improve from 0.96583\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9610 - loss: 0.1330 - val_accuracy: 0.9658 - val_loss: 0.1234\n",
      "Epoch 80/100\n",
      "\u001b[1m806/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9615 - loss: 0.1319\n",
      "Epoch 80: val_accuracy did not improve from 0.96583\n",
      "\n",
      "Epoch 80: val_accuracy did not improve from 0.96583\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9614 - loss: 0.1320 - val_accuracy: 0.9658 - val_loss: 0.1228\n",
      "Epoch 81/100\n",
      "\u001b[1m792/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9616 - loss: 0.1309\n",
      "Epoch 81: val_accuracy improved from 0.96583 to 0.96617, saving model to MLP_Models\\Base_MLP_Model-lr_0.001-bs_64-20250917-075025\\best_model_epoch-81_val_acc-0.9662.keras\n",
      "\n",
      "Epoch 81: val_accuracy improved from 0.96583 to 0.96617, saving model to wandb_models/Base_MLP_Model-lr_0.001-bs_64-20250917-075025/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9615 - loss: 0.1310 - val_accuracy: 0.9662 - val_loss: 0.1223\n",
      "Epoch 82/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 959us/step - accuracy: 0.9618 - loss: 0.1299\n",
      "Epoch 82: val_accuracy did not improve from 0.96617\n",
      "\n",
      "Epoch 82: val_accuracy did not improve from 0.96617\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9616 - loss: 0.1301 - val_accuracy: 0.9662 - val_loss: 0.1218\n",
      "Epoch 83/100\n",
      "\u001b[1m829/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9622 - loss: 0.1290\n",
      "Epoch 83: val_accuracy improved from 0.96617 to 0.96633, saving model to MLP_Models\\Base_MLP_Model-lr_0.001-bs_64-20250917-075025\\best_model_epoch-83_val_acc-0.9663.keras\n",
      "\n",
      "Epoch 83: val_accuracy improved from 0.96617 to 0.96633, saving model to wandb_models/Base_MLP_Model-lr_0.001-bs_64-20250917-075025/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9620 - loss: 0.1291 - val_accuracy: 0.9663 - val_loss: 0.1213\n",
      "Epoch 84/100\n",
      "\u001b[1m840/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 964us/step - accuracy: 0.9627 - loss: 0.1281\n",
      "Epoch 84: val_accuracy improved from 0.96633 to 0.96650, saving model to MLP_Models\\Base_MLP_Model-lr_0.001-bs_64-20250917-075025\\best_model_epoch-84_val_acc-0.9665.keras\n",
      "\n",
      "Epoch 84: val_accuracy improved from 0.96633 to 0.96650, saving model to wandb_models/Base_MLP_Model-lr_0.001-bs_64-20250917-075025/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9624 - loss: 0.1282 - val_accuracy: 0.9665 - val_loss: 0.1208\n",
      "Epoch 85/100\n",
      "\u001b[1m818/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9630 - loss: 0.1271\n",
      "Epoch 85: val_accuracy did not improve from 0.96650\n",
      "\n",
      "Epoch 85: val_accuracy did not improve from 0.96650\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9626 - loss: 0.1273 - val_accuracy: 0.9665 - val_loss: 0.1203\n",
      "Epoch 86/100\n",
      "\u001b[1m839/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9635 - loss: 0.1262\n",
      "Epoch 86: val_accuracy improved from 0.96650 to 0.96667, saving model to MLP_Models\\Base_MLP_Model-lr_0.001-bs_64-20250917-075025\\best_model_epoch-86_val_acc-0.9667.keras\n",
      "\n",
      "Epoch 86: val_accuracy improved from 0.96650 to 0.96667, saving model to wandb_models/Base_MLP_Model-lr_0.001-bs_64-20250917-075025/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9629 - loss: 0.1264 - val_accuracy: 0.9667 - val_loss: 0.1198\n",
      "Epoch 87/100\n",
      "\u001b[1m828/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 975us/step - accuracy: 0.9635 - loss: 0.1253\n",
      "Epoch 87: val_accuracy improved from 0.96667 to 0.96683, saving model to MLP_Models\\Base_MLP_Model-lr_0.001-bs_64-20250917-075025\\best_model_epoch-87_val_acc-0.9668.keras\n",
      "\n",
      "Epoch 87: val_accuracy improved from 0.96667 to 0.96683, saving model to wandb_models/Base_MLP_Model-lr_0.001-bs_64-20250917-075025/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9631 - loss: 0.1255 - val_accuracy: 0.9668 - val_loss: 0.1193\n",
      "Epoch 88/100\n",
      "\u001b[1m814/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9641 - loss: 0.1245\n",
      "Epoch 88: val_accuracy did not improve from 0.96683\n",
      "\n",
      "Epoch 88: val_accuracy did not improve from 0.96683\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9634 - loss: 0.1246 - val_accuracy: 0.9667 - val_loss: 0.1189\n",
      "Epoch 89/100\n",
      "\u001b[1m814/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9643 - loss: 0.1236\n",
      "Epoch 89: val_accuracy did not improve from 0.96683\n",
      "\n",
      "Epoch 89: val_accuracy did not improve from 0.96683\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9636 - loss: 0.1237 - val_accuracy: 0.9668 - val_loss: 0.1185\n",
      "Epoch 90/100\n",
      "\u001b[1m811/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9646 - loss: 0.1227\n",
      "Epoch 90: val_accuracy did not improve from 0.96683\n",
      "\n",
      "Epoch 90: val_accuracy did not improve from 0.96683\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9638 - loss: 0.1229 - val_accuracy: 0.9668 - val_loss: 0.1180\n",
      "Epoch 91/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9649 - loss: 0.1218\n",
      "Epoch 91: val_accuracy did not improve from 0.96683\n",
      "\n",
      "Epoch 91: val_accuracy did not improve from 0.96683\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9642 - loss: 0.1220 - val_accuracy: 0.9667 - val_loss: 0.1176\n",
      "Epoch 92/100\n",
      "\u001b[1m821/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9651 - loss: 0.1210\n",
      "Epoch 92: val_accuracy did not improve from 0.96683\n",
      "\n",
      "Epoch 92: val_accuracy did not improve from 0.96683\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9643 - loss: 0.1212 - val_accuracy: 0.9668 - val_loss: 0.1172\n",
      "Epoch 93/100\n",
      "\u001b[1m840/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9654 - loss: 0.1201\n",
      "Epoch 93: val_accuracy improved from 0.96683 to 0.96717, saving model to MLP_Models\\Base_MLP_Model-lr_0.001-bs_64-20250917-075025\\best_model_epoch-93_val_acc-0.9672.keras\n",
      "\n",
      "Epoch 93: val_accuracy improved from 0.96683 to 0.96717, saving model to wandb_models/Base_MLP_Model-lr_0.001-bs_64-20250917-075025/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9646 - loss: 0.1204 - val_accuracy: 0.9672 - val_loss: 0.1168\n",
      "Epoch 94/100\n",
      "\u001b[1m822/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9657 - loss: 0.1193\n",
      "Epoch 94: val_accuracy improved from 0.96717 to 0.96733, saving model to MLP_Models\\Base_MLP_Model-lr_0.001-bs_64-20250917-075025\\best_model_epoch-94_val_acc-0.9673.keras\n",
      "\n",
      "Epoch 94: val_accuracy improved from 0.96717 to 0.96733, saving model to wandb_models/Base_MLP_Model-lr_0.001-bs_64-20250917-075025/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9650 - loss: 0.1196 - val_accuracy: 0.9673 - val_loss: 0.1164\n",
      "Epoch 95/100\n",
      "\u001b[1m836/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9658 - loss: 0.1185\n",
      "Epoch 95: val_accuracy did not improve from 0.96733\n",
      "\n",
      "Epoch 95: val_accuracy did not improve from 0.96733\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9651 - loss: 0.1187 - val_accuracy: 0.9673 - val_loss: 0.1160\n",
      "Epoch 96/100\n",
      "\u001b[1m811/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9661 - loss: 0.1177\n",
      "Epoch 96: val_accuracy improved from 0.96733 to 0.96750, saving model to MLP_Models\\Base_MLP_Model-lr_0.001-bs_64-20250917-075025\\best_model_epoch-96_val_acc-0.9675.keras\n",
      "\n",
      "Epoch 96: val_accuracy improved from 0.96733 to 0.96750, saving model to wandb_models/Base_MLP_Model-lr_0.001-bs_64-20250917-075025/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9653 - loss: 0.1179 - val_accuracy: 0.9675 - val_loss: 0.1156\n",
      "Epoch 97/100\n",
      "\u001b[1m813/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9663 - loss: 0.1169\n",
      "Epoch 97: val_accuracy did not improve from 0.96750\n",
      "\n",
      "Epoch 97: val_accuracy did not improve from 0.96750\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9655 - loss: 0.1172 - val_accuracy: 0.9675 - val_loss: 0.1153\n",
      "Epoch 98/100\n",
      "\u001b[1m820/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9665 - loss: 0.1161\n",
      "Epoch 98: val_accuracy did not improve from 0.96750\n",
      "\n",
      "Epoch 98: val_accuracy did not improve from 0.96750\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9658 - loss: 0.1164 - val_accuracy: 0.9673 - val_loss: 0.1149\n",
      "Epoch 99/100\n",
      "\u001b[1m805/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9667 - loss: 0.1153  \n",
      "Epoch 99: val_accuracy improved from 0.96750 to 0.96800, saving model to MLP_Models\\Base_MLP_Model-lr_0.001-bs_64-20250917-075025\\best_model_epoch-99_val_acc-0.9680.keras\n",
      "\n",
      "Epoch 99: val_accuracy improved from 0.96750 to 0.96800, saving model to wandb_models/Base_MLP_Model-lr_0.001-bs_64-20250917-075025/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9661 - loss: 0.1156 - val_accuracy: 0.9680 - val_loss: 0.1145\n",
      "Epoch 100/100\n",
      "\u001b[1m840/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9668 - loss: 0.1145\n",
      "Epoch 100: val_accuracy did not improve from 0.96800\n",
      "\n",
      "Epoch 100: val_accuracy did not improve from 0.96800\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9663 - loss: 0.1148 - val_accuracy: 0.9680 - val_loss: 0.1142\n",
      "\n",
      "Training history saved to: MLP_Models\\Base_MLP_Model-lr_0.001-bs_64-20250917-075025\\training_history.pkl\n",
      "\n",
      "--- Peak Performance Summary ---\n",
      "Best validation accuracy:           0.9680\n",
      "Associated training accuracy:       0.9661\n",
      "Occurred at epoch:                  99\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch/accuracy</td><td>▁▄▅▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇█████████████████████</td></tr><tr><td>epoch/epoch</td><td>▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇███</td></tr><tr><td>epoch/learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/loss</td><td>█▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/val_accuracy</td><td>▁▂▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇▇▇██████████████████</td></tr><tr><td>epoch/val_loss</td><td>█▅▄▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch/accuracy</td><td>0.96633</td></tr><tr><td>epoch/epoch</td><td>99</td></tr><tr><td>epoch/learning_rate</td><td>0.001</td></tr><tr><td>epoch/loss</td><td>0.11485</td></tr><tr><td>epoch/val_accuracy</td><td>0.968</td></tr><tr><td>epoch/val_loss</td><td>0.11417</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Base_MLP_Model-lr_0.001-bs_64-20250917-075025</strong> at: <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/si261zoy' target=\"_blank\">https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/si261zoy</a><br> View project at: <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2' target=\"_blank\">https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2</a><br>Synced 5 W&B file(s), 0 media file(s), 140 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250917_075025-si261zoy\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Adapting the normalisation layer...\n",
      "Adaptation complete.\n",
      "\n",
      "\n",
      "--- Starting Experiment: Base_MLP_Model ---\n",
      "\n",
      "--- Model Architecture ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"Base_MLP_Model\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"Base_MLP_Model\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ normalization_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Normalization</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">25,120</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,112</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">330</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ normalization_3 (\u001b[38;5;33mNormalization\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m1\u001b[0m)      │             \u001b[38;5;34m3\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_3 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m784\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_12 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │        \u001b[38;5;34m25,120\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_13 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m2,112\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_14 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m2,080\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_15 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │           \u001b[38;5;34m330\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">29,645</span> (115.80 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m29,645\u001b[0m (115.80 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">29,642</span> (115.79 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m29,642\u001b[0m (115.79 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3</span> (16.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m3\u001b[0m (16.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Hyperparameters ---\n",
      "optimiser           : SGD\n",
      "learning_rate       : 0.01\n",
      "epochs              : 100\n",
      "batch_size          : 64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "creating run (0.0s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\TimVos\\VSC Projects\\CSE5ML\\Assessment 2\\wandb\\run-20250917_075530-3o7bm0gq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/3o7bm0gq' target=\"_blank\">Base_MLP_Model-lr_0.01-bs_64-20250917-075530</a></strong> to <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2' target=\"_blank\">https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/3o7bm0gq' target=\"_blank\">https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/3o7bm0gq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m837/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5493 - loss: 1.3422\n",
      "Epoch 1: val_accuracy improved from None to 0.91167, saving model to MLP_Models\\Base_MLP_Model-lr_0.01-bs_64-20250917-075530\\best_model_epoch-01_val_acc-0.9117.keras\n",
      "\n",
      "Epoch 1: val_accuracy improved from None to 0.91167, saving model to wandb_models/Base_MLP_Model-lr_0.01-bs_64-20250917-075530/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7454 - loss: 0.8071 - val_accuracy: 0.9117 - val_loss: 0.2998\n",
      "Epoch 2/100\n",
      "\u001b[1m837/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8998 - loss: 0.3348\n",
      "Epoch 2: val_accuracy improved from 0.91167 to 0.93417, saving model to MLP_Models\\Base_MLP_Model-lr_0.01-bs_64-20250917-075530\\best_model_epoch-02_val_acc-0.9342.keras\n",
      "\n",
      "Epoch 2: val_accuracy improved from 0.91167 to 0.93417, saving model to wandb_models/Base_MLP_Model-lr_0.01-bs_64-20250917-075530/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9063 - loss: 0.3129 - val_accuracy: 0.9342 - val_loss: 0.2207\n",
      "Epoch 3/100\n",
      "\u001b[1m816/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 992us/step - accuracy: 0.9239 - loss: 0.2579\n",
      "Epoch 3: val_accuracy improved from 0.93417 to 0.94450, saving model to MLP_Models\\Base_MLP_Model-lr_0.01-bs_64-20250917-075530\\best_model_epoch-03_val_acc-0.9445.keras\n",
      "\n",
      "Epoch 3: val_accuracy improved from 0.93417 to 0.94450, saving model to wandb_models/Base_MLP_Model-lr_0.01-bs_64-20250917-075530/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9261 - loss: 0.2479 - val_accuracy: 0.9445 - val_loss: 0.1828\n",
      "Epoch 4/100\n",
      "\u001b[1m810/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 999us/step - accuracy: 0.9363 - loss: 0.2183\n",
      "Epoch 4: val_accuracy improved from 0.94450 to 0.95200, saving model to MLP_Models\\Base_MLP_Model-lr_0.01-bs_64-20250917-075530\\best_model_epoch-04_val_acc-0.9520.keras\n",
      "\n",
      "Epoch 4: val_accuracy improved from 0.94450 to 0.95200, saving model to wandb_models/Base_MLP_Model-lr_0.01-bs_64-20250917-075530/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9375 - loss: 0.2114 - val_accuracy: 0.9520 - val_loss: 0.1604\n",
      "Epoch 5/100\n",
      "\u001b[1m820/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 986us/step - accuracy: 0.9447 - loss: 0.1926\n",
      "Epoch 5: val_accuracy improved from 0.95200 to 0.95583, saving model to MLP_Models\\Base_MLP_Model-lr_0.01-bs_64-20250917-075530\\best_model_epoch-05_val_acc-0.9558.keras\n",
      "\n",
      "Epoch 5: val_accuracy improved from 0.95200 to 0.95583, saving model to wandb_models/Base_MLP_Model-lr_0.01-bs_64-20250917-075530/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9453 - loss: 0.1868 - val_accuracy: 0.9558 - val_loss: 0.1452\n",
      "Epoch 6/100\n",
      "\u001b[1m824/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9498 - loss: 0.1737\n",
      "Epoch 6: val_accuracy improved from 0.95583 to 0.95833, saving model to MLP_Models\\Base_MLP_Model-lr_0.01-bs_64-20250917-075530\\best_model_epoch-06_val_acc-0.9583.keras\n",
      "\n",
      "Epoch 6: val_accuracy improved from 0.95583 to 0.95833, saving model to wandb_models/Base_MLP_Model-lr_0.01-bs_64-20250917-075530/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9505 - loss: 0.1687 - val_accuracy: 0.9583 - val_loss: 0.1346\n",
      "Epoch 7/100\n",
      "\u001b[1m807/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9536 - loss: 0.1592\n",
      "Epoch 7: val_accuracy improved from 0.95833 to 0.96150, saving model to MLP_Models\\Base_MLP_Model-lr_0.01-bs_64-20250917-075530\\best_model_epoch-07_val_acc-0.9615.keras\n",
      "\n",
      "Epoch 7: val_accuracy improved from 0.95833 to 0.96150, saving model to wandb_models/Base_MLP_Model-lr_0.01-bs_64-20250917-075530/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9545 - loss: 0.1545 - val_accuracy: 0.9615 - val_loss: 0.1268\n",
      "Epoch 8/100\n",
      "\u001b[1m826/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9573 - loss: 0.1470\n",
      "Epoch 8: val_accuracy improved from 0.96150 to 0.96300, saving model to MLP_Models\\Base_MLP_Model-lr_0.01-bs_64-20250917-075530\\best_model_epoch-08_val_acc-0.9630.keras\n",
      "\n",
      "Epoch 8: val_accuracy improved from 0.96150 to 0.96300, saving model to wandb_models/Base_MLP_Model-lr_0.01-bs_64-20250917-075530/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9580 - loss: 0.1429 - val_accuracy: 0.9630 - val_loss: 0.1210\n",
      "Epoch 9/100\n",
      "\u001b[1m799/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9603 - loss: 0.1369\n",
      "Epoch 9: val_accuracy did not improve from 0.96300\n",
      "\n",
      "Epoch 9: val_accuracy did not improve from 0.96300\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9612 - loss: 0.1330 - val_accuracy: 0.9630 - val_loss: 0.1164\n",
      "Epoch 10/100\n",
      "\u001b[1m803/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9632 - loss: 0.1280\n",
      "Epoch 10: val_accuracy improved from 0.96300 to 0.96567, saving model to MLP_Models\\Base_MLP_Model-lr_0.01-bs_64-20250917-075530\\best_model_epoch-10_val_acc-0.9657.keras\n",
      "\n",
      "Epoch 10: val_accuracy improved from 0.96300 to 0.96567, saving model to wandb_models/Base_MLP_Model-lr_0.01-bs_64-20250917-075530/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9639 - loss: 0.1245 - val_accuracy: 0.9657 - val_loss: 0.1132\n",
      "Epoch 11/100\n",
      "\u001b[1m805/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9650 - loss: 0.1205\n",
      "Epoch 11: val_accuracy improved from 0.96567 to 0.96600, saving model to MLP_Models\\Base_MLP_Model-lr_0.01-bs_64-20250917-075530\\best_model_epoch-11_val_acc-0.9660.keras\n",
      "\n",
      "Epoch 11: val_accuracy improved from 0.96567 to 0.96600, saving model to wandb_models/Base_MLP_Model-lr_0.01-bs_64-20250917-075530/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9657 - loss: 0.1172 - val_accuracy: 0.9660 - val_loss: 0.1105\n",
      "Epoch 12/100\n",
      "\u001b[1m822/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9668 - loss: 0.1138\n",
      "Epoch 12: val_accuracy improved from 0.96600 to 0.96783, saving model to MLP_Models\\Base_MLP_Model-lr_0.01-bs_64-20250917-075530\\best_model_epoch-12_val_acc-0.9678.keras\n",
      "\n",
      "Epoch 12: val_accuracy improved from 0.96600 to 0.96783, saving model to wandb_models/Base_MLP_Model-lr_0.01-bs_64-20250917-075530/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9675 - loss: 0.1108 - val_accuracy: 0.9678 - val_loss: 0.1082\n",
      "Epoch 13/100\n",
      "\u001b[1m818/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9681 - loss: 0.1078\n",
      "Epoch 13: val_accuracy improved from 0.96783 to 0.96833, saving model to MLP_Models\\Base_MLP_Model-lr_0.01-bs_64-20250917-075530\\best_model_epoch-13_val_acc-0.9683.keras\n",
      "\n",
      "Epoch 13: val_accuracy improved from 0.96783 to 0.96833, saving model to wandb_models/Base_MLP_Model-lr_0.01-bs_64-20250917-075530/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9693 - loss: 0.1051 - val_accuracy: 0.9683 - val_loss: 0.1066\n",
      "Epoch 14/100\n",
      "\u001b[1m804/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9700 - loss: 0.1023\n",
      "Epoch 14: val_accuracy improved from 0.96833 to 0.96900, saving model to MLP_Models\\Base_MLP_Model-lr_0.01-bs_64-20250917-075530\\best_model_epoch-14_val_acc-0.9690.keras\n",
      "\n",
      "Epoch 14: val_accuracy improved from 0.96833 to 0.96900, saving model to wandb_models/Base_MLP_Model-lr_0.01-bs_64-20250917-075530/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9711 - loss: 0.0998 - val_accuracy: 0.9690 - val_loss: 0.1051\n",
      "Epoch 15/100\n",
      "\u001b[1m827/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9716 - loss: 0.0971\n",
      "Epoch 15: val_accuracy improved from 0.96900 to 0.96950, saving model to MLP_Models\\Base_MLP_Model-lr_0.01-bs_64-20250917-075530\\best_model_epoch-15_val_acc-0.9695.keras\n",
      "\n",
      "Epoch 15: val_accuracy improved from 0.96900 to 0.96950, saving model to wandb_models/Base_MLP_Model-lr_0.01-bs_64-20250917-075530/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9727 - loss: 0.0950 - val_accuracy: 0.9695 - val_loss: 0.1044\n",
      "Epoch 16/100\n",
      "\u001b[1m839/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9732 - loss: 0.0926\n",
      "Epoch 16: val_accuracy did not improve from 0.96950\n",
      "\n",
      "Epoch 16: val_accuracy did not improve from 0.96950\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9739 - loss: 0.0908 - val_accuracy: 0.9687 - val_loss: 0.1036\n",
      "Epoch 17/100\n",
      "\u001b[1m825/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9746 - loss: 0.0886\n",
      "Epoch 17: val_accuracy did not improve from 0.96950\n",
      "\n",
      "Epoch 17: val_accuracy did not improve from 0.96950\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9751 - loss: 0.0868 - val_accuracy: 0.9690 - val_loss: 0.1027\n",
      "Epoch 18/100\n",
      "\u001b[1m823/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 983us/step - accuracy: 0.9760 - loss: 0.0847\n",
      "Epoch 18: val_accuracy did not improve from 0.96950\n",
      "\n",
      "Epoch 18: val_accuracy did not improve from 0.96950\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9764 - loss: 0.0831 - val_accuracy: 0.9687 - val_loss: 0.1027\n",
      "Epoch 19/100\n",
      "\u001b[1m839/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9773 - loss: 0.0812\n",
      "Epoch 19: val_accuracy improved from 0.96950 to 0.97000, saving model to MLP_Models\\Base_MLP_Model-lr_0.01-bs_64-20250917-075530\\best_model_epoch-19_val_acc-0.9700.keras\n",
      "\n",
      "Epoch 19: val_accuracy improved from 0.96950 to 0.97000, saving model to wandb_models/Base_MLP_Model-lr_0.01-bs_64-20250917-075530/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9776 - loss: 0.0797 - val_accuracy: 0.9700 - val_loss: 0.1019\n",
      "Epoch 20/100\n",
      "\u001b[1m822/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 981us/step - accuracy: 0.9783 - loss: 0.0779\n",
      "Epoch 20: val_accuracy did not improve from 0.97000\n",
      "\n",
      "Epoch 20: val_accuracy did not improve from 0.97000\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9784 - loss: 0.0765 - val_accuracy: 0.9698 - val_loss: 0.1017\n",
      "Epoch 21/100\n",
      "\u001b[1m826/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9793 - loss: 0.0748\n",
      "Epoch 21: val_accuracy did not improve from 0.97000\n",
      "\n",
      "Epoch 21: val_accuracy did not improve from 0.97000\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9790 - loss: 0.0735 - val_accuracy: 0.9698 - val_loss: 0.1014\n",
      "Epoch 22/100\n",
      "\u001b[1m823/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9801 - loss: 0.0720\n",
      "Epoch 22: val_accuracy improved from 0.97000 to 0.97017, saving model to MLP_Models\\Base_MLP_Model-lr_0.01-bs_64-20250917-075530\\best_model_epoch-22_val_acc-0.9702.keras\n",
      "\n",
      "Epoch 22: val_accuracy improved from 0.97000 to 0.97017, saving model to wandb_models/Base_MLP_Model-lr_0.01-bs_64-20250917-075530/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9799 - loss: 0.0707 - val_accuracy: 0.9702 - val_loss: 0.1009\n",
      "Epoch 23/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9807 - loss: 0.0692\n",
      "Epoch 23: val_accuracy did not improve from 0.97017\n",
      "\n",
      "Epoch 23: val_accuracy did not improve from 0.97017\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9807 - loss: 0.0680 - val_accuracy: 0.9700 - val_loss: 0.1008\n",
      "Epoch 24/100\n",
      "\u001b[1m831/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9812 - loss: 0.0666\n",
      "Epoch 24: val_accuracy improved from 0.97017 to 0.97033, saving model to MLP_Models\\Base_MLP_Model-lr_0.01-bs_64-20250917-075530\\best_model_epoch-24_val_acc-0.9703.keras\n",
      "\n",
      "Epoch 24: val_accuracy improved from 0.97017 to 0.97033, saving model to wandb_models/Base_MLP_Model-lr_0.01-bs_64-20250917-075530/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9811 - loss: 0.0655 - val_accuracy: 0.9703 - val_loss: 0.1007\n",
      "Epoch 25/100\n",
      "\u001b[1m811/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9820 - loss: 0.0642\n",
      "Epoch 25: val_accuracy improved from 0.97033 to 0.97050, saving model to MLP_Models\\Base_MLP_Model-lr_0.01-bs_64-20250917-075530\\best_model_epoch-25_val_acc-0.9705.keras\n",
      "\n",
      "Epoch 25: val_accuracy improved from 0.97033 to 0.97050, saving model to wandb_models/Base_MLP_Model-lr_0.01-bs_64-20250917-075530/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9820 - loss: 0.0631 - val_accuracy: 0.9705 - val_loss: 0.1009\n",
      "Epoch 26/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9827 - loss: 0.0617\n",
      "Epoch 26: val_accuracy improved from 0.97050 to 0.97117, saving model to MLP_Models\\Base_MLP_Model-lr_0.01-bs_64-20250917-075530\\best_model_epoch-26_val_acc-0.9712.keras\n",
      "\n",
      "Epoch 26: val_accuracy improved from 0.97050 to 0.97117, saving model to wandb_models/Base_MLP_Model-lr_0.01-bs_64-20250917-075530/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9826 - loss: 0.0608 - val_accuracy: 0.9712 - val_loss: 0.1008\n",
      "Epoch 27/100\n",
      "\u001b[1m832/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9837 - loss: 0.0595\n",
      "Epoch 27: val_accuracy improved from 0.97117 to 0.97133, saving model to MLP_Models\\Base_MLP_Model-lr_0.01-bs_64-20250917-075530\\best_model_epoch-27_val_acc-0.9713.keras\n",
      "\n",
      "Epoch 27: val_accuracy improved from 0.97117 to 0.97133, saving model to wandb_models/Base_MLP_Model-lr_0.01-bs_64-20250917-075530/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9835 - loss: 0.0586 - val_accuracy: 0.9713 - val_loss: 0.1011\n",
      "Epoch 28/100\n",
      "\u001b[1m825/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9841 - loss: 0.0574\n",
      "Epoch 28: val_accuracy improved from 0.97133 to 0.97200, saving model to MLP_Models\\Base_MLP_Model-lr_0.01-bs_64-20250917-075530\\best_model_epoch-28_val_acc-0.9720.keras\n",
      "\n",
      "Epoch 28: val_accuracy improved from 0.97133 to 0.97200, saving model to wandb_models/Base_MLP_Model-lr_0.01-bs_64-20250917-075530/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.9841 - loss: 0.0565 - val_accuracy: 0.9720 - val_loss: 0.1012\n",
      "Epoch 29/100\n",
      "\u001b[1m832/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9847 - loss: 0.0554\n",
      "Epoch 29: val_accuracy did not improve from 0.97200\n",
      "\n",
      "Epoch 29: val_accuracy did not improve from 0.97200\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9846 - loss: 0.0545 - val_accuracy: 0.9717 - val_loss: 0.1012\n",
      "Epoch 30/100\n",
      "\u001b[1m820/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9855 - loss: 0.0534\n",
      "Epoch 30: val_accuracy did not improve from 0.97200\n",
      "\n",
      "Epoch 30: val_accuracy did not improve from 0.97200\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - accuracy: 0.9854 - loss: 0.0526 - val_accuracy: 0.9715 - val_loss: 0.1014\n",
      "Epoch 31/100\n",
      "\u001b[1m838/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9861 - loss: 0.0516\n",
      "Epoch 31: val_accuracy did not improve from 0.97200\n",
      "\n",
      "Epoch 31: val_accuracy did not improve from 0.97200\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9860 - loss: 0.0508 - val_accuracy: 0.9715 - val_loss: 0.1014\n",
      "Epoch 32/100\n",
      "\u001b[1m817/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9869 - loss: 0.0498\n",
      "Epoch 32: val_accuracy did not improve from 0.97200\n",
      "\n",
      "Epoch 32: val_accuracy did not improve from 0.97200\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9867 - loss: 0.0490 - val_accuracy: 0.9712 - val_loss: 0.1014\n",
      "Epoch 33/100\n",
      "\u001b[1m840/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9871 - loss: 0.0480\n",
      "Epoch 33: val_accuracy did not improve from 0.97200\n",
      "\n",
      "Epoch 33: val_accuracy did not improve from 0.97200\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9871 - loss: 0.0474 - val_accuracy: 0.9712 - val_loss: 0.1023\n",
      "Epoch 34/100\n",
      "\u001b[1m822/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9878 - loss: 0.0464\n",
      "Epoch 34: val_accuracy did not improve from 0.97200\n",
      "\n",
      "Epoch 34: val_accuracy did not improve from 0.97200\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9877 - loss: 0.0458 - val_accuracy: 0.9710 - val_loss: 0.1024\n",
      "Epoch 35/100\n",
      "\u001b[1m831/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9882 - loss: 0.0449\n",
      "Epoch 35: val_accuracy did not improve from 0.97200\n",
      "\n",
      "Epoch 35: val_accuracy did not improve from 0.97200\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9884 - loss: 0.0442 - val_accuracy: 0.9713 - val_loss: 0.1031\n",
      "Epoch 36/100\n",
      "\u001b[1m812/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9885 - loss: 0.0434\n",
      "Epoch 36: val_accuracy did not improve from 0.97200\n",
      "\n",
      "Epoch 36: val_accuracy did not improve from 0.97200\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9887 - loss: 0.0426 - val_accuracy: 0.9715 - val_loss: 0.1032\n",
      "Epoch 37/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9891 - loss: 0.0418\n",
      "Epoch 37: val_accuracy did not improve from 0.97200\n",
      "\n",
      "Epoch 37: val_accuracy did not improve from 0.97200\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9892 - loss: 0.0412 - val_accuracy: 0.9715 - val_loss: 0.1038\n",
      "Epoch 38/100\n",
      "\u001b[1m810/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9895 - loss: 0.0403\n",
      "Epoch 38: val_accuracy did not improve from 0.97200\n",
      "\n",
      "Epoch 38: val_accuracy did not improve from 0.97200\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9896 - loss: 0.0397 - val_accuracy: 0.9713 - val_loss: 0.1050\n",
      "\n",
      "Training history saved to: MLP_Models\\Base_MLP_Model-lr_0.01-bs_64-20250917-075530\\training_history.pkl\n",
      "\n",
      "--- Peak Performance Summary ---\n",
      "Best validation accuracy:           0.9720\n",
      "Associated training accuracy:       0.9841\n",
      "Occurred at epoch:                  28\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch/accuracy</td><td>▁▆▆▇▇▇▇▇▇▇▇▇▇▇████████████████████████</td></tr><tr><td>epoch/epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>epoch/learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/loss</td><td>█▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/val_accuracy</td><td>▁▄▅▆▆▆▇▇▇▇▇███████████████████████████</td></tr><tr><td>epoch/val_loss</td><td>█▅▄▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch/accuracy</td><td>0.98957</td></tr><tr><td>epoch/epoch</td><td>37</td></tr><tr><td>epoch/learning_rate</td><td>0.01</td></tr><tr><td>epoch/loss</td><td>0.03971</td></tr><tr><td>epoch/val_accuracy</td><td>0.97133</td></tr><tr><td>epoch/val_loss</td><td>0.10498</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Base_MLP_Model-lr_0.01-bs_64-20250917-075530</strong> at: <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/3o7bm0gq' target=\"_blank\">https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/3o7bm0gq</a><br> View project at: <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2' target=\"_blank\">https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2</a><br>Synced 5 W&B file(s), 0 media file(s), 42 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250917_075530-3o7bm0gq\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Adapting the normalisation layer...\n",
      "Adaptation complete.\n",
      "\n",
      "\n",
      "--- Starting Experiment: Base_MLP_Model_2 ---\n",
      "\n",
      "--- Model Architecture ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"Base_MLP_Model_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"Base_MLP_Model_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ normalization_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Normalization</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">50,240</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,320</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">650</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ normalization_4 (\u001b[38;5;33mNormalization\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m1\u001b[0m)      │             \u001b[38;5;34m3\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_4 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m784\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_16 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m50,240\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_17 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │         \u001b[38;5;34m8,320\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_18 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_19 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │           \u001b[38;5;34m650\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">67,469</span> (263.55 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m67,469\u001b[0m (263.55 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">67,466</span> (263.54 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m67,466\u001b[0m (263.54 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3</span> (16.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m3\u001b[0m (16.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Hyperparameters ---\n",
      "optimiser           : adamw\n",
      "learning_rate       : 0.001\n",
      "epochs              : 100\n",
      "batch_size          : 64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "creating run (0.0s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\TimVos\\VSC Projects\\CSE5ML\\Assessment 2\\wandb\\run-20250917_075738-ochi7svc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/ochi7svc' target=\"_blank\">Base_MLP_Model_2-lr_0.001-bs_64-20250917-075738</a></strong> to <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2' target=\"_blank\">https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/ochi7svc' target=\"_blank\">https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/ochi7svc</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m836/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8462 - loss: 0.4809\n",
      "Epoch 1: val_accuracy improved from None to 0.96917, saving model to MLP_Models\\Base_MLP_Model_2-lr_0.001-bs_64-20250917-075738\\best_model_epoch-01_val_acc-0.9692.keras\n",
      "\n",
      "Epoch 1: val_accuracy improved from None to 0.96917, saving model to wandb_models/Base_MLP_Model_2-lr_0.001-bs_64-20250917-075738/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9151 - loss: 0.2723 - val_accuracy: 0.9692 - val_loss: 0.1087\n",
      "Epoch 2/100\n",
      "\u001b[1m831/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9569 - loss: 0.1349\n",
      "Epoch 2: val_accuracy improved from 0.96917 to 0.97433, saving model to MLP_Models\\Base_MLP_Model_2-lr_0.001-bs_64-20250917-075738\\best_model_epoch-02_val_acc-0.9743.keras\n",
      "\n",
      "Epoch 2: val_accuracy improved from 0.96917 to 0.97433, saving model to wandb_models/Base_MLP_Model_2-lr_0.001-bs_64-20250917-075738/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9610 - loss: 0.1229 - val_accuracy: 0.9743 - val_loss: 0.0839\n",
      "Epoch 3/100\n",
      "\u001b[1m832/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9714 - loss: 0.0938\n",
      "Epoch 3: val_accuracy improved from 0.97433 to 0.97533, saving model to MLP_Models\\Base_MLP_Model_2-lr_0.001-bs_64-20250917-075738\\best_model_epoch-03_val_acc-0.9753.keras\n",
      "\n",
      "Epoch 3: val_accuracy improved from 0.97433 to 0.97533, saving model to wandb_models/Base_MLP_Model_2-lr_0.001-bs_64-20250917-075738/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9725 - loss: 0.0882 - val_accuracy: 0.9753 - val_loss: 0.0799\n",
      "Epoch 4/100\n",
      "\u001b[1m834/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9780 - loss: 0.0723\n",
      "Epoch 4: val_accuracy improved from 0.97533 to 0.97567, saving model to MLP_Models\\Base_MLP_Model_2-lr_0.001-bs_64-20250917-075738\\best_model_epoch-04_val_acc-0.9757.keras\n",
      "\n",
      "Epoch 4: val_accuracy improved from 0.97533 to 0.97567, saving model to wandb_models/Base_MLP_Model_2-lr_0.001-bs_64-20250917-075738/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.9792 - loss: 0.0681 - val_accuracy: 0.9757 - val_loss: 0.0877\n",
      "Epoch 5/100\n",
      "\u001b[1m824/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9806 - loss: 0.0614\n",
      "Epoch 5: val_accuracy did not improve from 0.97567\n",
      "\n",
      "Epoch 5: val_accuracy did not improve from 0.97567\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9809 - loss: 0.0585 - val_accuracy: 0.9757 - val_loss: 0.0872\n",
      "Epoch 6/100\n",
      "\u001b[1m840/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9830 - loss: 0.0537\n",
      "Epoch 6: val_accuracy did not improve from 0.97567\n",
      "\n",
      "Epoch 6: val_accuracy did not improve from 0.97567\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9840 - loss: 0.0503 - val_accuracy: 0.9742 - val_loss: 0.1012\n",
      "Epoch 7/100\n",
      "\u001b[1m820/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9862 - loss: 0.0427\n",
      "Epoch 7: val_accuracy improved from 0.97567 to 0.97767, saving model to MLP_Models\\Base_MLP_Model_2-lr_0.001-bs_64-20250917-075738\\best_model_epoch-07_val_acc-0.9777.keras\n",
      "\n",
      "Epoch 7: val_accuracy improved from 0.97567 to 0.97767, saving model to wandb_models/Base_MLP_Model_2-lr_0.001-bs_64-20250917-075738/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9866 - loss: 0.0419 - val_accuracy: 0.9777 - val_loss: 0.0880\n",
      "Epoch 8/100\n",
      "\u001b[1m831/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9872 - loss: 0.0378\n",
      "Epoch 8: val_accuracy did not improve from 0.97767\n",
      "\n",
      "Epoch 8: val_accuracy did not improve from 0.97767\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9865 - loss: 0.0393 - val_accuracy: 0.9740 - val_loss: 0.1142\n",
      "Epoch 9/100\n",
      "\u001b[1m830/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9880 - loss: 0.0347\n",
      "Epoch 9: val_accuracy improved from 0.97767 to 0.97867, saving model to MLP_Models\\Base_MLP_Model_2-lr_0.001-bs_64-20250917-075738\\best_model_epoch-09_val_acc-0.9787.keras\n",
      "\n",
      "Epoch 9: val_accuracy improved from 0.97767 to 0.97867, saving model to wandb_models/Base_MLP_Model_2-lr_0.001-bs_64-20250917-075738/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9882 - loss: 0.0341 - val_accuracy: 0.9787 - val_loss: 0.0955\n",
      "Epoch 10/100\n",
      "\u001b[1m819/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9887 - loss: 0.0307\n",
      "Epoch 10: val_accuracy did not improve from 0.97867\n",
      "\n",
      "Epoch 10: val_accuracy did not improve from 0.97867\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9884 - loss: 0.0325 - val_accuracy: 0.9735 - val_loss: 0.1098\n",
      "Epoch 11/100\n",
      "\u001b[1m820/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9895 - loss: 0.0308\n",
      "Epoch 11: val_accuracy did not improve from 0.97867\n",
      "\n",
      "Epoch 11: val_accuracy did not improve from 0.97867\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9892 - loss: 0.0302 - val_accuracy: 0.9755 - val_loss: 0.1129\n",
      "Epoch 12/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9916 - loss: 0.0243\n",
      "Epoch 12: val_accuracy did not improve from 0.97867\n",
      "\n",
      "Epoch 12: val_accuracy did not improve from 0.97867\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9918 - loss: 0.0240 - val_accuracy: 0.9780 - val_loss: 0.0962\n",
      "Epoch 13/100\n",
      "\u001b[1m828/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9899 - loss: 0.0287\n",
      "Epoch 13: val_accuracy did not improve from 0.97867\n",
      "\n",
      "Epoch 13: val_accuracy did not improve from 0.97867\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9907 - loss: 0.0271 - val_accuracy: 0.9762 - val_loss: 0.1198\n",
      "Epoch 14/100\n",
      "\u001b[1m829/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9912 - loss: 0.0244\n",
      "Epoch 14: val_accuracy did not improve from 0.97867\n",
      "\n",
      "Epoch 14: val_accuracy did not improve from 0.97867\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9926 - loss: 0.0222 - val_accuracy: 0.9768 - val_loss: 0.1224\n",
      "Epoch 15/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9920 - loss: 0.0231\n",
      "Epoch 15: val_accuracy did not improve from 0.97867\n",
      "\n",
      "Epoch 15: val_accuracy did not improve from 0.97867\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9922 - loss: 0.0227 - val_accuracy: 0.9722 - val_loss: 0.1497\n",
      "Epoch 16/100\n",
      "\u001b[1m838/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9925 - loss: 0.0210\n",
      "Epoch 16: val_accuracy did not improve from 0.97867\n",
      "\n",
      "Epoch 16: val_accuracy did not improve from 0.97867\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9932 - loss: 0.0192 - val_accuracy: 0.9753 - val_loss: 0.1294\n",
      "Epoch 17/100\n",
      "\u001b[1m829/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9925 - loss: 0.0218\n",
      "Epoch 17: val_accuracy did not improve from 0.97867\n",
      "\n",
      "Epoch 17: val_accuracy did not improve from 0.97867\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9922 - loss: 0.0225 - val_accuracy: 0.9767 - val_loss: 0.1257\n",
      "Epoch 18/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9938 - loss: 0.0182\n",
      "Epoch 18: val_accuracy did not improve from 0.97867\n",
      "\n",
      "Epoch 18: val_accuracy did not improve from 0.97867\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9939 - loss: 0.0181 - val_accuracy: 0.9742 - val_loss: 0.1294\n",
      "Epoch 19/100\n",
      "\u001b[1m816/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9926 - loss: 0.0229\n",
      "Epoch 19: val_accuracy did not improve from 0.97867\n",
      "\n",
      "Epoch 19: val_accuracy did not improve from 0.97867\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9936 - loss: 0.0192 - val_accuracy: 0.9762 - val_loss: 0.1211\n",
      "\n",
      "Training history saved to: MLP_Models\\Base_MLP_Model_2-lr_0.001-bs_64-20250917-075738\\training_history.pkl\n",
      "\n",
      "--- Peak Performance Summary ---\n",
      "Best validation accuracy:           0.9787\n",
      "Associated training accuracy:       0.9882\n",
      "Occurred at epoch:                  9\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch/accuracy</td><td>▁▅▆▇▇▇▇▇▇██████████</td></tr><tr><td>epoch/epoch</td><td>▁▁▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇██</td></tr><tr><td>epoch/learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/loss</td><td>█▄▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/val_accuracy</td><td>▁▅▆▆▆▅▇▅█▄▆█▆▇▃▆▇▅▆</td></tr><tr><td>epoch/val_loss</td><td>▄▁▁▂▂▃▂▄▃▄▄▃▅▅█▆▆▆▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch/accuracy</td><td>0.99361</td></tr><tr><td>epoch/epoch</td><td>18</td></tr><tr><td>epoch/learning_rate</td><td>0.001</td></tr><tr><td>epoch/loss</td><td>0.01922</td></tr><tr><td>epoch/val_accuracy</td><td>0.97617</td></tr><tr><td>epoch/val_loss</td><td>0.12111</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Base_MLP_Model_2-lr_0.001-bs_64-20250917-075738</strong> at: <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/ochi7svc' target=\"_blank\">https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/ochi7svc</a><br> View project at: <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2' target=\"_blank\">https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2</a><br>Synced 5 W&B file(s), 0 media file(s), 12 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250917_075738-ochi7svc\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Adapting the normalisation layer...\n",
      "Adaptation complete.\n",
      "\n",
      "\n",
      "--- Starting Experiment: Base_MLP_Model_2 ---\n",
      "\n",
      "--- Model Architecture ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"Base_MLP_Model_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"Base_MLP_Model_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ normalization_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Normalization</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">50,240</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,320</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">650</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ normalization_5 (\u001b[38;5;33mNormalization\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m1\u001b[0m)      │             \u001b[38;5;34m3\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_5 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m784\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_20 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m50,240\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_21 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │         \u001b[38;5;34m8,320\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_22 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_23 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │           \u001b[38;5;34m650\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">67,469</span> (263.55 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m67,469\u001b[0m (263.55 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">67,466</span> (263.54 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m67,466\u001b[0m (263.54 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3</span> (16.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m3\u001b[0m (16.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Hyperparameters ---\n",
      "optimiser           : adamw\n",
      "learning_rate       : 0.01\n",
      "epochs              : 100\n",
      "batch_size          : 64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "creating run (0.0s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\TimVos\\VSC Projects\\CSE5ML\\Assessment 2\\wandb\\run-20250917_075846-in5wgf26</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/in5wgf26' target=\"_blank\">Base_MLP_Model_2-lr_0.01-bs_64-20250917-075846</a></strong> to <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2' target=\"_blank\">https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/in5wgf26' target=\"_blank\">https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/in5wgf26</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m818/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8496 - loss: 0.4844\n",
      "Epoch 1: val_accuracy improved from None to 0.94600, saving model to MLP_Models\\Base_MLP_Model_2-lr_0.01-bs_64-20250917-075846\\best_model_epoch-01_val_acc-0.9460.keras\n",
      "\n",
      "Epoch 1: val_accuracy improved from None to 0.94600, saving model to wandb_models/Base_MLP_Model_2-lr_0.01-bs_64-20250917-075846/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9032 - loss: 0.3336 - val_accuracy: 0.9460 - val_loss: 0.1902\n",
      "Epoch 2/100\n",
      "\u001b[1m829/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9357 - loss: 0.2398\n",
      "Epoch 2: val_accuracy improved from 0.94600 to 0.94833, saving model to MLP_Models\\Base_MLP_Model_2-lr_0.01-bs_64-20250917-075846\\best_model_epoch-02_val_acc-0.9483.keras\n",
      "\n",
      "Epoch 2: val_accuracy improved from 0.94600 to 0.94833, saving model to wandb_models/Base_MLP_Model_2-lr_0.01-bs_64-20250917-075846/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9389 - loss: 0.2290 - val_accuracy: 0.9483 - val_loss: 0.1960\n",
      "Epoch 3/100\n",
      "\u001b[1m829/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9441 - loss: 0.2137\n",
      "Epoch 3: val_accuracy improved from 0.94833 to 0.95650, saving model to MLP_Models\\Base_MLP_Model_2-lr_0.01-bs_64-20250917-075846\\best_model_epoch-03_val_acc-0.9565.keras\n",
      "\n",
      "Epoch 3: val_accuracy improved from 0.94833 to 0.95650, saving model to wandb_models/Base_MLP_Model_2-lr_0.01-bs_64-20250917-075846/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9444 - loss: 0.2095 - val_accuracy: 0.9565 - val_loss: 0.1956\n",
      "Epoch 4/100\n",
      "\u001b[1m835/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9480 - loss: 0.1972\n",
      "Epoch 4: val_accuracy did not improve from 0.95650\n",
      "\n",
      "Epoch 4: val_accuracy did not improve from 0.95650\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9476 - loss: 0.2003 - val_accuracy: 0.9542 - val_loss: 0.1566\n",
      "Epoch 5/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9521 - loss: 0.1854\n",
      "Epoch 5: val_accuracy improved from 0.95650 to 0.95933, saving model to MLP_Models\\Base_MLP_Model_2-lr_0.01-bs_64-20250917-075846\\best_model_epoch-05_val_acc-0.9593.keras\n",
      "\n",
      "Epoch 5: val_accuracy improved from 0.95650 to 0.95933, saving model to wandb_models/Base_MLP_Model_2-lr_0.01-bs_64-20250917-075846/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9526 - loss: 0.1837 - val_accuracy: 0.9593 - val_loss: 0.1670\n",
      "Epoch 6/100\n",
      "\u001b[1m836/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9514 - loss: 0.1910\n",
      "Epoch 6: val_accuracy improved from 0.95933 to 0.96300, saving model to MLP_Models\\Base_MLP_Model_2-lr_0.01-bs_64-20250917-075846\\best_model_epoch-06_val_acc-0.9630.keras\n",
      "\n",
      "Epoch 6: val_accuracy improved from 0.95933 to 0.96300, saving model to wandb_models/Base_MLP_Model_2-lr_0.01-bs_64-20250917-075846/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9525 - loss: 0.1877 - val_accuracy: 0.9630 - val_loss: 0.1490\n",
      "Epoch 7/100\n",
      "\u001b[1m811/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9593 - loss: 0.1681\n",
      "Epoch 7: val_accuracy improved from 0.96300 to 0.96333, saving model to MLP_Models\\Base_MLP_Model_2-lr_0.01-bs_64-20250917-075846\\best_model_epoch-07_val_acc-0.9633.keras\n",
      "\n",
      "Epoch 7: val_accuracy improved from 0.96300 to 0.96333, saving model to wandb_models/Base_MLP_Model_2-lr_0.01-bs_64-20250917-075846/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9587 - loss: 0.1660 - val_accuracy: 0.9633 - val_loss: 0.1426\n",
      "Epoch 8/100\n",
      "\u001b[1m835/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9598 - loss: 0.1594\n",
      "Epoch 8: val_accuracy improved from 0.96333 to 0.96500, saving model to MLP_Models\\Base_MLP_Model_2-lr_0.01-bs_64-20250917-075846\\best_model_epoch-08_val_acc-0.9650.keras\n",
      "\n",
      "Epoch 8: val_accuracy improved from 0.96333 to 0.96500, saving model to wandb_models/Base_MLP_Model_2-lr_0.01-bs_64-20250917-075846/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.9600 - loss: 0.1583 - val_accuracy: 0.9650 - val_loss: 0.1372\n",
      "Epoch 9/100\n",
      "\u001b[1m827/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9632 - loss: 0.1495\n",
      "Epoch 9: val_accuracy improved from 0.96500 to 0.96633, saving model to MLP_Models\\Base_MLP_Model_2-lr_0.01-bs_64-20250917-075846\\best_model_epoch-09_val_acc-0.9663.keras\n",
      "\n",
      "Epoch 9: val_accuracy improved from 0.96500 to 0.96633, saving model to wandb_models/Base_MLP_Model_2-lr_0.01-bs_64-20250917-075846/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9613 - loss: 0.1541 - val_accuracy: 0.9663 - val_loss: 0.1468\n",
      "Epoch 10/100\n",
      "\u001b[1m834/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9614 - loss: 0.1595\n",
      "Epoch 10: val_accuracy did not improve from 0.96633\n",
      "\n",
      "Epoch 10: val_accuracy did not improve from 0.96633\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9618 - loss: 0.1535 - val_accuracy: 0.9613 - val_loss: 0.1669\n",
      "Epoch 11/100\n",
      "\u001b[1m839/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9621 - loss: 0.1545\n",
      "Epoch 11: val_accuracy did not improve from 0.96633\n",
      "\n",
      "Epoch 11: val_accuracy did not improve from 0.96633\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9596 - loss: 0.1657 - val_accuracy: 0.9627 - val_loss: 0.1643\n",
      "Epoch 12/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9602 - loss: 0.1638\n",
      "Epoch 12: val_accuracy did not improve from 0.96633\n",
      "\n",
      "Epoch 12: val_accuracy did not improve from 0.96633\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9626 - loss: 0.1509 - val_accuracy: 0.9645 - val_loss: 0.1695\n",
      "Epoch 13/100\n",
      "\u001b[1m837/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9659 - loss: 0.1389\n",
      "Epoch 13: val_accuracy did not improve from 0.96633\n",
      "\n",
      "Epoch 13: val_accuracy did not improve from 0.96633\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9646 - loss: 0.1430 - val_accuracy: 0.9642 - val_loss: 0.1542\n",
      "Epoch 14/100\n",
      "\u001b[1m839/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9648 - loss: 0.1427\n",
      "Epoch 14: val_accuracy did not improve from 0.96633\n",
      "\n",
      "Epoch 14: val_accuracy did not improve from 0.96633\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9648 - loss: 0.1402 - val_accuracy: 0.9630 - val_loss: 0.1528\n",
      "Epoch 15/100\n",
      "\u001b[1m832/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9657 - loss: 0.1393\n",
      "Epoch 15: val_accuracy did not improve from 0.96633\n",
      "\n",
      "Epoch 15: val_accuracy did not improve from 0.96633\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9650 - loss: 0.1409 - val_accuracy: 0.9655 - val_loss: 0.1564\n",
      "Epoch 16/100\n",
      "\u001b[1m823/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9648 - loss: 0.1458\n",
      "Epoch 16: val_accuracy did not improve from 0.96633\n",
      "\n",
      "Epoch 16: val_accuracy did not improve from 0.96633\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9641 - loss: 0.1486 - val_accuracy: 0.9635 - val_loss: 0.1671\n",
      "Epoch 17/100\n",
      "\u001b[1m817/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9666 - loss: 0.1396\n",
      "Epoch 17: val_accuracy improved from 0.96633 to 0.96833, saving model to MLP_Models\\Base_MLP_Model_2-lr_0.01-bs_64-20250917-075846\\best_model_epoch-17_val_acc-0.9683.keras\n",
      "\n",
      "Epoch 17: val_accuracy improved from 0.96633 to 0.96833, saving model to wandb_models/Base_MLP_Model_2-lr_0.01-bs_64-20250917-075846/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9670 - loss: 0.1331 - val_accuracy: 0.9683 - val_loss: 0.1479\n",
      "Epoch 18/100\n",
      "\u001b[1m829/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9689 - loss: 0.1300\n",
      "Epoch 18: val_accuracy did not improve from 0.96833\n",
      "\n",
      "Epoch 18: val_accuracy did not improve from 0.96833\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9669 - loss: 0.1365 - val_accuracy: 0.9657 - val_loss: 0.1646\n",
      "Epoch 19/100\n",
      "\u001b[1m830/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9663 - loss: 0.1409\n",
      "Epoch 19: val_accuracy did not improve from 0.96833\n",
      "\n",
      "Epoch 19: val_accuracy did not improve from 0.96833\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9656 - loss: 0.1455 - val_accuracy: 0.9672 - val_loss: 0.1481\n",
      "Epoch 20/100\n",
      "\u001b[1m817/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9698 - loss: 0.1223\n",
      "Epoch 20: val_accuracy did not improve from 0.96833\n",
      "\n",
      "Epoch 20: val_accuracy did not improve from 0.96833\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9689 - loss: 0.1282 - val_accuracy: 0.9482 - val_loss: 0.2601\n",
      "Epoch 21/100\n",
      "\u001b[1m812/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9643 - loss: 0.1610\n",
      "Epoch 21: val_accuracy did not improve from 0.96833\n",
      "\n",
      "Epoch 21: val_accuracy did not improve from 0.96833\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9667 - loss: 0.1418 - val_accuracy: 0.9643 - val_loss: 0.1682\n",
      "Epoch 22/100\n",
      "\u001b[1m816/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9681 - loss: 0.1402\n",
      "Epoch 22: val_accuracy did not improve from 0.96833\n",
      "\n",
      "Epoch 22: val_accuracy did not improve from 0.96833\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9667 - loss: 0.1445 - val_accuracy: 0.9593 - val_loss: 0.2112\n",
      "Epoch 23/100\n",
      "\u001b[1m829/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9688 - loss: 0.1261\n",
      "Epoch 23: val_accuracy did not improve from 0.96833\n",
      "\n",
      "Epoch 23: val_accuracy did not improve from 0.96833\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9690 - loss: 0.1248 - val_accuracy: 0.9668 - val_loss: 0.1588\n",
      "Epoch 24/100\n",
      "\u001b[1m813/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9699 - loss: 0.1268\n",
      "Epoch 24: val_accuracy did not improve from 0.96833\n",
      "\n",
      "Epoch 24: val_accuracy did not improve from 0.96833\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9688 - loss: 0.1316 - val_accuracy: 0.9663 - val_loss: 0.1544\n",
      "Epoch 25/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9721 - loss: 0.1155\n",
      "Epoch 25: val_accuracy did not improve from 0.96833\n",
      "\n",
      "Epoch 25: val_accuracy did not improve from 0.96833\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9715 - loss: 0.1166 - val_accuracy: 0.9655 - val_loss: 0.1651\n",
      "Epoch 26/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9690 - loss: 0.1350\n",
      "Epoch 26: val_accuracy did not improve from 0.96833\n",
      "\n",
      "Epoch 26: val_accuracy did not improve from 0.96833\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9703 - loss: 0.1248 - val_accuracy: 0.9633 - val_loss: 0.1708\n",
      "Epoch 27/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9731 - loss: 0.1152\n",
      "Epoch 27: val_accuracy did not improve from 0.96833\n",
      "\n",
      "Epoch 27: val_accuracy did not improve from 0.96833\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9726 - loss: 0.1157 - val_accuracy: 0.9632 - val_loss: 0.1912\n",
      "\n",
      "Training history saved to: MLP_Models\\Base_MLP_Model_2-lr_0.01-bs_64-20250917-075846\\training_history.pkl\n",
      "\n",
      "--- Peak Performance Summary ---\n",
      "Best validation accuracy:           0.9683\n",
      "Associated training accuracy:       0.9670\n",
      "Occurred at epoch:                  17\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch/accuracy</td><td>▁▅▅▅▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇█▇▇█████</td></tr><tr><td>epoch/epoch</td><td>▁▁▂▂▂▂▃▃▃▃▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇██</td></tr><tr><td>epoch/learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/loss</td><td>█▅▄▄▃▃▃▂▂▂▃▂▂▂▂▂▂▂▂▁▂▂▁▂▁▁▁</td></tr><tr><td>epoch/val_accuracy</td><td>▁▂▄▄▅▆▆▇▇▆▆▇▇▆▇▆█▇█▂▇▅█▇▇▆▆</td></tr><tr><td>epoch/val_loss</td><td>▄▄▄▂▃▂▁▁▂▃▃▃▂▂▂▃▂▃▂█▃▅▂▂▃▃▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch/accuracy</td><td>0.97263</td></tr><tr><td>epoch/epoch</td><td>26</td></tr><tr><td>epoch/learning_rate</td><td>0.01</td></tr><tr><td>epoch/loss</td><td>0.11571</td></tr><tr><td>epoch/val_accuracy</td><td>0.96317</td></tr><tr><td>epoch/val_loss</td><td>0.19121</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Base_MLP_Model_2-lr_0.01-bs_64-20250917-075846</strong> at: <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/in5wgf26' target=\"_blank\">https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/in5wgf26</a><br> View project at: <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2' target=\"_blank\">https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2</a><br>Synced 5 W&B file(s), 0 media file(s), 18 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250917_075846-in5wgf26\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Adapting the normalisation layer...\n",
      "Adaptation complete.\n",
      "\n",
      "\n",
      "--- Starting Experiment: Base_MLP_Model_2 ---\n",
      "\n",
      "--- Model Architecture ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"Base_MLP_Model_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"Base_MLP_Model_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ normalization_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Normalization</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_24 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">50,240</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_25 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,320</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_26 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_27 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">650</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ normalization_6 (\u001b[38;5;33mNormalization\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m1\u001b[0m)      │             \u001b[38;5;34m3\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_6 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m784\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_24 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m50,240\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_25 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │         \u001b[38;5;34m8,320\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_26 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_27 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │           \u001b[38;5;34m650\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">67,469</span> (263.55 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m67,469\u001b[0m (263.55 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">67,466</span> (263.54 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m67,466\u001b[0m (263.54 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3</span> (16.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m3\u001b[0m (16.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Hyperparameters ---\n",
      "optimiser           : SGD\n",
      "learning_rate       : 0.001\n",
      "epochs              : 100\n",
      "batch_size          : 64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "creating run (0.0s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\TimVos\\VSC Projects\\CSE5ML\\Assessment 2\\wandb\\run-20250917_080017-khxrhawb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/khxrhawb' target=\"_blank\">Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017</a></strong> to <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2' target=\"_blank\">https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/khxrhawb' target=\"_blank\">https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/khxrhawb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m824/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.3088 - loss: 2.0911\n",
      "Epoch 1: val_accuracy improved from None to 0.77433, saving model to MLP_Models\\Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017\\best_model_epoch-01_val_acc-0.7743.keras\n",
      "\n",
      "Epoch 1: val_accuracy improved from None to 0.77433, saving model to wandb_models/Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.4971 - loss: 1.7510 - val_accuracy: 0.7743 - val_loss: 1.0537\n",
      "Epoch 2/100\n",
      "\u001b[1m810/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7772 - loss: 0.9277\n",
      "Epoch 2: val_accuracy improved from 0.77433 to 0.87000, saving model to MLP_Models\\Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017\\best_model_epoch-02_val_acc-0.8700.keras\n",
      "\n",
      "Epoch 2: val_accuracy improved from 0.77433 to 0.87000, saving model to wandb_models/Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.8025 - loss: 0.7921 - val_accuracy: 0.8700 - val_loss: 0.5301\n",
      "Epoch 3/100\n",
      "\u001b[1m836/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8490 - loss: 0.5605\n",
      "Epoch 3: val_accuracy improved from 0.87000 to 0.89150, saving model to MLP_Models\\Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017\\best_model_epoch-03_val_acc-0.8915.keras\n",
      "\n",
      "Epoch 3: val_accuracy improved from 0.87000 to 0.89150, saving model to wandb_models/Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.8558 - loss: 0.5285 - val_accuracy: 0.8915 - val_loss: 0.4015\n",
      "Epoch 4/100\n",
      "\u001b[1m836/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8724 - loss: 0.4523\n",
      "Epoch 4: val_accuracy improved from 0.89150 to 0.90417, saving model to MLP_Models\\Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017\\best_model_epoch-04_val_acc-0.9042.keras\n",
      "\n",
      "Epoch 4: val_accuracy improved from 0.89150 to 0.90417, saving model to wandb_models/Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.8760 - loss: 0.4393 - val_accuracy: 0.9042 - val_loss: 0.3461\n",
      "Epoch 5/100\n",
      "\u001b[1m808/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8862 - loss: 0.3994\n",
      "Epoch 5: val_accuracy improved from 0.90417 to 0.91083, saving model to MLP_Models\\Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017\\best_model_epoch-05_val_acc-0.9108.keras\n",
      "\n",
      "Epoch 5: val_accuracy improved from 0.90417 to 0.91083, saving model to wandb_models/Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.8885 - loss: 0.3925 - val_accuracy: 0.9108 - val_loss: 0.3140\n",
      "Epoch 6/100\n",
      "\u001b[1m825/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8961 - loss: 0.3659\n",
      "Epoch 6: val_accuracy improved from 0.91083 to 0.91500, saving model to MLP_Models\\Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017\\best_model_epoch-06_val_acc-0.9150.keras\n",
      "\n",
      "Epoch 6: val_accuracy improved from 0.91083 to 0.91500, saving model to wandb_models/Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.8973 - loss: 0.3620 - val_accuracy: 0.9150 - val_loss: 0.2922\n",
      "Epoch 7/100\n",
      "\u001b[1m824/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9032 - loss: 0.3419\n",
      "Epoch 7: val_accuracy improved from 0.91500 to 0.92050, saving model to MLP_Models\\Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017\\best_model_epoch-07_val_acc-0.9205.keras\n",
      "\n",
      "Epoch 7: val_accuracy improved from 0.91500 to 0.92050, saving model to wandb_models/Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9036 - loss: 0.3396 - val_accuracy: 0.9205 - val_loss: 0.2759\n",
      "Epoch 8/100\n",
      "\u001b[1m823/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9078 - loss: 0.3233\n",
      "Epoch 8: val_accuracy improved from 0.92050 to 0.92350, saving model to MLP_Models\\Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017\\best_model_epoch-08_val_acc-0.9235.keras\n",
      "\n",
      "Epoch 8: val_accuracy improved from 0.92050 to 0.92350, saving model to wandb_models/Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9079 - loss: 0.3220 - val_accuracy: 0.9235 - val_loss: 0.2630\n",
      "Epoch 9/100\n",
      "\u001b[1m829/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9126 - loss: 0.3081\n",
      "Epoch 9: val_accuracy improved from 0.92350 to 0.92667, saving model to MLP_Models\\Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017\\best_model_epoch-09_val_acc-0.9267.keras\n",
      "\n",
      "Epoch 9: val_accuracy improved from 0.92350 to 0.92667, saving model to wandb_models/Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9119 - loss: 0.3075 - val_accuracy: 0.9267 - val_loss: 0.2522\n",
      "Epoch 10/100\n",
      "\u001b[1m821/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9155 - loss: 0.2954\n",
      "Epoch 10: val_accuracy improved from 0.92667 to 0.92950, saving model to MLP_Models\\Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017\\best_model_epoch-10_val_acc-0.9295.keras\n",
      "\n",
      "Epoch 10: val_accuracy improved from 0.92667 to 0.92950, saving model to wandb_models/Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9150 - loss: 0.2953 - val_accuracy: 0.9295 - val_loss: 0.2430\n",
      "Epoch 11/100\n",
      "\u001b[1m814/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9184 - loss: 0.2844\n",
      "Epoch 11: val_accuracy improved from 0.92950 to 0.93150, saving model to MLP_Models\\Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017\\best_model_epoch-11_val_acc-0.9315.keras\n",
      "\n",
      "Epoch 11: val_accuracy improved from 0.92950 to 0.93150, saving model to wandb_models/Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9180 - loss: 0.2846 - val_accuracy: 0.9315 - val_loss: 0.2350\n",
      "Epoch 12/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9212 - loss: 0.2747\n",
      "Epoch 12: val_accuracy improved from 0.93150 to 0.93250, saving model to MLP_Models\\Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017\\best_model_epoch-12_val_acc-0.9325.keras\n",
      "\n",
      "Epoch 12: val_accuracy improved from 0.93150 to 0.93250, saving model to wandb_models/Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9203 - loss: 0.2751 - val_accuracy: 0.9325 - val_loss: 0.2279\n",
      "Epoch 13/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9237 - loss: 0.2660\n",
      "Epoch 13: val_accuracy improved from 0.93250 to 0.93500, saving model to MLP_Models\\Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017\\best_model_epoch-13_val_acc-0.9350.keras\n",
      "\n",
      "Epoch 13: val_accuracy improved from 0.93250 to 0.93500, saving model to wandb_models/Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9229 - loss: 0.2665 - val_accuracy: 0.9350 - val_loss: 0.2215\n",
      "Epoch 14/100\n",
      "\u001b[1m824/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9263 - loss: 0.2580\n",
      "Epoch 14: val_accuracy improved from 0.93500 to 0.93617, saving model to MLP_Models\\Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017\\best_model_epoch-14_val_acc-0.9362.keras\n",
      "\n",
      "Epoch 14: val_accuracy improved from 0.93500 to 0.93617, saving model to wandb_models/Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9250 - loss: 0.2586 - val_accuracy: 0.9362 - val_loss: 0.2156\n",
      "Epoch 15/100\n",
      "\u001b[1m838/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9285 - loss: 0.2508\n",
      "Epoch 15: val_accuracy improved from 0.93617 to 0.93767, saving model to MLP_Models\\Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017\\best_model_epoch-15_val_acc-0.9377.keras\n",
      "\n",
      "Epoch 15: val_accuracy improved from 0.93617 to 0.93767, saving model to wandb_models/Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9271 - loss: 0.2514 - val_accuracy: 0.9377 - val_loss: 0.2102\n",
      "Epoch 16/100\n",
      "\u001b[1m837/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9304 - loss: 0.2441\n",
      "Epoch 16: val_accuracy improved from 0.93767 to 0.93950, saving model to MLP_Models\\Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017\\best_model_epoch-16_val_acc-0.9395.keras\n",
      "\n",
      "Epoch 16: val_accuracy improved from 0.93767 to 0.93950, saving model to wandb_models/Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9291 - loss: 0.2447 - val_accuracy: 0.9395 - val_loss: 0.2052\n",
      "Epoch 17/100\n",
      "\u001b[1m821/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9327 - loss: 0.2378\n",
      "Epoch 17: val_accuracy improved from 0.93950 to 0.94250, saving model to MLP_Models\\Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017\\best_model_epoch-17_val_acc-0.9425.keras\n",
      "\n",
      "Epoch 17: val_accuracy improved from 0.93950 to 0.94250, saving model to wandb_models/Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9313 - loss: 0.2384 - val_accuracy: 0.9425 - val_loss: 0.2006\n",
      "Epoch 18/100\n",
      "\u001b[1m824/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9339 - loss: 0.2320\n",
      "Epoch 18: val_accuracy improved from 0.94250 to 0.94367, saving model to MLP_Models\\Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017\\best_model_epoch-18_val_acc-0.9437.keras\n",
      "\n",
      "Epoch 18: val_accuracy improved from 0.94250 to 0.94367, saving model to wandb_models/Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9328 - loss: 0.2326 - val_accuracy: 0.9437 - val_loss: 0.1962\n",
      "Epoch 19/100\n",
      "\u001b[1m838/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9354 - loss: 0.2265\n",
      "Epoch 19: val_accuracy improved from 0.94367 to 0.94483, saving model to MLP_Models\\Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017\\best_model_epoch-19_val_acc-0.9448.keras\n",
      "\n",
      "Epoch 19: val_accuracy improved from 0.94367 to 0.94483, saving model to wandb_models/Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9344 - loss: 0.2271 - val_accuracy: 0.9448 - val_loss: 0.1921\n",
      "Epoch 20/100\n",
      "\u001b[1m820/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9370 - loss: 0.2213\n",
      "Epoch 20: val_accuracy improved from 0.94483 to 0.94583, saving model to MLP_Models\\Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017\\best_model_epoch-20_val_acc-0.9458.keras\n",
      "\n",
      "Epoch 20: val_accuracy improved from 0.94483 to 0.94583, saving model to wandb_models/Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9359 - loss: 0.2219 - val_accuracy: 0.9458 - val_loss: 0.1883\n",
      "Epoch 21/100\n",
      "\u001b[1m821/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9382 - loss: 0.2164\n",
      "Epoch 21: val_accuracy improved from 0.94583 to 0.94800, saving model to MLP_Models\\Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017\\best_model_epoch-21_val_acc-0.9480.keras\n",
      "\n",
      "Epoch 21: val_accuracy improved from 0.94583 to 0.94800, saving model to wandb_models/Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9371 - loss: 0.2170 - val_accuracy: 0.9480 - val_loss: 0.1847\n",
      "Epoch 22/100\n",
      "\u001b[1m813/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9396 - loss: 0.2117\n",
      "Epoch 22: val_accuracy improved from 0.94800 to 0.94950, saving model to MLP_Models\\Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017\\best_model_epoch-22_val_acc-0.9495.keras\n",
      "\n",
      "Epoch 22: val_accuracy improved from 0.94800 to 0.94950, saving model to wandb_models/Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9385 - loss: 0.2124 - val_accuracy: 0.9495 - val_loss: 0.1813\n",
      "Epoch 23/100\n",
      "\u001b[1m827/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9409 - loss: 0.2073\n",
      "Epoch 23: val_accuracy improved from 0.94950 to 0.95067, saving model to MLP_Models\\Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017\\best_model_epoch-23_val_acc-0.9507.keras\n",
      "\n",
      "Epoch 23: val_accuracy improved from 0.94950 to 0.95067, saving model to wandb_models/Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9399 - loss: 0.2080 - val_accuracy: 0.9507 - val_loss: 0.1780\n",
      "Epoch 24/100\n",
      "\u001b[1m821/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9419 - loss: 0.2030\n",
      "Epoch 24: val_accuracy improved from 0.95067 to 0.95183, saving model to MLP_Models\\Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017\\best_model_epoch-24_val_acc-0.9518.keras\n",
      "\n",
      "Epoch 24: val_accuracy improved from 0.95067 to 0.95183, saving model to wandb_models/Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9411 - loss: 0.2037 - val_accuracy: 0.9518 - val_loss: 0.1750\n",
      "Epoch 25/100\n",
      "\u001b[1m835/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9430 - loss: 0.1990\n",
      "Epoch 25: val_accuracy improved from 0.95183 to 0.95283, saving model to MLP_Models\\Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017\\best_model_epoch-25_val_acc-0.9528.keras\n",
      "\n",
      "Epoch 25: val_accuracy improved from 0.95183 to 0.95283, saving model to wandb_models/Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9422 - loss: 0.1997 - val_accuracy: 0.9528 - val_loss: 0.1721\n",
      "Epoch 26/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9439 - loss: 0.1951\n",
      "Epoch 26: val_accuracy improved from 0.95283 to 0.95350, saving model to MLP_Models\\Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017\\best_model_epoch-26_val_acc-0.9535.keras\n",
      "\n",
      "Epoch 26: val_accuracy improved from 0.95283 to 0.95350, saving model to wandb_models/Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9432 - loss: 0.1958 - val_accuracy: 0.9535 - val_loss: 0.1694\n",
      "Epoch 27/100\n",
      "\u001b[1m818/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9453 - loss: 0.1914\n",
      "Epoch 27: val_accuracy improved from 0.95350 to 0.95367, saving model to MLP_Models\\Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017\\best_model_epoch-27_val_acc-0.9537.keras\n",
      "\n",
      "Epoch 27: val_accuracy improved from 0.95350 to 0.95367, saving model to wandb_models/Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9443 - loss: 0.1920 - val_accuracy: 0.9537 - val_loss: 0.1667\n",
      "Epoch 28/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9463 - loss: 0.1878\n",
      "Epoch 28: val_accuracy improved from 0.95367 to 0.95417, saving model to MLP_Models\\Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017\\best_model_epoch-28_val_acc-0.9542.keras\n",
      "\n",
      "Epoch 28: val_accuracy improved from 0.95367 to 0.95417, saving model to wandb_models/Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9454 - loss: 0.1884 - val_accuracy: 0.9542 - val_loss: 0.1642\n",
      "Epoch 29/100\n",
      "\u001b[1m837/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9476 - loss: 0.1844\n",
      "Epoch 29: val_accuracy improved from 0.95417 to 0.95467, saving model to MLP_Models\\Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017\\best_model_epoch-29_val_acc-0.9547.keras\n",
      "\n",
      "Epoch 29: val_accuracy improved from 0.95417 to 0.95467, saving model to wandb_models/Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9467 - loss: 0.1850 - val_accuracy: 0.9547 - val_loss: 0.1618\n",
      "Epoch 30/100\n",
      "\u001b[1m840/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9485 - loss: 0.1811\n",
      "Epoch 30: val_accuracy improved from 0.95467 to 0.95567, saving model to MLP_Models\\Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017\\best_model_epoch-30_val_acc-0.9557.keras\n",
      "\n",
      "Epoch 30: val_accuracy improved from 0.95467 to 0.95567, saving model to wandb_models/Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9476 - loss: 0.1816 - val_accuracy: 0.9557 - val_loss: 0.1596\n",
      "Epoch 31/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9493 - loss: 0.1779\n",
      "Epoch 31: val_accuracy improved from 0.95567 to 0.95617, saving model to MLP_Models\\Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017\\best_model_epoch-31_val_acc-0.9562.keras\n",
      "\n",
      "Epoch 31: val_accuracy improved from 0.95567 to 0.95617, saving model to wandb_models/Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9488 - loss: 0.1784 - val_accuracy: 0.9562 - val_loss: 0.1574\n",
      "Epoch 32/100\n",
      "\u001b[1m815/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9502 - loss: 0.1748\n",
      "Epoch 32: val_accuracy improved from 0.95617 to 0.95683, saving model to MLP_Models\\Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017\\best_model_epoch-32_val_acc-0.9568.keras\n",
      "\n",
      "Epoch 32: val_accuracy improved from 0.95617 to 0.95683, saving model to wandb_models/Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9495 - loss: 0.1754 - val_accuracy: 0.9568 - val_loss: 0.1554\n",
      "Epoch 33/100\n",
      "\u001b[1m824/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9511 - loss: 0.1719\n",
      "Epoch 33: val_accuracy improved from 0.95683 to 0.95717, saving model to MLP_Models\\Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017\\best_model_epoch-33_val_acc-0.9572.keras\n",
      "\n",
      "Epoch 33: val_accuracy improved from 0.95683 to 0.95717, saving model to wandb_models/Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9503 - loss: 0.1724 - val_accuracy: 0.9572 - val_loss: 0.1534\n",
      "Epoch 34/100\n",
      "\u001b[1m807/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9519 - loss: 0.1691\n",
      "Epoch 34: val_accuracy improved from 0.95717 to 0.95733, saving model to MLP_Models\\Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017\\best_model_epoch-34_val_acc-0.9573.keras\n",
      "\n",
      "Epoch 34: val_accuracy improved from 0.95717 to 0.95733, saving model to wandb_models/Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9511 - loss: 0.1695 - val_accuracy: 0.9573 - val_loss: 0.1515\n",
      "Epoch 35/100\n",
      "\u001b[1m839/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9523 - loss: 0.1663\n",
      "Epoch 35: val_accuracy improved from 0.95733 to 0.95783, saving model to MLP_Models\\Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017\\best_model_epoch-35_val_acc-0.9578.keras\n",
      "\n",
      "Epoch 35: val_accuracy improved from 0.95733 to 0.95783, saving model to wandb_models/Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9518 - loss: 0.1667 - val_accuracy: 0.9578 - val_loss: 0.1497\n",
      "Epoch 36/100\n",
      "\u001b[1m827/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9531 - loss: 0.1637\n",
      "Epoch 36: val_accuracy improved from 0.95783 to 0.95817, saving model to MLP_Models\\Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017\\best_model_epoch-36_val_acc-0.9582.keras\n",
      "\n",
      "Epoch 36: val_accuracy improved from 0.95783 to 0.95817, saving model to wandb_models/Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9525 - loss: 0.1640 - val_accuracy: 0.9582 - val_loss: 0.1480\n",
      "Epoch 37/100\n",
      "\u001b[1m814/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9540 - loss: 0.1611\n",
      "Epoch 37: val_accuracy improved from 0.95817 to 0.95867, saving model to MLP_Models\\Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017\\best_model_epoch-37_val_acc-0.9587.keras\n",
      "\n",
      "Epoch 37: val_accuracy improved from 0.95817 to 0.95867, saving model to wandb_models/Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9534 - loss: 0.1615 - val_accuracy: 0.9587 - val_loss: 0.1463\n",
      "Epoch 38/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9546 - loss: 0.1586\n",
      "Epoch 38: val_accuracy improved from 0.95867 to 0.95900, saving model to MLP_Models\\Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017\\best_model_epoch-38_val_acc-0.9590.keras\n",
      "\n",
      "Epoch 38: val_accuracy improved from 0.95867 to 0.95900, saving model to wandb_models/Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9538 - loss: 0.1590 - val_accuracy: 0.9590 - val_loss: 0.1447\n",
      "Epoch 39/100\n",
      "\u001b[1m824/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9551 - loss: 0.1562\n",
      "Epoch 39: val_accuracy improved from 0.95900 to 0.95950, saving model to MLP_Models\\Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017\\best_model_epoch-39_val_acc-0.9595.keras\n",
      "\n",
      "Epoch 39: val_accuracy improved from 0.95900 to 0.95950, saving model to wandb_models/Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9545 - loss: 0.1566 - val_accuracy: 0.9595 - val_loss: 0.1432\n",
      "Epoch 40/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9557 - loss: 0.1539\n",
      "Epoch 40: val_accuracy improved from 0.95950 to 0.96033, saving model to MLP_Models\\Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017\\best_model_epoch-40_val_acc-0.9603.keras\n",
      "\n",
      "Epoch 40: val_accuracy improved from 0.95950 to 0.96033, saving model to wandb_models/Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9550 - loss: 0.1542 - val_accuracy: 0.9603 - val_loss: 0.1417\n",
      "Epoch 41/100\n",
      "\u001b[1m835/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9564 - loss: 0.1516\n",
      "Epoch 41: val_accuracy improved from 0.96033 to 0.96050, saving model to MLP_Models\\Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017\\best_model_epoch-41_val_acc-0.9605.keras\n",
      "\n",
      "Epoch 41: val_accuracy improved from 0.96033 to 0.96050, saving model to wandb_models/Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9556 - loss: 0.1519 - val_accuracy: 0.9605 - val_loss: 0.1403\n",
      "Epoch 42/100\n",
      "\u001b[1m826/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9571 - loss: 0.1494\n",
      "Epoch 42: val_accuracy improved from 0.96050 to 0.96117, saving model to MLP_Models\\Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017\\best_model_epoch-42_val_acc-0.9612.keras\n",
      "\n",
      "Epoch 42: val_accuracy improved from 0.96050 to 0.96117, saving model to wandb_models/Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.9563 - loss: 0.1497 - val_accuracy: 0.9612 - val_loss: 0.1389\n",
      "Epoch 43/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9578 - loss: 0.1472\n",
      "Epoch 43: val_accuracy improved from 0.96117 to 0.96167, saving model to MLP_Models\\Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017\\best_model_epoch-43_val_acc-0.9617.keras\n",
      "\n",
      "Epoch 43: val_accuracy improved from 0.96117 to 0.96167, saving model to wandb_models/Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9570 - loss: 0.1476 - val_accuracy: 0.9617 - val_loss: 0.1375\n",
      "Epoch 44/100\n",
      "\u001b[1m822/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9584 - loss: 0.1452\n",
      "Epoch 44: val_accuracy did not improve from 0.96167\n",
      "\n",
      "Epoch 44: val_accuracy did not improve from 0.96167\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9578 - loss: 0.1455 - val_accuracy: 0.9617 - val_loss: 0.1362\n",
      "Epoch 45/100\n",
      "\u001b[1m826/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9590 - loss: 0.1431\n",
      "Epoch 45: val_accuracy improved from 0.96167 to 0.96283, saving model to MLP_Models\\Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017\\best_model_epoch-45_val_acc-0.9628.keras\n",
      "\n",
      "Epoch 45: val_accuracy improved from 0.96167 to 0.96283, saving model to wandb_models/Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9585 - loss: 0.1435 - val_accuracy: 0.9628 - val_loss: 0.1349\n",
      "Epoch 46/100\n",
      "\u001b[1m823/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9596 - loss: 0.1412\n",
      "Epoch 46: val_accuracy improved from 0.96283 to 0.96300, saving model to MLP_Models\\Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017\\best_model_epoch-46_val_acc-0.9630.keras\n",
      "\n",
      "Epoch 46: val_accuracy improved from 0.96283 to 0.96300, saving model to wandb_models/Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9590 - loss: 0.1416 - val_accuracy: 0.9630 - val_loss: 0.1337\n",
      "Epoch 47/100\n",
      "\u001b[1m826/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9599 - loss: 0.1392\n",
      "Epoch 47: val_accuracy improved from 0.96300 to 0.96317, saving model to MLP_Models\\Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017\\best_model_epoch-47_val_acc-0.9632.keras\n",
      "\n",
      "Epoch 47: val_accuracy improved from 0.96300 to 0.96317, saving model to wandb_models/Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9596 - loss: 0.1396 - val_accuracy: 0.9632 - val_loss: 0.1325\n",
      "Epoch 48/100\n",
      "\u001b[1m828/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9604 - loss: 0.1374\n",
      "Epoch 48: val_accuracy improved from 0.96317 to 0.96333, saving model to MLP_Models\\Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017\\best_model_epoch-48_val_acc-0.9633.keras\n",
      "\n",
      "Epoch 48: val_accuracy improved from 0.96317 to 0.96333, saving model to wandb_models/Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9601 - loss: 0.1378 - val_accuracy: 0.9633 - val_loss: 0.1314\n",
      "Epoch 49/100\n",
      "\u001b[1m815/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9611 - loss: 0.1355\n",
      "Epoch 49: val_accuracy did not improve from 0.96333\n",
      "\n",
      "Epoch 49: val_accuracy did not improve from 0.96333\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9608 - loss: 0.1360 - val_accuracy: 0.9633 - val_loss: 0.1303\n",
      "Epoch 50/100\n",
      "\u001b[1m826/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9620 - loss: 0.1338\n",
      "Epoch 50: val_accuracy improved from 0.96333 to 0.96367, saving model to MLP_Models\\Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017\\best_model_epoch-50_val_acc-0.9637.keras\n",
      "\n",
      "Epoch 50: val_accuracy improved from 0.96333 to 0.96367, saving model to wandb_models/Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.9615 - loss: 0.1342 - val_accuracy: 0.9637 - val_loss: 0.1292\n",
      "Epoch 51/100\n",
      "\u001b[1m838/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9623 - loss: 0.1321\n",
      "Epoch 51: val_accuracy improved from 0.96367 to 0.96417, saving model to MLP_Models\\Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017\\best_model_epoch-51_val_acc-0.9642.keras\n",
      "\n",
      "Epoch 51: val_accuracy improved from 0.96367 to 0.96417, saving model to wandb_models/Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9618 - loss: 0.1325 - val_accuracy: 0.9642 - val_loss: 0.1282\n",
      "Epoch 52/100\n",
      "\u001b[1m832/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9626 - loss: 0.1304\n",
      "Epoch 52: val_accuracy improved from 0.96417 to 0.96433, saving model to MLP_Models\\Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017\\best_model_epoch-52_val_acc-0.9643.keras\n",
      "\n",
      "Epoch 52: val_accuracy improved from 0.96417 to 0.96433, saving model to wandb_models/Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9621 - loss: 0.1308 - val_accuracy: 0.9643 - val_loss: 0.1272\n",
      "Epoch 53/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9631 - loss: 0.1288\n",
      "Epoch 53: val_accuracy improved from 0.96433 to 0.96483, saving model to MLP_Models\\Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017\\best_model_epoch-53_val_acc-0.9648.keras\n",
      "\n",
      "Epoch 53: val_accuracy improved from 0.96433 to 0.96483, saving model to wandb_models/Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.9626 - loss: 0.1292 - val_accuracy: 0.9648 - val_loss: 0.1263\n",
      "Epoch 54/100\n",
      "\u001b[1m831/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9636 - loss: 0.1272\n",
      "Epoch 54: val_accuracy improved from 0.96483 to 0.96500, saving model to MLP_Models\\Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017\\best_model_epoch-54_val_acc-0.9650.keras\n",
      "\n",
      "Epoch 54: val_accuracy improved from 0.96483 to 0.96500, saving model to wandb_models/Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9631 - loss: 0.1276 - val_accuracy: 0.9650 - val_loss: 0.1254\n",
      "Epoch 55/100\n",
      "\u001b[1m827/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9640 - loss: 0.1256\n",
      "Epoch 55: val_accuracy improved from 0.96500 to 0.96517, saving model to MLP_Models\\Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017\\best_model_epoch-55_val_acc-0.9652.keras\n",
      "\n",
      "Epoch 55: val_accuracy improved from 0.96500 to 0.96517, saving model to wandb_models/Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9635 - loss: 0.1261 - val_accuracy: 0.9652 - val_loss: 0.1245\n",
      "Epoch 56/100\n",
      "\u001b[1m836/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9643 - loss: 0.1241\n",
      "Epoch 56: val_accuracy improved from 0.96517 to 0.96550, saving model to MLP_Models\\Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017\\best_model_epoch-56_val_acc-0.9655.keras\n",
      "\n",
      "Epoch 56: val_accuracy improved from 0.96517 to 0.96550, saving model to wandb_models/Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9639 - loss: 0.1246 - val_accuracy: 0.9655 - val_loss: 0.1237\n",
      "Epoch 57/100\n",
      "\u001b[1m837/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9646 - loss: 0.1226\n",
      "Epoch 57: val_accuracy improved from 0.96550 to 0.96567, saving model to MLP_Models\\Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017\\best_model_epoch-57_val_acc-0.9657.keras\n",
      "\n",
      "Epoch 57: val_accuracy improved from 0.96550 to 0.96567, saving model to wandb_models/Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9641 - loss: 0.1231 - val_accuracy: 0.9657 - val_loss: 0.1228\n",
      "Epoch 58/100\n",
      "\u001b[1m823/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9650 - loss: 0.1212\n",
      "Epoch 58: val_accuracy improved from 0.96567 to 0.96650, saving model to MLP_Models\\Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017\\best_model_epoch-58_val_acc-0.9665.keras\n",
      "\n",
      "Epoch 58: val_accuracy improved from 0.96567 to 0.96650, saving model to wandb_models/Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9644 - loss: 0.1217 - val_accuracy: 0.9665 - val_loss: 0.1220\n",
      "Epoch 59/100\n",
      "\u001b[1m823/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9654 - loss: 0.1198\n",
      "Epoch 59: val_accuracy improved from 0.96650 to 0.96683, saving model to MLP_Models\\Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017\\best_model_epoch-59_val_acc-0.9668.keras\n",
      "\n",
      "Epoch 59: val_accuracy improved from 0.96650 to 0.96683, saving model to wandb_models/Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9650 - loss: 0.1203 - val_accuracy: 0.9668 - val_loss: 0.1212\n",
      "Epoch 60/100\n",
      "\u001b[1m822/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9660 - loss: 0.1184\n",
      "Epoch 60: val_accuracy improved from 0.96683 to 0.96717, saving model to MLP_Models\\Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017\\best_model_epoch-60_val_acc-0.9672.keras\n",
      "\n",
      "Epoch 60: val_accuracy improved from 0.96683 to 0.96717, saving model to wandb_models/Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.9654 - loss: 0.1189 - val_accuracy: 0.9672 - val_loss: 0.1205\n",
      "Epoch 61/100\n",
      "\u001b[1m825/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9664 - loss: 0.1171\n",
      "Epoch 61: val_accuracy improved from 0.96717 to 0.96767, saving model to MLP_Models\\Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017\\best_model_epoch-61_val_acc-0.9677.keras\n",
      "\n",
      "Epoch 61: val_accuracy improved from 0.96717 to 0.96767, saving model to wandb_models/Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.9659 - loss: 0.1176 - val_accuracy: 0.9677 - val_loss: 0.1197\n",
      "Epoch 62/100\n",
      "\u001b[1m834/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9666 - loss: 0.1158\n",
      "Epoch 62: val_accuracy did not improve from 0.96767\n",
      "\n",
      "Epoch 62: val_accuracy did not improve from 0.96767\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.9662 - loss: 0.1163 - val_accuracy: 0.9677 - val_loss: 0.1190\n",
      "Epoch 63/100\n",
      "\u001b[1m834/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9670 - loss: 0.1145\n",
      "Epoch 63: val_accuracy improved from 0.96767 to 0.96783, saving model to MLP_Models\\Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017\\best_model_epoch-63_val_acc-0.9678.keras\n",
      "\n",
      "Epoch 63: val_accuracy improved from 0.96767 to 0.96783, saving model to wandb_models/Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9666 - loss: 0.1150 - val_accuracy: 0.9678 - val_loss: 0.1182\n",
      "Epoch 64/100\n",
      "\u001b[1m832/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9672 - loss: 0.1133\n",
      "Epoch 64: val_accuracy did not improve from 0.96783\n",
      "\n",
      "Epoch 64: val_accuracy did not improve from 0.96783\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.9669 - loss: 0.1137 - val_accuracy: 0.9677 - val_loss: 0.1176\n",
      "Epoch 65/100\n",
      "\u001b[1m834/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9675 - loss: 0.1121\n",
      "Epoch 65: val_accuracy improved from 0.96783 to 0.96800, saving model to MLP_Models\\Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017\\best_model_epoch-65_val_acc-0.9680.keras\n",
      "\n",
      "Epoch 65: val_accuracy improved from 0.96783 to 0.96800, saving model to wandb_models/Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9673 - loss: 0.1125 - val_accuracy: 0.9680 - val_loss: 0.1169\n",
      "Epoch 66/100\n",
      "\u001b[1m819/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9679 - loss: 0.1108\n",
      "Epoch 66: val_accuracy did not improve from 0.96800\n",
      "\n",
      "Epoch 66: val_accuracy did not improve from 0.96800\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.9676 - loss: 0.1113 - val_accuracy: 0.9680 - val_loss: 0.1162\n",
      "Epoch 67/100\n",
      "\u001b[1m835/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9683 - loss: 0.1097\n",
      "Epoch 67: val_accuracy did not improve from 0.96800\n",
      "\n",
      "Epoch 67: val_accuracy did not improve from 0.96800\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.9680 - loss: 0.1101 - val_accuracy: 0.9680 - val_loss: 0.1156\n",
      "Epoch 68/100\n",
      "\u001b[1m825/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9688 - loss: 0.1085\n",
      "Epoch 68: val_accuracy did not improve from 0.96800\n",
      "\n",
      "Epoch 68: val_accuracy did not improve from 0.96800\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9684 - loss: 0.1090 - val_accuracy: 0.9678 - val_loss: 0.1149\n",
      "Epoch 69/100\n",
      "\u001b[1m830/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9694 - loss: 0.1074\n",
      "Epoch 69: val_accuracy did not improve from 0.96800\n",
      "\n",
      "Epoch 69: val_accuracy did not improve from 0.96800\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9689 - loss: 0.1078 - val_accuracy: 0.9678 - val_loss: 0.1143\n",
      "Epoch 70/100\n",
      "\u001b[1m834/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9698 - loss: 0.1063\n",
      "Epoch 70: val_accuracy did not improve from 0.96800\n",
      "\n",
      "Epoch 70: val_accuracy did not improve from 0.96800\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.9693 - loss: 0.1067 - val_accuracy: 0.9677 - val_loss: 0.1138\n",
      "Epoch 71/100\n",
      "\u001b[1m823/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9700 - loss: 0.1052\n",
      "Epoch 71: val_accuracy did not improve from 0.96800\n",
      "\n",
      "Epoch 71: val_accuracy did not improve from 0.96800\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9696 - loss: 0.1056 - val_accuracy: 0.9680 - val_loss: 0.1132\n",
      "Epoch 72/100\n",
      "\u001b[1m815/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9705 - loss: 0.1041\n",
      "Epoch 72: val_accuracy improved from 0.96800 to 0.96833, saving model to MLP_Models\\Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017\\best_model_epoch-72_val_acc-0.9683.keras\n",
      "\n",
      "Epoch 72: val_accuracy improved from 0.96800 to 0.96833, saving model to wandb_models/Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9699 - loss: 0.1046 - val_accuracy: 0.9683 - val_loss: 0.1126\n",
      "Epoch 73/100\n",
      "\u001b[1m808/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9710 - loss: 0.1031\n",
      "Epoch 73: val_accuracy improved from 0.96833 to 0.96867, saving model to MLP_Models\\Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017\\best_model_epoch-73_val_acc-0.9687.keras\n",
      "\n",
      "Epoch 73: val_accuracy improved from 0.96833 to 0.96867, saving model to wandb_models/Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9702 - loss: 0.1035 - val_accuracy: 0.9687 - val_loss: 0.1121\n",
      "Epoch 74/100\n",
      "\u001b[1m820/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9712 - loss: 0.1021\n",
      "Epoch 74: val_accuracy improved from 0.96867 to 0.96900, saving model to MLP_Models\\Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017\\best_model_epoch-74_val_acc-0.9690.keras\n",
      "\n",
      "Epoch 74: val_accuracy improved from 0.96867 to 0.96900, saving model to wandb_models/Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9705 - loss: 0.1025 - val_accuracy: 0.9690 - val_loss: 0.1115\n",
      "Epoch 75/100\n",
      "\u001b[1m823/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9714 - loss: 0.1010\n",
      "Epoch 75: val_accuracy improved from 0.96900 to 0.96933, saving model to MLP_Models\\Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017\\best_model_epoch-75_val_acc-0.9693.keras\n",
      "\n",
      "Epoch 75: val_accuracy improved from 0.96900 to 0.96933, saving model to wandb_models/Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9708 - loss: 0.1015 - val_accuracy: 0.9693 - val_loss: 0.1110\n",
      "Epoch 76/100\n",
      "\u001b[1m809/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9719 - loss: 0.1000\n",
      "Epoch 76: val_accuracy improved from 0.96933 to 0.96950, saving model to MLP_Models\\Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017\\best_model_epoch-76_val_acc-0.9695.keras\n",
      "\n",
      "Epoch 76: val_accuracy improved from 0.96933 to 0.96950, saving model to wandb_models/Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9712 - loss: 0.1005 - val_accuracy: 0.9695 - val_loss: 0.1105\n",
      "Epoch 77/100\n",
      "\u001b[1m831/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9722 - loss: 0.0991\n",
      "Epoch 77: val_accuracy improved from 0.96950 to 0.96983, saving model to MLP_Models\\Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017\\best_model_epoch-77_val_acc-0.9698.keras\n",
      "\n",
      "Epoch 77: val_accuracy improved from 0.96950 to 0.96983, saving model to wandb_models/Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9716 - loss: 0.0995 - val_accuracy: 0.9698 - val_loss: 0.1100\n",
      "Epoch 78/100\n",
      "\u001b[1m834/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9725 - loss: 0.0981\n",
      "Epoch 78: val_accuracy did not improve from 0.96983\n",
      "\n",
      "Epoch 78: val_accuracy did not improve from 0.96983\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9719 - loss: 0.0985 - val_accuracy: 0.9698 - val_loss: 0.1095\n",
      "Epoch 79/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9728 - loss: 0.0971\n",
      "Epoch 79: val_accuracy did not improve from 0.96983\n",
      "\n",
      "Epoch 79: val_accuracy did not improve from 0.96983\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9721 - loss: 0.0975 - val_accuracy: 0.9698 - val_loss: 0.1090\n",
      "Epoch 80/100\n",
      "\u001b[1m830/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9732 - loss: 0.0962\n",
      "Epoch 80: val_accuracy improved from 0.96983 to 0.97000, saving model to MLP_Models\\Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017\\best_model_epoch-80_val_acc-0.9700.keras\n",
      "\n",
      "Epoch 80: val_accuracy improved from 0.96983 to 0.97000, saving model to wandb_models/Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9724 - loss: 0.0966 - val_accuracy: 0.9700 - val_loss: 0.1086\n",
      "Epoch 81/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9734 - loss: 0.0953\n",
      "Epoch 81: val_accuracy did not improve from 0.97000\n",
      "\n",
      "Epoch 81: val_accuracy did not improve from 0.97000\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9727 - loss: 0.0957 - val_accuracy: 0.9700 - val_loss: 0.1081\n",
      "Epoch 82/100\n",
      "\u001b[1m833/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9737 - loss: 0.0944\n",
      "Epoch 82: val_accuracy did not improve from 0.97000\n",
      "\n",
      "Epoch 82: val_accuracy did not improve from 0.97000\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9729 - loss: 0.0948 - val_accuracy: 0.9700 - val_loss: 0.1077\n",
      "Epoch 83/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9739 - loss: 0.0935\n",
      "Epoch 83: val_accuracy improved from 0.97000 to 0.97017, saving model to MLP_Models\\Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017\\best_model_epoch-83_val_acc-0.9702.keras\n",
      "\n",
      "Epoch 83: val_accuracy improved from 0.97000 to 0.97017, saving model to wandb_models/Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9732 - loss: 0.0939 - val_accuracy: 0.9702 - val_loss: 0.1073\n",
      "Epoch 84/100\n",
      "\u001b[1m838/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9741 - loss: 0.0926\n",
      "Epoch 84: val_accuracy improved from 0.97017 to 0.97033, saving model to MLP_Models\\Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017\\best_model_epoch-84_val_acc-0.9703.keras\n",
      "\n",
      "Epoch 84: val_accuracy improved from 0.97017 to 0.97033, saving model to wandb_models/Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9734 - loss: 0.0930 - val_accuracy: 0.9703 - val_loss: 0.1068\n",
      "Epoch 85/100\n",
      "\u001b[1m833/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9744 - loss: 0.0918\n",
      "Epoch 85: val_accuracy improved from 0.97033 to 0.97050, saving model to MLP_Models\\Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017\\best_model_epoch-85_val_acc-0.9705.keras\n",
      "\n",
      "Epoch 85: val_accuracy improved from 0.97033 to 0.97050, saving model to wandb_models/Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.9737 - loss: 0.0922 - val_accuracy: 0.9705 - val_loss: 0.1064\n",
      "Epoch 86/100\n",
      "\u001b[1m832/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9747 - loss: 0.0909\n",
      "Epoch 86: val_accuracy did not improve from 0.97050\n",
      "\n",
      "Epoch 86: val_accuracy did not improve from 0.97050\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9741 - loss: 0.0913 - val_accuracy: 0.9705 - val_loss: 0.1061\n",
      "Epoch 87/100\n",
      "\u001b[1m822/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9751 - loss: 0.0901\n",
      "Epoch 87: val_accuracy improved from 0.97050 to 0.97067, saving model to MLP_Models\\Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017\\best_model_epoch-87_val_acc-0.9707.keras\n",
      "\n",
      "Epoch 87: val_accuracy improved from 0.97050 to 0.97067, saving model to wandb_models/Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.9745 - loss: 0.0905 - val_accuracy: 0.9707 - val_loss: 0.1057\n",
      "Epoch 88/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9751 - loss: 0.0893\n",
      "Epoch 88: val_accuracy did not improve from 0.97067\n",
      "\n",
      "Epoch 88: val_accuracy did not improve from 0.97067\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9746 - loss: 0.0896 - val_accuracy: 0.9705 - val_loss: 0.1053\n",
      "Epoch 89/100\n",
      "\u001b[1m838/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9752 - loss: 0.0885\n",
      "Epoch 89: val_accuracy did not improve from 0.97067\n",
      "\n",
      "Epoch 89: val_accuracy did not improve from 0.97067\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.9748 - loss: 0.0888 - val_accuracy: 0.9707 - val_loss: 0.1049\n",
      "Epoch 90/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9755 - loss: 0.0877\n",
      "Epoch 90: val_accuracy did not improve from 0.97067\n",
      "\n",
      "Epoch 90: val_accuracy did not improve from 0.97067\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 8ms/step - accuracy: 0.9751 - loss: 0.0880 - val_accuracy: 0.9707 - val_loss: 0.1045\n",
      "Epoch 91/100\n",
      "\u001b[1m834/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9759 - loss: 0.0869\n",
      "Epoch 91: val_accuracy did not improve from 0.97067\n",
      "\n",
      "Epoch 91: val_accuracy did not improve from 0.97067\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9752 - loss: 0.0872 - val_accuracy: 0.9707 - val_loss: 0.1042\n",
      "Epoch 92/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9761 - loss: 0.0861\n",
      "Epoch 92: val_accuracy improved from 0.97067 to 0.97083, saving model to MLP_Models\\Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017\\best_model_epoch-92_val_acc-0.9708.keras\n",
      "\n",
      "Epoch 92: val_accuracy improved from 0.97067 to 0.97083, saving model to wandb_models/Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - accuracy: 0.9755 - loss: 0.0865 - val_accuracy: 0.9708 - val_loss: 0.1038\n",
      "Epoch 93/100\n",
      "\u001b[1m836/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9762 - loss: 0.0854\n",
      "Epoch 93: val_accuracy improved from 0.97083 to 0.97117, saving model to MLP_Models\\Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017\\best_model_epoch-93_val_acc-0.9712.keras\n",
      "\n",
      "Epoch 93: val_accuracy improved from 0.97083 to 0.97117, saving model to wandb_models/Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9756 - loss: 0.0857 - val_accuracy: 0.9712 - val_loss: 0.1035\n",
      "Epoch 94/100\n",
      "\u001b[1m835/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9767 - loss: 0.0846\n",
      "Epoch 94: val_accuracy did not improve from 0.97117\n",
      "\n",
      "Epoch 94: val_accuracy did not improve from 0.97117\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9759 - loss: 0.0850 - val_accuracy: 0.9712 - val_loss: 0.1031\n",
      "Epoch 95/100\n",
      "\u001b[1m835/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9768 - loss: 0.0839\n",
      "Epoch 95: val_accuracy improved from 0.97117 to 0.97133, saving model to MLP_Models\\Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017\\best_model_epoch-95_val_acc-0.9713.keras\n",
      "\n",
      "Epoch 95: val_accuracy improved from 0.97117 to 0.97133, saving model to wandb_models/Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9761 - loss: 0.0842 - val_accuracy: 0.9713 - val_loss: 0.1028\n",
      "Epoch 96/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9773 - loss: 0.0832\n",
      "Epoch 96: val_accuracy improved from 0.97133 to 0.97167, saving model to MLP_Models\\Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017\\best_model_epoch-96_val_acc-0.9717.keras\n",
      "\n",
      "Epoch 96: val_accuracy improved from 0.97133 to 0.97167, saving model to wandb_models/Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9764 - loss: 0.0835 - val_accuracy: 0.9717 - val_loss: 0.1025\n",
      "Epoch 97/100\n",
      "\u001b[1m830/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9774 - loss: 0.0824\n",
      "Epoch 97: val_accuracy did not improve from 0.97167\n",
      "\n",
      "Epoch 97: val_accuracy did not improve from 0.97167\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.9766 - loss: 0.0828 - val_accuracy: 0.9717 - val_loss: 0.1022\n",
      "Epoch 98/100\n",
      "\u001b[1m834/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9777 - loss: 0.0817\n",
      "Epoch 98: val_accuracy improved from 0.97167 to 0.97183, saving model to MLP_Models\\Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017\\best_model_epoch-98_val_acc-0.9718.keras\n",
      "\n",
      "Epoch 98: val_accuracy improved from 0.97167 to 0.97183, saving model to wandb_models/Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9768 - loss: 0.0820 - val_accuracy: 0.9718 - val_loss: 0.1018\n",
      "Epoch 99/100\n",
      "\u001b[1m829/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9779 - loss: 0.0810\n",
      "Epoch 99: val_accuracy did not improve from 0.97183\n",
      "\n",
      "Epoch 99: val_accuracy did not improve from 0.97183\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9770 - loss: 0.0813 - val_accuracy: 0.9718 - val_loss: 0.1015\n",
      "Epoch 100/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9781 - loss: 0.0804\n",
      "Epoch 100: val_accuracy improved from 0.97183 to 0.97200, saving model to MLP_Models\\Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017\\best_model_epoch-100_val_acc-0.9720.keras\n",
      "\n",
      "Epoch 100: val_accuracy improved from 0.97183 to 0.97200, saving model to wandb_models/Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.9771 - loss: 0.0806 - val_accuracy: 0.9720 - val_loss: 0.1012\n",
      "\n",
      "Training history saved to: MLP_Models\\Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017\\training_history.pkl\n",
      "\n",
      "--- Peak Performance Summary ---\n",
      "Best validation accuracy:           0.9720\n",
      "Associated training accuracy:       0.9771\n",
      "Occurred at epoch:                  100\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch/accuracy</td><td>▁▃▄▄▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇███████████████████</td></tr><tr><td>epoch/epoch</td><td>▁▁▂▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>epoch/learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/loss</td><td>█▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/val_accuracy</td><td>▁▄▆▆▆▆▇▇▇▇▇▇▇▇▇█████████████████████████</td></tr><tr><td>epoch/val_loss</td><td>█▆▅▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch/accuracy</td><td>0.97715</td></tr><tr><td>epoch/epoch</td><td>99</td></tr><tr><td>epoch/learning_rate</td><td>0.001</td></tr><tr><td>epoch/loss</td><td>0.08065</td></tr><tr><td>epoch/val_accuracy</td><td>0.972</td></tr><tr><td>epoch/val_loss</td><td>0.10123</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Base_MLP_Model_2-lr_0.001-bs_64-20250917-080017</strong> at: <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/khxrhawb' target=\"_blank\">https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/khxrhawb</a><br> View project at: <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2' target=\"_blank\">https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2</a><br>Synced 5 W&B file(s), 0 media file(s), 156 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250917_080017-khxrhawb\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Adapting the normalisation layer...\n",
      "Adaptation complete.\n",
      "\n",
      "\n",
      "--- Starting Experiment: Base_MLP_Model_2 ---\n",
      "\n",
      "--- Model Architecture ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"Base_MLP_Model_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"Base_MLP_Model_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ normalization_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Normalization</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_28 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">50,240</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_29 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,320</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_30 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_31 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">650</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ normalization_7 (\u001b[38;5;33mNormalization\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m1\u001b[0m)      │             \u001b[38;5;34m3\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_7 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m784\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_28 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m50,240\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_29 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │         \u001b[38;5;34m8,320\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_30 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_31 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │           \u001b[38;5;34m650\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">67,469</span> (263.55 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m67,469\u001b[0m (263.55 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">67,466</span> (263.54 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m67,466\u001b[0m (263.54 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3</span> (16.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m3\u001b[0m (16.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Hyperparameters ---\n",
      "optimiser           : SGD\n",
      "learning_rate       : 0.01\n",
      "epochs              : 100\n",
      "batch_size          : 64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "creating run (0.0s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\TimVos\\VSC Projects\\CSE5ML\\Assessment 2\\wandb\\run-20250917_080628-j8if8od3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/j8if8od3' target=\"_blank\">Base_MLP_Model_2-lr_0.01-bs_64-20250917-080628</a></strong> to <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2' target=\"_blank\">https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/j8if8od3' target=\"_blank\">https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/j8if8od3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m838/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6628 - loss: 1.0657\n",
      "Epoch 1: val_accuracy improved from None to 0.92450, saving model to MLP_Models\\Base_MLP_Model_2-lr_0.01-bs_64-20250917-080628\\best_model_epoch-01_val_acc-0.9245.keras\n",
      "\n",
      "Epoch 1: val_accuracy improved from None to 0.92450, saving model to wandb_models/Base_MLP_Model_2-lr_0.01-bs_64-20250917-080628/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.8141 - loss: 0.6201 - val_accuracy: 0.9245 - val_loss: 0.2597\n",
      "Epoch 2/100\n",
      "\u001b[1m818/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9150 - loss: 0.2885\n",
      "Epoch 2: val_accuracy improved from 0.92450 to 0.94533, saving model to MLP_Models\\Base_MLP_Model_2-lr_0.01-bs_64-20250917-080628\\best_model_epoch-02_val_acc-0.9453.keras\n",
      "\n",
      "Epoch 2: val_accuracy improved from 0.92450 to 0.94533, saving model to wandb_models/Base_MLP_Model_2-lr_0.01-bs_64-20250917-080628/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.9215 - loss: 0.2687 - val_accuracy: 0.9453 - val_loss: 0.1878\n",
      "Epoch 3/100\n",
      "\u001b[1m824/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9350 - loss: 0.2208\n",
      "Epoch 3: val_accuracy improved from 0.94533 to 0.95733, saving model to MLP_Models\\Base_MLP_Model_2-lr_0.01-bs_64-20250917-080628\\best_model_epoch-03_val_acc-0.9573.keras\n",
      "\n",
      "Epoch 3: val_accuracy improved from 0.94533 to 0.95733, saving model to wandb_models/Base_MLP_Model_2-lr_0.01-bs_64-20250917-080628/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9378 - loss: 0.2115 - val_accuracy: 0.9573 - val_loss: 0.1545\n",
      "Epoch 4/100\n",
      "\u001b[1m818/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9464 - loss: 0.1840\n",
      "Epoch 4: val_accuracy improved from 0.95733 to 0.96183, saving model to MLP_Models\\Base_MLP_Model_2-lr_0.01-bs_64-20250917-080628\\best_model_epoch-04_val_acc-0.9618.keras\n",
      "\n",
      "Epoch 4: val_accuracy improved from 0.95733 to 0.96183, saving model to wandb_models/Base_MLP_Model_2-lr_0.01-bs_64-20250917-080628/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9476 - loss: 0.1784 - val_accuracy: 0.9618 - val_loss: 0.1360\n",
      "Epoch 5/100\n",
      "\u001b[1m811/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9539 - loss: 0.1594\n",
      "Epoch 5: val_accuracy improved from 0.96183 to 0.96433, saving model to MLP_Models\\Base_MLP_Model_2-lr_0.01-bs_64-20250917-080628\\best_model_epoch-05_val_acc-0.9643.keras\n",
      "\n",
      "Epoch 5: val_accuracy improved from 0.96183 to 0.96433, saving model to wandb_models/Base_MLP_Model_2-lr_0.01-bs_64-20250917-080628/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.9545 - loss: 0.1555 - val_accuracy: 0.9643 - val_loss: 0.1239\n",
      "Epoch 6/100\n",
      "\u001b[1m812/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9583 - loss: 0.1410\n",
      "Epoch 6: val_accuracy improved from 0.96433 to 0.96583, saving model to MLP_Models\\Base_MLP_Model_2-lr_0.01-bs_64-20250917-080628\\best_model_epoch-06_val_acc-0.9658.keras\n",
      "\n",
      "Epoch 6: val_accuracy improved from 0.96433 to 0.96583, saving model to wandb_models/Base_MLP_Model_2-lr_0.01-bs_64-20250917-080628/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9591 - loss: 0.1383 - val_accuracy: 0.9658 - val_loss: 0.1153\n",
      "Epoch 7/100\n",
      "\u001b[1m829/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9629 - loss: 0.1264\n",
      "Epoch 7: val_accuracy improved from 0.96583 to 0.96883, saving model to MLP_Models\\Base_MLP_Model_2-lr_0.01-bs_64-20250917-080628\\best_model_epoch-07_val_acc-0.9688.keras\n",
      "\n",
      "Epoch 7: val_accuracy improved from 0.96583 to 0.96883, saving model to wandb_models/Base_MLP_Model_2-lr_0.01-bs_64-20250917-080628/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.9633 - loss: 0.1244 - val_accuracy: 0.9688 - val_loss: 0.1089\n",
      "Epoch 8/100\n",
      "\u001b[1m822/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9664 - loss: 0.1144\n",
      "Epoch 8: val_accuracy improved from 0.96883 to 0.97017, saving model to MLP_Models\\Base_MLP_Model_2-lr_0.01-bs_64-20250917-080628\\best_model_epoch-08_val_acc-0.9702.keras\n",
      "\n",
      "Epoch 8: val_accuracy improved from 0.96883 to 0.97017, saving model to wandb_models/Base_MLP_Model_2-lr_0.01-bs_64-20250917-080628/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9669 - loss: 0.1131 - val_accuracy: 0.9702 - val_loss: 0.1038\n",
      "Epoch 9/100\n",
      "\u001b[1m820/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9696 - loss: 0.1044\n",
      "Epoch 9: val_accuracy improved from 0.97017 to 0.97200, saving model to MLP_Models\\Base_MLP_Model_2-lr_0.01-bs_64-20250917-080628\\best_model_epoch-09_val_acc-0.9720.keras\n",
      "\n",
      "Epoch 9: val_accuracy improved from 0.97017 to 0.97200, saving model to wandb_models/Base_MLP_Model_2-lr_0.01-bs_64-20250917-080628/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.9699 - loss: 0.1035 - val_accuracy: 0.9720 - val_loss: 0.0998\n",
      "Epoch 10/100\n",
      "\u001b[1m813/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9718 - loss: 0.0959\n",
      "Epoch 10: val_accuracy improved from 0.97200 to 0.97233, saving model to MLP_Models\\Base_MLP_Model_2-lr_0.01-bs_64-20250917-080628\\best_model_epoch-10_val_acc-0.9723.keras\n",
      "\n",
      "Epoch 10: val_accuracy improved from 0.97200 to 0.97233, saving model to wandb_models/Base_MLP_Model_2-lr_0.01-bs_64-20250917-080628/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.9722 - loss: 0.0953 - val_accuracy: 0.9723 - val_loss: 0.0966\n",
      "Epoch 11/100\n",
      "\u001b[1m840/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9741 - loss: 0.0886\n",
      "Epoch 11: val_accuracy did not improve from 0.97233\n",
      "\n",
      "Epoch 11: val_accuracy did not improve from 0.97233\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9743 - loss: 0.0882 - val_accuracy: 0.9722 - val_loss: 0.0940\n",
      "Epoch 12/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9757 - loss: 0.0822\n",
      "Epoch 12: val_accuracy improved from 0.97233 to 0.97267, saving model to MLP_Models\\Base_MLP_Model_2-lr_0.01-bs_64-20250917-080628\\best_model_epoch-12_val_acc-0.9727.keras\n",
      "\n",
      "Epoch 12: val_accuracy improved from 0.97233 to 0.97267, saving model to wandb_models/Base_MLP_Model_2-lr_0.01-bs_64-20250917-080628/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.9764 - loss: 0.0820 - val_accuracy: 0.9727 - val_loss: 0.0919\n",
      "Epoch 13/100\n",
      "\u001b[1m816/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9778 - loss: 0.0767\n",
      "Epoch 13: val_accuracy improved from 0.97267 to 0.97317, saving model to MLP_Models\\Base_MLP_Model_2-lr_0.01-bs_64-20250917-080628\\best_model_epoch-13_val_acc-0.9732.keras\n",
      "\n",
      "Epoch 13: val_accuracy improved from 0.97267 to 0.97317, saving model to wandb_models/Base_MLP_Model_2-lr_0.01-bs_64-20250917-080628/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9782 - loss: 0.0765 - val_accuracy: 0.9732 - val_loss: 0.0901\n",
      "Epoch 14/100\n",
      "\u001b[1m823/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9792 - loss: 0.0718\n",
      "Epoch 14: val_accuracy improved from 0.97317 to 0.97383, saving model to MLP_Models\\Base_MLP_Model_2-lr_0.01-bs_64-20250917-080628\\best_model_epoch-14_val_acc-0.9738.keras\n",
      "\n",
      "Epoch 14: val_accuracy improved from 0.97317 to 0.97383, saving model to wandb_models/Base_MLP_Model_2-lr_0.01-bs_64-20250917-080628/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.9797 - loss: 0.0715 - val_accuracy: 0.9738 - val_loss: 0.0886\n",
      "Epoch 15/100\n",
      "\u001b[1m821/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9804 - loss: 0.0674\n",
      "Epoch 15: val_accuracy improved from 0.97383 to 0.97500, saving model to MLP_Models\\Base_MLP_Model_2-lr_0.01-bs_64-20250917-080628\\best_model_epoch-15_val_acc-0.9750.keras\n",
      "\n",
      "Epoch 15: val_accuracy improved from 0.97383 to 0.97500, saving model to wandb_models/Base_MLP_Model_2-lr_0.01-bs_64-20250917-080628/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9809 - loss: 0.0671 - val_accuracy: 0.9750 - val_loss: 0.0875\n",
      "Epoch 16/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9818 - loss: 0.0632\n",
      "Epoch 16: val_accuracy did not improve from 0.97500\n",
      "\n",
      "Epoch 16: val_accuracy did not improve from 0.97500\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9820 - loss: 0.0630 - val_accuracy: 0.9748 - val_loss: 0.0868\n",
      "Epoch 17/100\n",
      "\u001b[1m837/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9833 - loss: 0.0593\n",
      "Epoch 17: val_accuracy did not improve from 0.97500\n",
      "\n",
      "Epoch 17: val_accuracy did not improve from 0.97500\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9835 - loss: 0.0591 - val_accuracy: 0.9750 - val_loss: 0.0862\n",
      "Epoch 18/100\n",
      "\u001b[1m831/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9846 - loss: 0.0558\n",
      "Epoch 18: val_accuracy did not improve from 0.97500\n",
      "\n",
      "Epoch 18: val_accuracy did not improve from 0.97500\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9848 - loss: 0.0557 - val_accuracy: 0.9750 - val_loss: 0.0856\n",
      "Epoch 19/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9856 - loss: 0.0525\n",
      "Epoch 19: val_accuracy improved from 0.97500 to 0.97583, saving model to MLP_Models\\Base_MLP_Model_2-lr_0.01-bs_64-20250917-080628\\best_model_epoch-19_val_acc-0.9758.keras\n",
      "\n",
      "Epoch 19: val_accuracy improved from 0.97500 to 0.97583, saving model to wandb_models/Base_MLP_Model_2-lr_0.01-bs_64-20250917-080628/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.9859 - loss: 0.0524 - val_accuracy: 0.9758 - val_loss: 0.0853\n",
      "Epoch 20/100\n",
      "\u001b[1m822/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9867 - loss: 0.0495\n",
      "Epoch 20: val_accuracy improved from 0.97583 to 0.97633, saving model to MLP_Models\\Base_MLP_Model_2-lr_0.01-bs_64-20250917-080628\\best_model_epoch-20_val_acc-0.9763.keras\n",
      "\n",
      "Epoch 20: val_accuracy improved from 0.97583 to 0.97633, saving model to wandb_models/Base_MLP_Model_2-lr_0.01-bs_64-20250917-080628/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9867 - loss: 0.0494 - val_accuracy: 0.9763 - val_loss: 0.0852\n",
      "Epoch 21/100\n",
      "\u001b[1m822/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9874 - loss: 0.0466\n",
      "Epoch 21: val_accuracy did not improve from 0.97633\n",
      "\n",
      "Epoch 21: val_accuracy did not improve from 0.97633\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9876 - loss: 0.0465 - val_accuracy: 0.9763 - val_loss: 0.0853\n",
      "Epoch 22/100\n",
      "\u001b[1m833/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9883 - loss: 0.0440\n",
      "Epoch 22: val_accuracy did not improve from 0.97633\n",
      "\n",
      "Epoch 22: val_accuracy did not improve from 0.97633\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.9886 - loss: 0.0439 - val_accuracy: 0.9763 - val_loss: 0.0855\n",
      "Epoch 23/100\n",
      "\u001b[1m834/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9889 - loss: 0.0416\n",
      "Epoch 23: val_accuracy did not improve from 0.97633\n",
      "\n",
      "Epoch 23: val_accuracy did not improve from 0.97633\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9893 - loss: 0.0415 - val_accuracy: 0.9762 - val_loss: 0.0855\n",
      "Epoch 24/100\n",
      "\u001b[1m820/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9898 - loss: 0.0393\n",
      "Epoch 24: val_accuracy did not improve from 0.97633\n",
      "\n",
      "Epoch 24: val_accuracy did not improve from 0.97633\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9899 - loss: 0.0392 - val_accuracy: 0.9757 - val_loss: 0.0858\n",
      "Epoch 25/100\n",
      "\u001b[1m823/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9905 - loss: 0.0372\n",
      "Epoch 25: val_accuracy did not improve from 0.97633\n",
      "\n",
      "Epoch 25: val_accuracy did not improve from 0.97633\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9907 - loss: 0.0370 - val_accuracy: 0.9757 - val_loss: 0.0860\n",
      "Epoch 26/100\n",
      "\u001b[1m825/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9910 - loss: 0.0350\n",
      "Epoch 26: val_accuracy did not improve from 0.97633\n",
      "\n",
      "Epoch 26: val_accuracy did not improve from 0.97633\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9913 - loss: 0.0349 - val_accuracy: 0.9757 - val_loss: 0.0863\n",
      "Epoch 27/100\n",
      "\u001b[1m831/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9919 - loss: 0.0331\n",
      "Epoch 27: val_accuracy did not improve from 0.97633\n",
      "\n",
      "Epoch 27: val_accuracy did not improve from 0.97633\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9920 - loss: 0.0330 - val_accuracy: 0.9758 - val_loss: 0.0867\n",
      "Epoch 28/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9927 - loss: 0.0312\n",
      "Epoch 28: val_accuracy did not improve from 0.97633\n",
      "\n",
      "Epoch 28: val_accuracy did not improve from 0.97633\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9926 - loss: 0.0311 - val_accuracy: 0.9763 - val_loss: 0.0870\n",
      "Epoch 29/100\n",
      "\u001b[1m825/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9932 - loss: 0.0295\n",
      "Epoch 29: val_accuracy improved from 0.97633 to 0.97650, saving model to MLP_Models\\Base_MLP_Model_2-lr_0.01-bs_64-20250917-080628\\best_model_epoch-29_val_acc-0.9765.keras\n",
      "\n",
      "Epoch 29: val_accuracy improved from 0.97633 to 0.97650, saving model to wandb_models/Base_MLP_Model_2-lr_0.01-bs_64-20250917-080628/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.9931 - loss: 0.0293 - val_accuracy: 0.9765 - val_loss: 0.0873\n",
      "Epoch 30/100\n",
      "\u001b[1m824/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9938 - loss: 0.0278\n",
      "Epoch 30: val_accuracy did not improve from 0.97650\n",
      "\n",
      "Epoch 30: val_accuracy did not improve from 0.97650\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.9937 - loss: 0.0276 - val_accuracy: 0.9760 - val_loss: 0.0875\n",
      "Epoch 31/100\n",
      "\u001b[1m836/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9944 - loss: 0.0262\n",
      "Epoch 31: val_accuracy did not improve from 0.97650\n",
      "\n",
      "Epoch 31: val_accuracy did not improve from 0.97650\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.9943 - loss: 0.0261 - val_accuracy: 0.9758 - val_loss: 0.0879\n",
      "Epoch 32/100\n",
      "\u001b[1m836/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9949 - loss: 0.0247\n",
      "Epoch 32: val_accuracy did not improve from 0.97650\n",
      "\n",
      "Epoch 32: val_accuracy did not improve from 0.97650\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9947 - loss: 0.0246 - val_accuracy: 0.9760 - val_loss: 0.0883\n",
      "Epoch 33/100\n",
      "\u001b[1m840/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9951 - loss: 0.0233\n",
      "Epoch 33: val_accuracy did not improve from 0.97650\n",
      "\n",
      "Epoch 33: val_accuracy did not improve from 0.97650\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9950 - loss: 0.0232 - val_accuracy: 0.9763 - val_loss: 0.0888\n",
      "Epoch 34/100\n",
      "\u001b[1m839/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9953 - loss: 0.0220\n",
      "Epoch 34: val_accuracy did not improve from 0.97650\n",
      "\n",
      "Epoch 34: val_accuracy did not improve from 0.97650\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.9955 - loss: 0.0219 - val_accuracy: 0.9763 - val_loss: 0.0893\n",
      "Epoch 35/100\n",
      "\u001b[1m828/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9956 - loss: 0.0208\n",
      "Epoch 35: val_accuracy did not improve from 0.97650\n",
      "\n",
      "Epoch 35: val_accuracy did not improve from 0.97650\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9958 - loss: 0.0206 - val_accuracy: 0.9763 - val_loss: 0.0898\n",
      "Epoch 36/100\n",
      "\u001b[1m840/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9961 - loss: 0.0196\n",
      "Epoch 36: val_accuracy did not improve from 0.97650\n",
      "\n",
      "Epoch 36: val_accuracy did not improve from 0.97650\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.9963 - loss: 0.0194 - val_accuracy: 0.9765 - val_loss: 0.0901\n",
      "Epoch 37/100\n",
      "\u001b[1m840/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9965 - loss: 0.0184\n",
      "Epoch 37: val_accuracy did not improve from 0.97650\n",
      "\n",
      "Epoch 37: val_accuracy did not improve from 0.97650\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.9966 - loss: 0.0183 - val_accuracy: 0.9765 - val_loss: 0.0908\n",
      "Epoch 38/100\n",
      "\u001b[1m836/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9968 - loss: 0.0174\n",
      "Epoch 38: val_accuracy improved from 0.97650 to 0.97667, saving model to MLP_Models\\Base_MLP_Model_2-lr_0.01-bs_64-20250917-080628\\best_model_epoch-38_val_acc-0.9767.keras\n",
      "\n",
      "Epoch 38: val_accuracy improved from 0.97650 to 0.97667, saving model to wandb_models/Base_MLP_Model_2-lr_0.01-bs_64-20250917-080628/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.9970 - loss: 0.0173 - val_accuracy: 0.9767 - val_loss: 0.0912\n",
      "Epoch 39/100\n",
      "\u001b[1m828/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9971 - loss: 0.0163\n",
      "Epoch 39: val_accuracy did not improve from 0.97667\n",
      "\n",
      "Epoch 39: val_accuracy did not improve from 0.97667\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9972 - loss: 0.0163 - val_accuracy: 0.9762 - val_loss: 0.0918\n",
      "Epoch 40/100\n",
      "\u001b[1m832/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9974 - loss: 0.0153\n",
      "Epoch 40: val_accuracy did not improve from 0.97667\n",
      "\n",
      "Epoch 40: val_accuracy did not improve from 0.97667\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.9975 - loss: 0.0153 - val_accuracy: 0.9763 - val_loss: 0.0924\n",
      "Epoch 41/100\n",
      "\u001b[1m837/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9978 - loss: 0.0144\n",
      "Epoch 41: val_accuracy did not improve from 0.97667\n",
      "\n",
      "Epoch 41: val_accuracy did not improve from 0.97667\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - accuracy: 0.9979 - loss: 0.0144 - val_accuracy: 0.9762 - val_loss: 0.0931\n",
      "Epoch 42/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9981 - loss: 0.0136\n",
      "Epoch 42: val_accuracy did not improve from 0.97667\n",
      "\n",
      "Epoch 42: val_accuracy did not improve from 0.97667\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 9ms/step - accuracy: 0.9981 - loss: 0.0136 - val_accuracy: 0.9762 - val_loss: 0.0935\n",
      "Epoch 43/100\n",
      "\u001b[1m835/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9983 - loss: 0.0127\n",
      "Epoch 43: val_accuracy did not improve from 0.97667\n",
      "\n",
      "Epoch 43: val_accuracy did not improve from 0.97667\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - accuracy: 0.9983 - loss: 0.0127 - val_accuracy: 0.9763 - val_loss: 0.0940\n",
      "Epoch 44/100\n",
      "\u001b[1m834/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9984 - loss: 0.0120\n",
      "Epoch 44: val_accuracy did not improve from 0.97667\n",
      "\n",
      "Epoch 44: val_accuracy did not improve from 0.97667\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.9984 - loss: 0.0120 - val_accuracy: 0.9763 - val_loss: 0.0946\n",
      "Epoch 45/100\n",
      "\u001b[1m838/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9986 - loss: 0.0112\n",
      "Epoch 45: val_accuracy did not improve from 0.97667\n",
      "\n",
      "Epoch 45: val_accuracy did not improve from 0.97667\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.9985 - loss: 0.0113 - val_accuracy: 0.9763 - val_loss: 0.0952\n",
      "Epoch 46/100\n",
      "\u001b[1m832/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9988 - loss: 0.0106\n",
      "Epoch 46: val_accuracy did not improve from 0.97667\n",
      "\n",
      "Epoch 46: val_accuracy did not improve from 0.97667\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9987 - loss: 0.0106 - val_accuracy: 0.9765 - val_loss: 0.0960\n",
      "Epoch 47/100\n",
      "\u001b[1m832/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9989 - loss: 0.0099\n",
      "Epoch 47: val_accuracy did not improve from 0.97667\n",
      "\n",
      "Epoch 47: val_accuracy did not improve from 0.97667\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9989 - loss: 0.0100 - val_accuracy: 0.9765 - val_loss: 0.0966\n",
      "Epoch 48/100\n",
      "\u001b[1m839/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9990 - loss: 0.0093\n",
      "Epoch 48: val_accuracy did not improve from 0.97667\n",
      "\n",
      "Epoch 48: val_accuracy did not improve from 0.97667\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.9991 - loss: 0.0094 - val_accuracy: 0.9767 - val_loss: 0.0973\n",
      "\n",
      "Training history saved to: MLP_Models\\Base_MLP_Model_2-lr_0.01-bs_64-20250917-080628\\training_history.pkl\n",
      "\n",
      "--- Peak Performance Summary ---\n",
      "Best validation accuracy:           0.9767\n",
      "Associated training accuracy:       0.9970\n",
      "Occurred at epoch:                  38\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch/accuracy</td><td>▁▅▆▆▆▇▇▇▇▇▇▇▇▇▇█████████████████████████</td></tr><tr><td>epoch/epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>epoch/learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/loss</td><td>█▄▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/val_accuracy</td><td>▁▄▅▆▆▇▇▇▇▇██████████████████████████████</td></tr><tr><td>epoch/val_loss</td><td>█▅▄▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch/accuracy</td><td>0.99906</td></tr><tr><td>epoch/epoch</td><td>47</td></tr><tr><td>epoch/learning_rate</td><td>0.01</td></tr><tr><td>epoch/loss</td><td>0.00935</td></tr><tr><td>epoch/val_accuracy</td><td>0.97667</td></tr><tr><td>epoch/val_loss</td><td>0.09728</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Base_MLP_Model_2-lr_0.01-bs_64-20250917-080628</strong> at: <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/j8if8od3' target=\"_blank\">https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/j8if8od3</a><br> View project at: <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2' target=\"_blank\">https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2</a><br>Synced 5 W&B file(s), 0 media file(s), 36 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250917_080628-j8if8od3\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Adapting the normalisation layer...\n",
      "Adaptation complete.\n",
      "\n",
      "\n",
      "--- Starting Experiment: Base_MLP_Model_3 ---\n",
      "\n",
      "--- Model Architecture ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"Base_MLP_Model_3\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"Base_MLP_Model_3\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ normalization_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Normalization</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_32 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">100,480</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_33 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">33,024</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_34 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,448</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_35 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">650</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ normalization_8 (\u001b[38;5;33mNormalization\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m1\u001b[0m)      │             \u001b[38;5;34m3\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_8 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m784\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_32 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m100,480\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_33 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │        \u001b[38;5;34m33,024\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_34 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m16,448\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_35 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │           \u001b[38;5;34m650\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">150,605</span> (588.30 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m150,605\u001b[0m (588.30 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">150,602</span> (588.29 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m150,602\u001b[0m (588.29 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3</span> (16.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m3\u001b[0m (16.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Hyperparameters ---\n",
      "optimiser           : adamw\n",
      "learning_rate       : 0.001\n",
      "epochs              : 100\n",
      "batch_size          : 64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "creating run (0.0s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\TimVos\\VSC Projects\\CSE5ML\\Assessment 2\\wandb\\run-20250917_081005-1jhszhb6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/1jhszhb6' target=\"_blank\">Base_MLP_Model_3-lr_0.001-bs_64-20250917-081005</a></strong> to <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2' target=\"_blank\">https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/1jhszhb6' target=\"_blank\">https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/1jhszhb6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m835/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8750 - loss: 0.4070\n",
      "Epoch 1: val_accuracy improved from None to 0.96750, saving model to MLP_Models\\Base_MLP_Model_3-lr_0.001-bs_64-20250917-081005\\best_model_epoch-01_val_acc-0.9675.keras\n",
      "\n",
      "Epoch 1: val_accuracy improved from None to 0.96750, saving model to wandb_models/Base_MLP_Model_3-lr_0.001-bs_64-20250917-081005/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 13ms/step - accuracy: 0.9283 - loss: 0.2356 - val_accuracy: 0.9675 - val_loss: 0.1171\n",
      "Epoch 2/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9648 - loss: 0.1146\n",
      "Epoch 2: val_accuracy improved from 0.96750 to 0.97533, saving model to MLP_Models\\Base_MLP_Model_3-lr_0.001-bs_64-20250917-081005\\best_model_epoch-02_val_acc-0.9753.keras\n",
      "\n",
      "Epoch 2: val_accuracy improved from 0.96750 to 0.97533, saving model to wandb_models/Base_MLP_Model_3-lr_0.001-bs_64-20250917-081005/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 11ms/step - accuracy: 0.9684 - loss: 0.1022 - val_accuracy: 0.9753 - val_loss: 0.0822\n",
      "Epoch 3/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9752 - loss: 0.0759\n",
      "Epoch 3: val_accuracy improved from 0.97533 to 0.97900, saving model to MLP_Models\\Base_MLP_Model_3-lr_0.001-bs_64-20250917-081005\\best_model_epoch-03_val_acc-0.9790.keras\n",
      "\n",
      "Epoch 3: val_accuracy improved from 0.97533 to 0.97900, saving model to wandb_models/Base_MLP_Model_3-lr_0.001-bs_64-20250917-081005/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9763 - loss: 0.0723 - val_accuracy: 0.9790 - val_loss: 0.0749\n",
      "Epoch 4/100\n",
      "\u001b[1m834/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9821 - loss: 0.0575\n",
      "Epoch 4: val_accuracy did not improve from 0.97900\n",
      "\n",
      "Epoch 4: val_accuracy did not improve from 0.97900\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.9827 - loss: 0.0547 - val_accuracy: 0.9752 - val_loss: 0.0897\n",
      "Epoch 5/100\n",
      "\u001b[1m831/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9849 - loss: 0.0450\n",
      "Epoch 5: val_accuracy did not improve from 0.97900\n",
      "\n",
      "Epoch 5: val_accuracy did not improve from 0.97900\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.9857 - loss: 0.0435 - val_accuracy: 0.9725 - val_loss: 0.1069\n",
      "Epoch 6/100\n",
      "\u001b[1m834/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9857 - loss: 0.0404\n",
      "Epoch 6: val_accuracy did not improve from 0.97900\n",
      "\n",
      "Epoch 6: val_accuracy did not improve from 0.97900\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9861 - loss: 0.0400 - val_accuracy: 0.9763 - val_loss: 0.1020\n",
      "Epoch 7/100\n",
      "\u001b[1m829/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9893 - loss: 0.0311\n",
      "Epoch 7: val_accuracy did not improve from 0.97900\n",
      "\n",
      "Epoch 7: val_accuracy did not improve from 0.97900\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 11ms/step - accuracy: 0.9888 - loss: 0.0321 - val_accuracy: 0.9785 - val_loss: 0.1006\n",
      "Epoch 8/100\n",
      "\u001b[1m829/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9892 - loss: 0.0310\n",
      "Epoch 8: val_accuracy did not improve from 0.97900\n",
      "\n",
      "Epoch 8: val_accuracy did not improve from 0.97900\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.9894 - loss: 0.0320 - val_accuracy: 0.9758 - val_loss: 0.1000\n",
      "Epoch 9/100\n",
      "\u001b[1m839/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9916 - loss: 0.0246\n",
      "Epoch 9: val_accuracy did not improve from 0.97900\n",
      "\n",
      "Epoch 9: val_accuracy did not improve from 0.97900\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9904 - loss: 0.0282 - val_accuracy: 0.9780 - val_loss: 0.0921\n",
      "Epoch 10/100\n",
      "\u001b[1m835/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9928 - loss: 0.0224\n",
      "Epoch 10: val_accuracy did not improve from 0.97900\n",
      "\n",
      "Epoch 10: val_accuracy did not improve from 0.97900\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9920 - loss: 0.0244 - val_accuracy: 0.9742 - val_loss: 0.1180\n",
      "Epoch 11/100\n",
      "\u001b[1m827/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9909 - loss: 0.0256\n",
      "Epoch 11: val_accuracy did not improve from 0.97900\n",
      "\n",
      "Epoch 11: val_accuracy did not improve from 0.97900\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9920 - loss: 0.0231 - val_accuracy: 0.9782 - val_loss: 0.1021\n",
      "Epoch 12/100\n",
      "\u001b[1m831/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9931 - loss: 0.0223\n",
      "Epoch 12: val_accuracy improved from 0.97900 to 0.98083, saving model to MLP_Models\\Base_MLP_Model_3-lr_0.001-bs_64-20250917-081005\\best_model_epoch-12_val_acc-0.9808.keras\n",
      "\n",
      "Epoch 12: val_accuracy improved from 0.97900 to 0.98083, saving model to wandb_models/Base_MLP_Model_3-lr_0.001-bs_64-20250917-081005/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9928 - loss: 0.0227 - val_accuracy: 0.9808 - val_loss: 0.0967\n",
      "Epoch 13/100\n",
      "\u001b[1m836/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9929 - loss: 0.0214\n",
      "Epoch 13: val_accuracy did not improve from 0.98083\n",
      "\n",
      "Epoch 13: val_accuracy did not improve from 0.98083\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9928 - loss: 0.0220 - val_accuracy: 0.9773 - val_loss: 0.1024\n",
      "Epoch 14/100\n",
      "\u001b[1m830/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9943 - loss: 0.0192\n",
      "Epoch 14: val_accuracy did not improve from 0.98083\n",
      "\n",
      "Epoch 14: val_accuracy did not improve from 0.98083\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9946 - loss: 0.0173 - val_accuracy: 0.9797 - val_loss: 0.0946\n",
      "Epoch 15/100\n",
      "\u001b[1m831/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9947 - loss: 0.0168\n",
      "Epoch 15: val_accuracy did not improve from 0.98083\n",
      "\n",
      "Epoch 15: val_accuracy did not improve from 0.98083\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9932 - loss: 0.0208 - val_accuracy: 0.9772 - val_loss: 0.1253\n",
      "Epoch 16/100\n",
      "\u001b[1m834/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9938 - loss: 0.0196\n",
      "Epoch 16: val_accuracy did not improve from 0.98083\n",
      "\n",
      "Epoch 16: val_accuracy did not improve from 0.98083\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9943 - loss: 0.0180 - val_accuracy: 0.9793 - val_loss: 0.1084\n",
      "Epoch 17/100\n",
      "\u001b[1m828/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9947 - loss: 0.0167\n",
      "Epoch 17: val_accuracy did not improve from 0.98083\n",
      "\n",
      "Epoch 17: val_accuracy did not improve from 0.98083\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9947 - loss: 0.0166 - val_accuracy: 0.9748 - val_loss: 0.1260\n",
      "Epoch 18/100\n",
      "\u001b[1m838/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9940 - loss: 0.0198\n",
      "Epoch 18: val_accuracy did not improve from 0.98083\n",
      "\n",
      "Epoch 18: val_accuracy did not improve from 0.98083\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9945 - loss: 0.0179 - val_accuracy: 0.9775 - val_loss: 0.1168\n",
      "Epoch 19/100\n",
      "\u001b[1m837/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9960 - loss: 0.0146\n",
      "Epoch 19: val_accuracy did not improve from 0.98083\n",
      "\n",
      "Epoch 19: val_accuracy did not improve from 0.98083\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9950 - loss: 0.0165 - val_accuracy: 0.9780 - val_loss: 0.1216\n",
      "Epoch 20/100\n",
      "\u001b[1m838/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9952 - loss: 0.0145\n",
      "Epoch 20: val_accuracy did not improve from 0.98083\n",
      "\n",
      "Epoch 20: val_accuracy did not improve from 0.98083\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9950 - loss: 0.0160 - val_accuracy: 0.9773 - val_loss: 0.1231\n",
      "Epoch 21/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9947 - loss: 0.0175\n",
      "Epoch 21: val_accuracy did not improve from 0.98083\n",
      "\n",
      "Epoch 21: val_accuracy did not improve from 0.98083\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9954 - loss: 0.0147 - val_accuracy: 0.9807 - val_loss: 0.1071\n",
      "Epoch 22/100\n",
      "\u001b[1m832/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9965 - loss: 0.0117\n",
      "Epoch 22: val_accuracy did not improve from 0.98083\n",
      "\n",
      "Epoch 22: val_accuracy did not improve from 0.98083\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9958 - loss: 0.0140 - val_accuracy: 0.9777 - val_loss: 0.1197\n",
      "\n",
      "Training history saved to: MLP_Models\\Base_MLP_Model_3-lr_0.001-bs_64-20250917-081005\\training_history.pkl\n",
      "\n",
      "--- Peak Performance Summary ---\n",
      "Best validation accuracy:           0.9808\n",
      "Associated training accuracy:       0.9928\n",
      "Occurred at epoch:                  12\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch/accuracy</td><td>▁▅▆▇▇▇▇▇▇█████████████</td></tr><tr><td>epoch/epoch</td><td>▁▁▂▂▂▃▃▃▄▄▄▅▅▅▆▆▆▇▇▇██</td></tr><tr><td>epoch/learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/loss</td><td>█▄▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/val_accuracy</td><td>▁▅▇▅▄▆▇▅▇▅▇█▆▇▆▇▅▆▇▆█▆</td></tr><tr><td>epoch/val_loss</td><td>▇▂▁▃▅▅▅▄▃▇▅▄▅▄█▆█▇▇█▅▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch/accuracy</td><td>0.99581</td></tr><tr><td>epoch/epoch</td><td>21</td></tr><tr><td>epoch/learning_rate</td><td>0.001</td></tr><tr><td>epoch/loss</td><td>0.01399</td></tr><tr><td>epoch/val_accuracy</td><td>0.97767</td></tr><tr><td>epoch/val_loss</td><td>0.11969</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Base_MLP_Model_3-lr_0.001-bs_64-20250917-081005</strong> at: <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/1jhszhb6' target=\"_blank\">https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/1jhszhb6</a><br> View project at: <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2' target=\"_blank\">https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2</a><br>Synced 5 W&B file(s), 0 media file(s), 8 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250917_081005-1jhszhb6\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Adapting the normalisation layer...\n",
      "Adaptation complete.\n",
      "\n",
      "\n",
      "--- Starting Experiment: Base_MLP_Model_3 ---\n",
      "\n",
      "--- Model Architecture ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"Base_MLP_Model_3\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"Base_MLP_Model_3\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ normalization_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Normalization</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_36 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">100,480</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_37 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">33,024</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_38 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,448</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_39 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">650</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ normalization_9 (\u001b[38;5;33mNormalization\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m1\u001b[0m)      │             \u001b[38;5;34m3\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_9 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m784\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_36 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m100,480\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_37 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │        \u001b[38;5;34m33,024\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_38 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m16,448\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_39 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │           \u001b[38;5;34m650\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">150,605</span> (588.30 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m150,605\u001b[0m (588.30 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">150,602</span> (588.29 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m150,602\u001b[0m (588.29 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3</span> (16.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m3\u001b[0m (16.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Hyperparameters ---\n",
      "optimiser           : adamw\n",
      "learning_rate       : 0.01\n",
      "epochs              : 100\n",
      "batch_size          : 64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "creating run (0.1s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\TimVos\\VSC Projects\\CSE5ML\\Assessment 2\\wandb\\run-20250917_081301-vmbl4z6v</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/vmbl4z6v' target=\"_blank\">Base_MLP_Model_3-lr_0.01-bs_64-20250917-081301</a></strong> to <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2' target=\"_blank\">https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/vmbl4z6v' target=\"_blank\">https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/vmbl4z6v</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m837/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8284 - loss: 0.5896\n",
      "Epoch 1: val_accuracy improved from None to 0.94467, saving model to MLP_Models\\Base_MLP_Model_3-lr_0.01-bs_64-20250917-081301\\best_model_epoch-01_val_acc-0.9447.keras\n",
      "\n",
      "Epoch 1: val_accuracy improved from None to 0.94467, saving model to wandb_models/Base_MLP_Model_3-lr_0.01-bs_64-20250917-081301/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - accuracy: 0.8968 - loss: 0.3572 - val_accuracy: 0.9447 - val_loss: 0.2020\n",
      "Epoch 2/100\n",
      "\u001b[1m840/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9343 - loss: 0.2435\n",
      "Epoch 2: val_accuracy improved from 0.94467 to 0.95150, saving model to MLP_Models\\Base_MLP_Model_3-lr_0.01-bs_64-20250917-081301\\best_model_epoch-02_val_acc-0.9515.keras\n",
      "\n",
      "Epoch 2: val_accuracy improved from 0.94467 to 0.95150, saving model to wandb_models/Base_MLP_Model_3-lr_0.01-bs_64-20250917-081301/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9350 - loss: 0.2412 - val_accuracy: 0.9515 - val_loss: 0.1799\n",
      "Epoch 3/100\n",
      "\u001b[1m839/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9441 - loss: 0.2130\n",
      "Epoch 3: val_accuracy improved from 0.95150 to 0.95317, saving model to MLP_Models\\Base_MLP_Model_3-lr_0.01-bs_64-20250917-081301\\best_model_epoch-03_val_acc-0.9532.keras\n",
      "\n",
      "Epoch 3: val_accuracy improved from 0.95150 to 0.95317, saving model to wandb_models/Base_MLP_Model_3-lr_0.01-bs_64-20250917-081301/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9444 - loss: 0.2131 - val_accuracy: 0.9532 - val_loss: 0.1750\n",
      "Epoch 4/100\n",
      "\u001b[1m827/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9474 - loss: 0.1963\n",
      "Epoch 4: val_accuracy improved from 0.95317 to 0.96233, saving model to MLP_Models\\Base_MLP_Model_3-lr_0.01-bs_64-20250917-081301\\best_model_epoch-04_val_acc-0.9623.keras\n",
      "\n",
      "Epoch 4: val_accuracy improved from 0.95317 to 0.96233, saving model to wandb_models/Base_MLP_Model_3-lr_0.01-bs_64-20250917-081301/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9475 - loss: 0.1974 - val_accuracy: 0.9623 - val_loss: 0.1446\n",
      "Epoch 5/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9523 - loss: 0.1846\n",
      "Epoch 5: val_accuracy did not improve from 0.96233\n",
      "\n",
      "Epoch 5: val_accuracy did not improve from 0.96233\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9505 - loss: 0.1936 - val_accuracy: 0.9572 - val_loss: 0.1768\n",
      "Epoch 6/100\n",
      "\u001b[1m837/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9542 - loss: 0.1781\n",
      "Epoch 6: val_accuracy did not improve from 0.96233\n",
      "\n",
      "Epoch 6: val_accuracy did not improve from 0.96233\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.9552 - loss: 0.1749 - val_accuracy: 0.9607 - val_loss: 0.1498\n",
      "Epoch 7/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9561 - loss: 0.1709\n",
      "Epoch 7: val_accuracy improved from 0.96233 to 0.96417, saving model to MLP_Models\\Base_MLP_Model_3-lr_0.01-bs_64-20250917-081301\\best_model_epoch-07_val_acc-0.9642.keras\n",
      "\n",
      "Epoch 7: val_accuracy improved from 0.96233 to 0.96417, saving model to wandb_models/Base_MLP_Model_3-lr_0.01-bs_64-20250917-081301/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9560 - loss: 0.1695 - val_accuracy: 0.9642 - val_loss: 0.1371\n",
      "Epoch 8/100\n",
      "\u001b[1m833/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9558 - loss: 0.1692\n",
      "Epoch 8: val_accuracy did not improve from 0.96417\n",
      "\n",
      "Epoch 8: val_accuracy did not improve from 0.96417\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.9571 - loss: 0.1708 - val_accuracy: 0.9600 - val_loss: 0.1495\n",
      "Epoch 9/100\n",
      "\u001b[1m835/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9596 - loss: 0.1607\n",
      "Epoch 9: val_accuracy improved from 0.96417 to 0.96483, saving model to MLP_Models\\Base_MLP_Model_3-lr_0.01-bs_64-20250917-081301\\best_model_epoch-09_val_acc-0.9648.keras\n",
      "\n",
      "Epoch 9: val_accuracy improved from 0.96417 to 0.96483, saving model to wandb_models/Base_MLP_Model_3-lr_0.01-bs_64-20250917-081301/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.9597 - loss: 0.1569 - val_accuracy: 0.9648 - val_loss: 0.1357\n",
      "Epoch 10/100\n",
      "\u001b[1m831/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9591 - loss: 0.1606\n",
      "Epoch 10: val_accuracy did not improve from 0.96483\n",
      "\n",
      "Epoch 10: val_accuracy did not improve from 0.96483\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.9586 - loss: 0.1629 - val_accuracy: 0.9647 - val_loss: 0.1363\n",
      "Epoch 11/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9600 - loss: 0.1580\n",
      "Epoch 11: val_accuracy did not improve from 0.96483\n",
      "\n",
      "Epoch 11: val_accuracy did not improve from 0.96483\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9602 - loss: 0.1602 - val_accuracy: 0.9592 - val_loss: 0.1578\n",
      "Epoch 12/100\n",
      "\u001b[1m831/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9634 - loss: 0.1448\n",
      "Epoch 12: val_accuracy did not improve from 0.96483\n",
      "\n",
      "Epoch 12: val_accuracy did not improve from 0.96483\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.9636 - loss: 0.1446 - val_accuracy: 0.9605 - val_loss: 0.1734\n",
      "Epoch 13/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9641 - loss: 0.1494\n",
      "Epoch 13: val_accuracy improved from 0.96483 to 0.96550, saving model to MLP_Models\\Base_MLP_Model_3-lr_0.01-bs_64-20250917-081301\\best_model_epoch-13_val_acc-0.9655.keras\n",
      "\n",
      "Epoch 13: val_accuracy improved from 0.96483 to 0.96550, saving model to wandb_models/Base_MLP_Model_3-lr_0.01-bs_64-20250917-081301/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9625 - loss: 0.1556 - val_accuracy: 0.9655 - val_loss: 0.1464\n",
      "Epoch 14/100\n",
      "\u001b[1m840/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9645 - loss: 0.1430\n",
      "Epoch 14: val_accuracy did not improve from 0.96550\n",
      "\n",
      "Epoch 14: val_accuracy did not improve from 0.96550\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.9655 - loss: 0.1402 - val_accuracy: 0.9583 - val_loss: 0.1818\n",
      "Epoch 15/100\n",
      "\u001b[1m836/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9655 - loss: 0.1375\n",
      "Epoch 15: val_accuracy did not improve from 0.96550\n",
      "\n",
      "Epoch 15: val_accuracy did not improve from 0.96550\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9649 - loss: 0.1429 - val_accuracy: 0.9628 - val_loss: 0.1469\n",
      "Epoch 16/100\n",
      "\u001b[1m839/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9637 - loss: 0.1495\n",
      "Epoch 16: val_accuracy did not improve from 0.96550\n",
      "\n",
      "Epoch 16: val_accuracy did not improve from 0.96550\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9648 - loss: 0.1469 - val_accuracy: 0.9648 - val_loss: 0.1471\n",
      "Epoch 17/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9670 - loss: 0.1366\n",
      "Epoch 17: val_accuracy did not improve from 0.96550\n",
      "\n",
      "Epoch 17: val_accuracy did not improve from 0.96550\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.9658 - loss: 0.1410 - val_accuracy: 0.9625 - val_loss: 0.1396\n",
      "Epoch 18/100\n",
      "\u001b[1m838/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9673 - loss: 0.1360\n",
      "Epoch 18: val_accuracy improved from 0.96550 to 0.96683, saving model to MLP_Models\\Base_MLP_Model_3-lr_0.01-bs_64-20250917-081301\\best_model_epoch-18_val_acc-0.9668.keras\n",
      "\n",
      "Epoch 18: val_accuracy improved from 0.96550 to 0.96683, saving model to wandb_models/Base_MLP_Model_3-lr_0.01-bs_64-20250917-081301/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9671 - loss: 0.1353 - val_accuracy: 0.9668 - val_loss: 0.1322\n",
      "Epoch 19/100\n",
      "\u001b[1m837/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9693 - loss: 0.1300\n",
      "Epoch 19: val_accuracy did not improve from 0.96683\n",
      "\n",
      "Epoch 19: val_accuracy did not improve from 0.96683\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9673 - loss: 0.1371 - val_accuracy: 0.9638 - val_loss: 0.1503\n",
      "Epoch 20/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9666 - loss: 0.1370\n",
      "Epoch 20: val_accuracy did not improve from 0.96683\n",
      "\n",
      "Epoch 20: val_accuracy did not improve from 0.96683\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.9677 - loss: 0.1345 - val_accuracy: 0.9603 - val_loss: 0.1574\n",
      "Epoch 21/100\n",
      "\u001b[1m831/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9667 - loss: 0.1393\n",
      "Epoch 21: val_accuracy did not improve from 0.96683\n",
      "\n",
      "Epoch 21: val_accuracy did not improve from 0.96683\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.9682 - loss: 0.1316 - val_accuracy: 0.9633 - val_loss: 0.1492\n",
      "Epoch 22/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9680 - loss: 0.1288\n",
      "Epoch 22: val_accuracy did not improve from 0.96683\n",
      "\n",
      "Epoch 22: val_accuracy did not improve from 0.96683\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9679 - loss: 0.1300 - val_accuracy: 0.9657 - val_loss: 0.1368\n",
      "Epoch 23/100\n",
      "\u001b[1m833/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9678 - loss: 0.1276\n",
      "Epoch 23: val_accuracy did not improve from 0.96683\n",
      "\n",
      "Epoch 23: val_accuracy did not improve from 0.96683\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9688 - loss: 0.1244 - val_accuracy: 0.9665 - val_loss: 0.1403\n",
      "Epoch 24/100\n",
      "\u001b[1m838/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9698 - loss: 0.1227\n",
      "Epoch 24: val_accuracy did not improve from 0.96683\n",
      "\n",
      "Epoch 24: val_accuracy did not improve from 0.96683\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9688 - loss: 0.1264 - val_accuracy: 0.9637 - val_loss: 0.1479\n",
      "Epoch 25/100\n",
      "\u001b[1m826/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9695 - loss: 0.1267\n",
      "Epoch 25: val_accuracy did not improve from 0.96683\n",
      "\n",
      "Epoch 25: val_accuracy did not improve from 0.96683\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9709 - loss: 0.1225 - val_accuracy: 0.9658 - val_loss: 0.1569\n",
      "Epoch 26/100\n",
      "\u001b[1m838/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9696 - loss: 0.1268\n",
      "Epoch 26: val_accuracy improved from 0.96683 to 0.96850, saving model to MLP_Models\\Base_MLP_Model_3-lr_0.01-bs_64-20250917-081301\\best_model_epoch-26_val_acc-0.9685.keras\n",
      "\n",
      "Epoch 26: val_accuracy improved from 0.96683 to 0.96850, saving model to wandb_models/Base_MLP_Model_3-lr_0.01-bs_64-20250917-081301/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9698 - loss: 0.1232 - val_accuracy: 0.9685 - val_loss: 0.1525\n",
      "Epoch 27/100\n",
      "\u001b[1m829/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9706 - loss: 0.1208\n",
      "Epoch 27: val_accuracy did not improve from 0.96850\n",
      "\n",
      "Epoch 27: val_accuracy did not improve from 0.96850\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.9696 - loss: 0.1270 - val_accuracy: 0.9607 - val_loss: 0.1778\n",
      "Epoch 28/100\n",
      "\u001b[1m837/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9696 - loss: 0.1242\n",
      "Epoch 28: val_accuracy did not improve from 0.96850\n",
      "\n",
      "Epoch 28: val_accuracy did not improve from 0.96850\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9706 - loss: 0.1195 - val_accuracy: 0.9592 - val_loss: 0.1993\n",
      "Epoch 29/100\n",
      "\u001b[1m830/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9715 - loss: 0.1192\n",
      "Epoch 29: val_accuracy did not improve from 0.96850\n",
      "\n",
      "Epoch 29: val_accuracy did not improve from 0.96850\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.9710 - loss: 0.1209 - val_accuracy: 0.9658 - val_loss: 0.1624\n",
      "Epoch 30/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9710 - loss: 0.1259\n",
      "Epoch 30: val_accuracy did not improve from 0.96850\n",
      "\n",
      "Epoch 30: val_accuracy did not improve from 0.96850\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9687 - loss: 0.1326 - val_accuracy: 0.9588 - val_loss: 0.1777\n",
      "Epoch 31/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9711 - loss: 0.1162\n",
      "Epoch 31: val_accuracy did not improve from 0.96850\n",
      "\n",
      "Epoch 31: val_accuracy did not improve from 0.96850\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.9716 - loss: 0.1169 - val_accuracy: 0.9645 - val_loss: 0.1624\n",
      "Epoch 32/100\n",
      "\u001b[1m829/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9714 - loss: 0.1234\n",
      "Epoch 32: val_accuracy did not improve from 0.96850\n",
      "\n",
      "Epoch 32: val_accuracy did not improve from 0.96850\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.9698 - loss: 0.1293 - val_accuracy: 0.9625 - val_loss: 0.1755\n",
      "Epoch 33/100\n",
      "\u001b[1m828/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9722 - loss: 0.1107\n",
      "Epoch 33: val_accuracy did not improve from 0.96850\n",
      "\n",
      "Epoch 33: val_accuracy did not improve from 0.96850\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.9714 - loss: 0.1147 - val_accuracy: 0.9618 - val_loss: 0.1655\n",
      "Epoch 34/100\n",
      "\u001b[1m824/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9719 - loss: 0.1222\n",
      "Epoch 34: val_accuracy did not improve from 0.96850\n",
      "\n",
      "Epoch 34: val_accuracy did not improve from 0.96850\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.9720 - loss: 0.1206 - val_accuracy: 0.9657 - val_loss: 0.1635\n",
      "Epoch 35/100\n",
      "\u001b[1m836/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9721 - loss: 0.1183\n",
      "Epoch 35: val_accuracy did not improve from 0.96850\n",
      "\n",
      "Epoch 35: val_accuracy did not improve from 0.96850\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.9727 - loss: 0.1143 - val_accuracy: 0.9683 - val_loss: 0.1524\n",
      "Epoch 36/100\n",
      "\u001b[1m827/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9731 - loss: 0.1121\n",
      "Epoch 36: val_accuracy did not improve from 0.96850\n",
      "\n",
      "Epoch 36: val_accuracy did not improve from 0.96850\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9735 - loss: 0.1087 - val_accuracy: 0.9648 - val_loss: 0.1732\n",
      "\n",
      "Training history saved to: MLP_Models\\Base_MLP_Model_3-lr_0.01-bs_64-20250917-081301\\training_history.pkl\n",
      "\n",
      "--- Peak Performance Summary ---\n",
      "Best validation accuracy:           0.9685\n",
      "Associated training accuracy:       0.9698\n",
      "Occurred at epoch:                  26\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch/accuracy</td><td>▁▄▅▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇█▇██████████████</td></tr><tr><td>epoch/epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>epoch/learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/loss</td><td>█▅▄▄▃▃▃▃▂▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▂▁▁▂▁▂▁▁▁▁</td></tr><tr><td>epoch/val_accuracy</td><td>▁▃▃▆▅▆▇▆▇▇▅▆▇▅▆▇▆█▇▆▆▇▇▇▇█▆▅▇▅▇▆▆▇█▇</td></tr><tr><td>epoch/val_loss</td><td>█▆▅▂▅▃▁▃▁▁▄▅▂▆▂▃▂▁▃▄▃▁▂▃▃▃▆█▄▆▄▅▄▄▃▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch/accuracy</td><td>0.97352</td></tr><tr><td>epoch/epoch</td><td>35</td></tr><tr><td>epoch/learning_rate</td><td>0.01</td></tr><tr><td>epoch/loss</td><td>0.10868</td></tr><tr><td>epoch/val_accuracy</td><td>0.96483</td></tr><tr><td>epoch/val_loss</td><td>0.17323</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Base_MLP_Model_3-lr_0.01-bs_64-20250917-081301</strong> at: <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/vmbl4z6v' target=\"_blank\">https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/vmbl4z6v</a><br> View project at: <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2' target=\"_blank\">https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2</a><br>Synced 5 W&B file(s), 0 media file(s), 18 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250917_081301-vmbl4z6v\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Adapting the normalisation layer...\n",
      "Adaptation complete.\n",
      "\n",
      "\n",
      "--- Starting Experiment: Base_MLP_Model_3 ---\n",
      "\n",
      "--- Model Architecture ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"Base_MLP_Model_3\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"Base_MLP_Model_3\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ normalization_10                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Normalization</span>)                 │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_40 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">100,480</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_41 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">33,024</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_42 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,448</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_43 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">650</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ normalization_10                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m1\u001b[0m)      │             \u001b[38;5;34m3\u001b[0m │\n",
       "│ (\u001b[38;5;33mNormalization\u001b[0m)                 │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_10 (\u001b[38;5;33mFlatten\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m784\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_40 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m100,480\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_41 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │        \u001b[38;5;34m33,024\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_42 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m16,448\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_43 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │           \u001b[38;5;34m650\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">150,605</span> (588.30 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m150,605\u001b[0m (588.30 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">150,602</span> (588.29 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m150,602\u001b[0m (588.29 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3</span> (16.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m3\u001b[0m (16.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Hyperparameters ---\n",
      "optimiser           : SGD\n",
      "learning_rate       : 0.001\n",
      "epochs              : 100\n",
      "batch_size          : 64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for wandb.init()..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\TimVos\\VSC Projects\\CSE5ML\\Assessment 2\\wandb\\run-20250917_081550-ege6dfg9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/ege6dfg9' target=\"_blank\">Base_MLP_Model_3-lr_0.001-bs_64-20250917-081550</a></strong> to <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2' target=\"_blank\">https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/ege6dfg9' target=\"_blank\">https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/ege6dfg9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m833/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3168 - loss: 2.0437\n",
      "Epoch 1: val_accuracy improved from None to 0.79883, saving model to MLP_Models\\Base_MLP_Model_3-lr_0.001-bs_64-20250917-081550\\best_model_epoch-01_val_acc-0.7988.keras\n",
      "\n",
      "Epoch 1: val_accuracy improved from None to 0.79883, saving model to wandb_models/Base_MLP_Model_3-lr_0.001-bs_64-20250917-081550/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.5145 - loss: 1.6499 - val_accuracy: 0.7988 - val_loss: 0.9268\n",
      "Epoch 2/100\n",
      "\u001b[1m829/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7958 - loss: 0.8401\n",
      "Epoch 2: val_accuracy improved from 0.79883 to 0.87717, saving model to MLP_Models\\Base_MLP_Model_3-lr_0.001-bs_64-20250917-081550\\best_model_epoch-02_val_acc-0.8772.keras\n",
      "\n",
      "Epoch 2: val_accuracy improved from 0.79883 to 0.87717, saving model to wandb_models/Base_MLP_Model_3-lr_0.001-bs_64-20250917-081550/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.8162 - loss: 0.7312 - val_accuracy: 0.8772 - val_loss: 0.4941\n",
      "Epoch 3/100\n",
      "\u001b[1m819/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8582 - loss: 0.5288\n",
      "Epoch 3: val_accuracy improved from 0.87717 to 0.90183, saving model to MLP_Models\\Base_MLP_Model_3-lr_0.001-bs_64-20250917-081550\\best_model_epoch-03_val_acc-0.9018.keras\n",
      "\n",
      "Epoch 3: val_accuracy improved from 0.87717 to 0.90183, saving model to wandb_models/Base_MLP_Model_3-lr_0.001-bs_64-20250917-081550/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.8637 - loss: 0.5004 - val_accuracy: 0.9018 - val_loss: 0.3768\n",
      "Epoch 4/100\n",
      "\u001b[1m830/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8795 - loss: 0.4276\n",
      "Epoch 4: val_accuracy improved from 0.90183 to 0.91383, saving model to MLP_Models\\Base_MLP_Model_3-lr_0.001-bs_64-20250917-081550\\best_model_epoch-04_val_acc-0.9138.keras\n",
      "\n",
      "Epoch 4: val_accuracy improved from 0.90183 to 0.91383, saving model to wandb_models/Base_MLP_Model_3-lr_0.001-bs_64-20250917-081550/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.8832 - loss: 0.4161 - val_accuracy: 0.9138 - val_loss: 0.3232\n",
      "Epoch 5/100\n",
      "\u001b[1m833/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8921 - loss: 0.3759\n",
      "Epoch 5: val_accuracy improved from 0.91383 to 0.92200, saving model to MLP_Models\\Base_MLP_Model_3-lr_0.001-bs_64-20250917-081550\\best_model_epoch-05_val_acc-0.9220.keras\n",
      "\n",
      "Epoch 5: val_accuracy improved from 0.91383 to 0.92200, saving model to wandb_models/Base_MLP_Model_3-lr_0.001-bs_64-20250917-081550/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.8945 - loss: 0.3701 - val_accuracy: 0.9220 - val_loss: 0.2912\n",
      "Epoch 6/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9021 - loss: 0.3427\n",
      "Epoch 6: val_accuracy improved from 0.92200 to 0.92517, saving model to MLP_Models\\Base_MLP_Model_3-lr_0.001-bs_64-20250917-081550\\best_model_epoch-06_val_acc-0.9252.keras\n",
      "\n",
      "Epoch 6: val_accuracy improved from 0.92200 to 0.92517, saving model to wandb_models/Base_MLP_Model_3-lr_0.001-bs_64-20250917-081550/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.9034 - loss: 0.3395 - val_accuracy: 0.9252 - val_loss: 0.2693\n",
      "Epoch 7/100\n",
      "\u001b[1m827/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9090 - loss: 0.3188\n",
      "Epoch 7: val_accuracy improved from 0.92517 to 0.92917, saving model to MLP_Models\\Base_MLP_Model_3-lr_0.001-bs_64-20250917-081550\\best_model_epoch-07_val_acc-0.9292.keras\n",
      "\n",
      "Epoch 7: val_accuracy improved from 0.92517 to 0.92917, saving model to wandb_models/Base_MLP_Model_3-lr_0.001-bs_64-20250917-081550/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.9099 - loss: 0.3170 - val_accuracy: 0.9292 - val_loss: 0.2529\n",
      "Epoch 8/100\n",
      "\u001b[1m839/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9143 - loss: 0.3003\n",
      "Epoch 8: val_accuracy improved from 0.92917 to 0.93200, saving model to MLP_Models\\Base_MLP_Model_3-lr_0.001-bs_64-20250917-081550\\best_model_epoch-08_val_acc-0.9320.keras\n",
      "\n",
      "Epoch 8: val_accuracy improved from 0.92917 to 0.93200, saving model to wandb_models/Base_MLP_Model_3-lr_0.001-bs_64-20250917-081550/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.9145 - loss: 0.2993 - val_accuracy: 0.9320 - val_loss: 0.2400\n",
      "Epoch 9/100\n",
      "\u001b[1m838/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9185 - loss: 0.2851\n",
      "Epoch 9: val_accuracy improved from 0.93200 to 0.93500, saving model to MLP_Models\\Base_MLP_Model_3-lr_0.001-bs_64-20250917-081550\\best_model_epoch-09_val_acc-0.9350.keras\n",
      "\n",
      "Epoch 9: val_accuracy improved from 0.93200 to 0.93500, saving model to wandb_models/Base_MLP_Model_3-lr_0.001-bs_64-20250917-081550/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.9184 - loss: 0.2847 - val_accuracy: 0.9350 - val_loss: 0.2293\n",
      "Epoch 10/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9221 - loss: 0.2724\n",
      "Epoch 10: val_accuracy improved from 0.93500 to 0.93650, saving model to MLP_Models\\Base_MLP_Model_3-lr_0.001-bs_64-20250917-081550\\best_model_epoch-10_val_acc-0.9365.keras\n",
      "\n",
      "Epoch 10: val_accuracy improved from 0.93500 to 0.93650, saving model to wandb_models/Base_MLP_Model_3-lr_0.001-bs_64-20250917-081550/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.9215 - loss: 0.2723 - val_accuracy: 0.9365 - val_loss: 0.2201\n",
      "Epoch 11/100\n",
      "\u001b[1m820/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9249 - loss: 0.2613\n",
      "Epoch 11: val_accuracy improved from 0.93650 to 0.93900, saving model to MLP_Models\\Base_MLP_Model_3-lr_0.001-bs_64-20250917-081550\\best_model_epoch-11_val_acc-0.9390.keras\n",
      "\n",
      "Epoch 11: val_accuracy improved from 0.93650 to 0.93900, saving model to wandb_models/Base_MLP_Model_3-lr_0.001-bs_64-20250917-081550/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.9245 - loss: 0.2614 - val_accuracy: 0.9390 - val_loss: 0.2121\n",
      "Epoch 12/100\n",
      "\u001b[1m838/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9280 - loss: 0.2515\n",
      "Epoch 12: val_accuracy improved from 0.93900 to 0.94083, saving model to MLP_Models\\Base_MLP_Model_3-lr_0.001-bs_64-20250917-081550\\best_model_epoch-12_val_acc-0.9408.keras\n",
      "\n",
      "Epoch 12: val_accuracy improved from 0.93900 to 0.94083, saving model to wandb_models/Base_MLP_Model_3-lr_0.001-bs_64-20250917-081550/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9276 - loss: 0.2518 - val_accuracy: 0.9408 - val_loss: 0.2050\n",
      "Epoch 13/100\n",
      "\u001b[1m831/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9305 - loss: 0.2428\n",
      "Epoch 13: val_accuracy improved from 0.94083 to 0.94300, saving model to MLP_Models\\Base_MLP_Model_3-lr_0.001-bs_64-20250917-081550\\best_model_epoch-13_val_acc-0.9430.keras\n",
      "\n",
      "Epoch 13: val_accuracy improved from 0.94083 to 0.94300, saving model to wandb_models/Base_MLP_Model_3-lr_0.001-bs_64-20250917-081550/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9301 - loss: 0.2431 - val_accuracy: 0.9430 - val_loss: 0.1986\n",
      "Epoch 14/100\n",
      "\u001b[1m836/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9326 - loss: 0.2349\n",
      "Epoch 14: val_accuracy improved from 0.94300 to 0.94500, saving model to MLP_Models\\Base_MLP_Model_3-lr_0.001-bs_64-20250917-081550\\best_model_epoch-14_val_acc-0.9450.keras\n",
      "\n",
      "Epoch 14: val_accuracy improved from 0.94300 to 0.94500, saving model to wandb_models/Base_MLP_Model_3-lr_0.001-bs_64-20250917-081550/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9322 - loss: 0.2352 - val_accuracy: 0.9450 - val_loss: 0.1928\n",
      "Epoch 15/100\n",
      "\u001b[1m838/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9346 - loss: 0.2277\n",
      "Epoch 15: val_accuracy improved from 0.94500 to 0.94617, saving model to MLP_Models\\Base_MLP_Model_3-lr_0.001-bs_64-20250917-081550\\best_model_epoch-15_val_acc-0.9462.keras\n",
      "\n",
      "Epoch 15: val_accuracy improved from 0.94500 to 0.94617, saving model to wandb_models/Base_MLP_Model_3-lr_0.001-bs_64-20250917-081550/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9344 - loss: 0.2280 - val_accuracy: 0.9462 - val_loss: 0.1875\n",
      "Epoch 16/100\n",
      "\u001b[1m821/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9370 - loss: 0.2210\n",
      "Epoch 16: val_accuracy improved from 0.94617 to 0.94800, saving model to MLP_Models\\Base_MLP_Model_3-lr_0.001-bs_64-20250917-081550\\best_model_epoch-16_val_acc-0.9480.keras\n",
      "\n",
      "Epoch 16: val_accuracy improved from 0.94617 to 0.94800, saving model to wandb_models/Base_MLP_Model_3-lr_0.001-bs_64-20250917-081550/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9365 - loss: 0.2213 - val_accuracy: 0.9480 - val_loss: 0.1826\n",
      "Epoch 17/100\n",
      "\u001b[1m839/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9393 - loss: 0.2148\n",
      "Epoch 17: val_accuracy improved from 0.94800 to 0.94867, saving model to MLP_Models\\Base_MLP_Model_3-lr_0.001-bs_64-20250917-081550\\best_model_epoch-17_val_acc-0.9487.keras\n",
      "\n",
      "Epoch 17: val_accuracy improved from 0.94800 to 0.94867, saving model to wandb_models/Base_MLP_Model_3-lr_0.001-bs_64-20250917-081550/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9382 - loss: 0.2151 - val_accuracy: 0.9487 - val_loss: 0.1781\n",
      "Epoch 18/100\n",
      "\u001b[1m836/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9407 - loss: 0.2090\n",
      "Epoch 18: val_accuracy improved from 0.94867 to 0.95033, saving model to MLP_Models\\Base_MLP_Model_3-lr_0.001-bs_64-20250917-081550\\best_model_epoch-18_val_acc-0.9503.keras\n",
      "\n",
      "Epoch 18: val_accuracy improved from 0.94867 to 0.95033, saving model to wandb_models/Base_MLP_Model_3-lr_0.001-bs_64-20250917-081550/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9398 - loss: 0.2093 - val_accuracy: 0.9503 - val_loss: 0.1738\n",
      "Epoch 19/100\n",
      "\u001b[1m825/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9424 - loss: 0.2036\n",
      "Epoch 19: val_accuracy improved from 0.95033 to 0.95167, saving model to MLP_Models\\Base_MLP_Model_3-lr_0.001-bs_64-20250917-081550\\best_model_epoch-19_val_acc-0.9517.keras\n",
      "\n",
      "Epoch 19: val_accuracy improved from 0.95033 to 0.95167, saving model to wandb_models/Base_MLP_Model_3-lr_0.001-bs_64-20250917-081550/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.9414 - loss: 0.2039 - val_accuracy: 0.9517 - val_loss: 0.1698\n",
      "Epoch 20/100\n",
      "\u001b[1m839/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9436 - loss: 0.1985\n",
      "Epoch 20: val_accuracy improved from 0.95167 to 0.95200, saving model to MLP_Models\\Base_MLP_Model_3-lr_0.001-bs_64-20250917-081550\\best_model_epoch-20_val_acc-0.9520.keras\n",
      "\n",
      "Epoch 20: val_accuracy improved from 0.95167 to 0.95200, saving model to wandb_models/Base_MLP_Model_3-lr_0.001-bs_64-20250917-081550/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9424 - loss: 0.1988 - val_accuracy: 0.9520 - val_loss: 0.1661\n",
      "Epoch 21/100\n",
      "\u001b[1m833/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9447 - loss: 0.1937\n",
      "Epoch 21: val_accuracy improved from 0.95200 to 0.95317, saving model to MLP_Models\\Base_MLP_Model_3-lr_0.001-bs_64-20250917-081550\\best_model_epoch-21_val_acc-0.9532.keras\n",
      "\n",
      "Epoch 21: val_accuracy improved from 0.95200 to 0.95317, saving model to wandb_models/Base_MLP_Model_3-lr_0.001-bs_64-20250917-081550/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9438 - loss: 0.1939 - val_accuracy: 0.9532 - val_loss: 0.1627\n",
      "Epoch 22/100\n",
      "\u001b[1m832/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9460 - loss: 0.1892\n",
      "Epoch 22: val_accuracy improved from 0.95317 to 0.95333, saving model to MLP_Models\\Base_MLP_Model_3-lr_0.001-bs_64-20250917-081550\\best_model_epoch-22_val_acc-0.9533.keras\n",
      "\n",
      "Epoch 22: val_accuracy improved from 0.95317 to 0.95333, saving model to wandb_models/Base_MLP_Model_3-lr_0.001-bs_64-20250917-081550/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9451 - loss: 0.1894 - val_accuracy: 0.9533 - val_loss: 0.1594\n",
      "Epoch 23/100\n",
      "\u001b[1m836/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9475 - loss: 0.1849\n",
      "Epoch 23: val_accuracy improved from 0.95333 to 0.95400, saving model to MLP_Models\\Base_MLP_Model_3-lr_0.001-bs_64-20250917-081550\\best_model_epoch-23_val_acc-0.9540.keras\n",
      "\n",
      "Epoch 23: val_accuracy improved from 0.95333 to 0.95400, saving model to wandb_models/Base_MLP_Model_3-lr_0.001-bs_64-20250917-081550/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9463 - loss: 0.1850 - val_accuracy: 0.9540 - val_loss: 0.1563\n",
      "Epoch 24/100\n",
      "\u001b[1m828/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9484 - loss: 0.1808\n",
      "Epoch 24: val_accuracy improved from 0.95400 to 0.95550, saving model to MLP_Models\\Base_MLP_Model_3-lr_0.001-bs_64-20250917-081550\\best_model_epoch-24_val_acc-0.9555.keras\n",
      "\n",
      "Epoch 24: val_accuracy improved from 0.95400 to 0.95550, saving model to wandb_models/Base_MLP_Model_3-lr_0.001-bs_64-20250917-081550/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9476 - loss: 0.1809 - val_accuracy: 0.9555 - val_loss: 0.1534\n",
      "Epoch 25/100\n",
      "\u001b[1m822/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9494 - loss: 0.1769\n",
      "Epoch 25: val_accuracy improved from 0.95550 to 0.95633, saving model to MLP_Models\\Base_MLP_Model_3-lr_0.001-bs_64-20250917-081550\\best_model_epoch-25_val_acc-0.9563.keras\n",
      "\n",
      "Epoch 25: val_accuracy improved from 0.95550 to 0.95633, saving model to wandb_models/Base_MLP_Model_3-lr_0.001-bs_64-20250917-081550/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9486 - loss: 0.1770 - val_accuracy: 0.9563 - val_loss: 0.1506\n",
      "Epoch 26/100\n",
      "\u001b[1m837/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9504 - loss: 0.1731\n",
      "Epoch 26: val_accuracy improved from 0.95633 to 0.95650, saving model to MLP_Models\\Base_MLP_Model_3-lr_0.001-bs_64-20250917-081550\\best_model_epoch-26_val_acc-0.9565.keras\n",
      "\n",
      "Epoch 26: val_accuracy improved from 0.95633 to 0.95650, saving model to wandb_models/Base_MLP_Model_3-lr_0.001-bs_64-20250917-081550/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9495 - loss: 0.1732 - val_accuracy: 0.9565 - val_loss: 0.1480\n",
      "Epoch 27/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9513 - loss: 0.1696\n",
      "Epoch 27: val_accuracy improved from 0.95650 to 0.95800, saving model to MLP_Models\\Base_MLP_Model_3-lr_0.001-bs_64-20250917-081550\\best_model_epoch-27_val_acc-0.9580.keras\n",
      "\n",
      "Epoch 27: val_accuracy improved from 0.95650 to 0.95800, saving model to wandb_models/Base_MLP_Model_3-lr_0.001-bs_64-20250917-081550/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9505 - loss: 0.1696 - val_accuracy: 0.9580 - val_loss: 0.1455\n",
      "Epoch 28/100\n",
      "\u001b[1m832/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9525 - loss: 0.1662\n",
      "Epoch 28: val_accuracy improved from 0.95800 to 0.95883, saving model to MLP_Models\\Base_MLP_Model_3-lr_0.001-bs_64-20250917-081550\\best_model_epoch-28_val_acc-0.9588.keras\n",
      "\n",
      "Epoch 28: val_accuracy improved from 0.95800 to 0.95883, saving model to wandb_models/Base_MLP_Model_3-lr_0.001-bs_64-20250917-081550/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9516 - loss: 0.1662 - val_accuracy: 0.9588 - val_loss: 0.1431\n",
      "Epoch 29/100\n",
      "\u001b[1m835/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9533 - loss: 0.1629\n",
      "Epoch 29: val_accuracy improved from 0.95883 to 0.95950, saving model to MLP_Models\\Base_MLP_Model_3-lr_0.001-bs_64-20250917-081550\\best_model_epoch-29_val_acc-0.9595.keras\n",
      "\n",
      "Epoch 29: val_accuracy improved from 0.95883 to 0.95950, saving model to wandb_models/Base_MLP_Model_3-lr_0.001-bs_64-20250917-081550/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9522 - loss: 0.1629 - val_accuracy: 0.9595 - val_loss: 0.1408\n",
      "Epoch 30/100\n",
      "\u001b[1m834/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9542 - loss: 0.1597\n",
      "Epoch 30: val_accuracy improved from 0.95950 to 0.96017, saving model to MLP_Models\\Base_MLP_Model_3-lr_0.001-bs_64-20250917-081550\\best_model_epoch-30_val_acc-0.9602.keras\n",
      "\n",
      "Epoch 30: val_accuracy improved from 0.95950 to 0.96017, saving model to wandb_models/Base_MLP_Model_3-lr_0.001-bs_64-20250917-081550/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9530 - loss: 0.1597 - val_accuracy: 0.9602 - val_loss: 0.1387\n",
      "Epoch 31/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9553 - loss: 0.1567\n",
      "Epoch 31: val_accuracy improved from 0.96017 to 0.96067, saving model to MLP_Models\\Base_MLP_Model_3-lr_0.001-bs_64-20250917-081550\\best_model_epoch-31_val_acc-0.9607.keras\n",
      "\n",
      "Epoch 31: val_accuracy improved from 0.96017 to 0.96067, saving model to wandb_models/Base_MLP_Model_3-lr_0.001-bs_64-20250917-081550/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9541 - loss: 0.1567 - val_accuracy: 0.9607 - val_loss: 0.1366\n",
      "Epoch 32/100\n",
      "\u001b[1m825/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9562 - loss: 0.1538\n",
      "Epoch 32: val_accuracy improved from 0.96067 to 0.96200, saving model to MLP_Models\\Base_MLP_Model_3-lr_0.001-bs_64-20250917-081550\\best_model_epoch-32_val_acc-0.9620.keras\n",
      "\n",
      "Epoch 32: val_accuracy improved from 0.96067 to 0.96200, saving model to wandb_models/Base_MLP_Model_3-lr_0.001-bs_64-20250917-081550/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9550 - loss: 0.1537 - val_accuracy: 0.9620 - val_loss: 0.1346\n",
      "Epoch 33/100\n",
      "\u001b[1m837/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9569 - loss: 0.1510\n",
      "Epoch 33: val_accuracy improved from 0.96200 to 0.96217, saving model to MLP_Models\\Base_MLP_Model_3-lr_0.001-bs_64-20250917-081550\\best_model_epoch-33_val_acc-0.9622.keras\n",
      "\n",
      "Epoch 33: val_accuracy improved from 0.96200 to 0.96217, saving model to wandb_models/Base_MLP_Model_3-lr_0.001-bs_64-20250917-081550/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9558 - loss: 0.1509 - val_accuracy: 0.9622 - val_loss: 0.1327\n",
      "Epoch 34/100\n",
      "\u001b[1m835/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9581 - loss: 0.1483\n",
      "Epoch 34: val_accuracy improved from 0.96217 to 0.96283, saving model to MLP_Models\\Base_MLP_Model_3-lr_0.001-bs_64-20250917-081550\\best_model_epoch-34_val_acc-0.9628.keras\n",
      "\n",
      "Epoch 34: val_accuracy improved from 0.96217 to 0.96283, saving model to wandb_models/Base_MLP_Model_3-lr_0.001-bs_64-20250917-081550/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9568 - loss: 0.1481 - val_accuracy: 0.9628 - val_loss: 0.1309\n",
      "Epoch 35/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9592 - loss: 0.1457\n",
      "Epoch 35: val_accuracy improved from 0.96283 to 0.96300, saving model to MLP_Models\\Base_MLP_Model_3-lr_0.001-bs_64-20250917-081550\\best_model_epoch-35_val_acc-0.9630.keras\n",
      "\n",
      "Epoch 35: val_accuracy improved from 0.96283 to 0.96300, saving model to wandb_models/Base_MLP_Model_3-lr_0.001-bs_64-20250917-081550/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9578 - loss: 0.1455 - val_accuracy: 0.9630 - val_loss: 0.1292\n",
      "Epoch 36/100\n",
      "\u001b[1m834/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9600 - loss: 0.1432\n",
      "Epoch 36: val_accuracy improved from 0.96300 to 0.96367, saving model to MLP_Models\\Base_MLP_Model_3-lr_0.001-bs_64-20250917-081550\\best_model_epoch-36_val_acc-0.9637.keras\n",
      "\n",
      "Epoch 36: val_accuracy improved from 0.96300 to 0.96367, saving model to wandb_models/Base_MLP_Model_3-lr_0.001-bs_64-20250917-081550/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9587 - loss: 0.1430 - val_accuracy: 0.9637 - val_loss: 0.1275\n",
      "Epoch 37/100\n",
      "\u001b[1m835/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9611 - loss: 0.1407\n",
      "Epoch 37: val_accuracy improved from 0.96367 to 0.96383, saving model to MLP_Models\\Base_MLP_Model_3-lr_0.001-bs_64-20250917-081550\\best_model_epoch-37_val_acc-0.9638.keras\n",
      "\n",
      "Epoch 37: val_accuracy improved from 0.96367 to 0.96383, saving model to wandb_models/Base_MLP_Model_3-lr_0.001-bs_64-20250917-081550/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9597 - loss: 0.1405 - val_accuracy: 0.9638 - val_loss: 0.1259\n",
      "Epoch 38/100\n",
      "\u001b[1m840/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9620 - loss: 0.1383\n",
      "Epoch 38: val_accuracy improved from 0.96383 to 0.96417, saving model to MLP_Models\\Base_MLP_Model_3-lr_0.001-bs_64-20250917-081550\\best_model_epoch-38_val_acc-0.9642.keras\n",
      "\n",
      "Epoch 38: val_accuracy improved from 0.96383 to 0.96417, saving model to wandb_models/Base_MLP_Model_3-lr_0.001-bs_64-20250917-081550/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9605 - loss: 0.1381 - val_accuracy: 0.9642 - val_loss: 0.1244\n",
      "Epoch 39/100\n",
      "\u001b[1m821/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9623 - loss: 0.1360\n",
      "Epoch 39: val_accuracy improved from 0.96417 to 0.96433, saving model to MLP_Models\\Base_MLP_Model_3-lr_0.001-bs_64-20250917-081550\\best_model_epoch-39_val_acc-0.9643.keras\n",
      "\n",
      "Epoch 39: val_accuracy improved from 0.96417 to 0.96433, saving model to wandb_models/Base_MLP_Model_3-lr_0.001-bs_64-20250917-081550/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9610 - loss: 0.1358 - val_accuracy: 0.9643 - val_loss: 0.1230\n",
      "Epoch 40/100\n",
      "\u001b[1m825/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9630 - loss: 0.1338\n",
      "Epoch 40: val_accuracy improved from 0.96433 to 0.96483, saving model to MLP_Models\\Base_MLP_Model_3-lr_0.001-bs_64-20250917-081550\\best_model_epoch-40_val_acc-0.9648.keras\n",
      "\n",
      "Epoch 40: val_accuracy improved from 0.96433 to 0.96483, saving model to wandb_models/Base_MLP_Model_3-lr_0.001-bs_64-20250917-081550/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9618 - loss: 0.1335 - val_accuracy: 0.9648 - val_loss: 0.1215\n",
      "Epoch 41/100\n",
      "\u001b[1m832/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9636 - loss: 0.1317\n",
      "Epoch 41: val_accuracy improved from 0.96483 to 0.96500, saving model to MLP_Models\\Base_MLP_Model_3-lr_0.001-bs_64-20250917-081550\\best_model_epoch-41_val_acc-0.9650.keras\n",
      "\n",
      "Epoch 41: val_accuracy improved from 0.96483 to 0.96500, saving model to wandb_models/Base_MLP_Model_3-lr_0.001-bs_64-20250917-081550/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9623 - loss: 0.1313 - val_accuracy: 0.9650 - val_loss: 0.1202\n",
      "Epoch 42/100\n",
      "\u001b[1m828/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9641 - loss: 0.1296\n",
      "Epoch 42: val_accuracy improved from 0.96500 to 0.96583, saving model to MLP_Models\\Base_MLP_Model_3-lr_0.001-bs_64-20250917-081550\\best_model_epoch-42_val_acc-0.9658.keras\n",
      "\n",
      "Epoch 42: val_accuracy improved from 0.96500 to 0.96583, saving model to wandb_models/Base_MLP_Model_3-lr_0.001-bs_64-20250917-081550/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9630 - loss: 0.1292 - val_accuracy: 0.9658 - val_loss: 0.1189\n",
      "Epoch 43/100\n",
      "\u001b[1m832/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9646 - loss: 0.1275\n",
      "Epoch 43: val_accuracy did not improve from 0.96583\n",
      "\n",
      "Epoch 43: val_accuracy did not improve from 0.96583\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9636 - loss: 0.1272 - val_accuracy: 0.9658 - val_loss: 0.1176\n",
      "Epoch 44/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9651 - loss: 0.1256\n",
      "Epoch 44: val_accuracy improved from 0.96583 to 0.96617, saving model to MLP_Models\\Base_MLP_Model_3-lr_0.001-bs_64-20250917-081550\\best_model_epoch-44_val_acc-0.9662.keras\n",
      "\n",
      "Epoch 44: val_accuracy improved from 0.96583 to 0.96617, saving model to wandb_models/Base_MLP_Model_3-lr_0.001-bs_64-20250917-081550/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9641 - loss: 0.1252 - val_accuracy: 0.9662 - val_loss: 0.1164\n",
      "Epoch 45/100\n",
      "\u001b[1m821/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9657 - loss: 0.1237\n",
      "Epoch 45: val_accuracy improved from 0.96617 to 0.96650, saving model to MLP_Models\\Base_MLP_Model_3-lr_0.001-bs_64-20250917-081550\\best_model_epoch-45_val_acc-0.9665.keras\n",
      "\n",
      "Epoch 45: val_accuracy improved from 0.96617 to 0.96650, saving model to wandb_models/Base_MLP_Model_3-lr_0.001-bs_64-20250917-081550/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9647 - loss: 0.1232 - val_accuracy: 0.9665 - val_loss: 0.1152\n",
      "Epoch 46/100\n",
      "\u001b[1m836/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9661 - loss: 0.1218\n",
      "Epoch 46: val_accuracy improved from 0.96650 to 0.96717, saving model to MLP_Models\\Base_MLP_Model_3-lr_0.001-bs_64-20250917-081550\\best_model_epoch-46_val_acc-0.9672.keras\n",
      "\n",
      "Epoch 46: val_accuracy improved from 0.96650 to 0.96717, saving model to wandb_models/Base_MLP_Model_3-lr_0.001-bs_64-20250917-081550/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9651 - loss: 0.1214 - val_accuracy: 0.9672 - val_loss: 0.1141\n",
      "Epoch 47/100\n",
      "\u001b[1m823/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9668 - loss: 0.1200\n",
      "Epoch 47: val_accuracy improved from 0.96717 to 0.96767, saving model to MLP_Models\\Base_MLP_Model_3-lr_0.001-bs_64-20250917-081550\\best_model_epoch-47_val_acc-0.9677.keras\n",
      "\n",
      "Epoch 47: val_accuracy improved from 0.96717 to 0.96767, saving model to wandb_models/Base_MLP_Model_3-lr_0.001-bs_64-20250917-081550/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9656 - loss: 0.1195 - val_accuracy: 0.9677 - val_loss: 0.1130\n",
      "Epoch 48/100\n",
      "\u001b[1m836/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9672 - loss: 0.1182\n",
      "Epoch 48: val_accuracy improved from 0.96767 to 0.96817, saving model to MLP_Models\\Base_MLP_Model_3-lr_0.001-bs_64-20250917-081550\\best_model_epoch-48_val_acc-0.9682.keras\n",
      "\n",
      "Epoch 48: val_accuracy improved from 0.96767 to 0.96817, saving model to wandb_models/Base_MLP_Model_3-lr_0.001-bs_64-20250917-081550/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9661 - loss: 0.1177 - val_accuracy: 0.9682 - val_loss: 0.1120\n",
      "Epoch 49/100\n",
      "\u001b[1m831/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9677 - loss: 0.1165\n",
      "Epoch 49: val_accuracy did not improve from 0.96817\n",
      "\n",
      "Epoch 49: val_accuracy did not improve from 0.96817\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9668 - loss: 0.1160 - val_accuracy: 0.9680 - val_loss: 0.1110\n",
      "Epoch 50/100\n",
      "\u001b[1m828/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9683 - loss: 0.1148\n",
      "Epoch 50: val_accuracy improved from 0.96817 to 0.96833, saving model to MLP_Models\\Base_MLP_Model_3-lr_0.001-bs_64-20250917-081550\\best_model_epoch-50_val_acc-0.9683.keras\n",
      "\n",
      "Epoch 50: val_accuracy improved from 0.96817 to 0.96833, saving model to wandb_models/Base_MLP_Model_3-lr_0.001-bs_64-20250917-081550/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9675 - loss: 0.1143 - val_accuracy: 0.9683 - val_loss: 0.1100\n",
      "Epoch 51/100\n",
      "\u001b[1m837/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9687 - loss: 0.1132\n",
      "Epoch 51: val_accuracy improved from 0.96833 to 0.96900, saving model to MLP_Models\\Base_MLP_Model_3-lr_0.001-bs_64-20250917-081550\\best_model_epoch-51_val_acc-0.9690.keras\n",
      "\n",
      "Epoch 51: val_accuracy improved from 0.96833 to 0.96900, saving model to wandb_models/Base_MLP_Model_3-lr_0.001-bs_64-20250917-081550/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9678 - loss: 0.1127 - val_accuracy: 0.9690 - val_loss: 0.1090\n",
      "Epoch 52/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9689 - loss: 0.1116\n",
      "Epoch 52: val_accuracy improved from 0.96900 to 0.96950, saving model to MLP_Models\\Base_MLP_Model_3-lr_0.001-bs_64-20250917-081550\\best_model_epoch-52_val_acc-0.9695.keras\n",
      "\n",
      "Epoch 52: val_accuracy improved from 0.96900 to 0.96950, saving model to wandb_models/Base_MLP_Model_3-lr_0.001-bs_64-20250917-081550/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9682 - loss: 0.1111 - val_accuracy: 0.9695 - val_loss: 0.1081\n",
      "Epoch 53/100\n",
      "\u001b[1m823/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9694 - loss: 0.1100\n",
      "Epoch 53: val_accuracy improved from 0.96950 to 0.96967, saving model to MLP_Models\\Base_MLP_Model_3-lr_0.001-bs_64-20250917-081550\\best_model_epoch-53_val_acc-0.9697.keras\n",
      "\n",
      "Epoch 53: val_accuracy improved from 0.96950 to 0.96967, saving model to wandb_models/Base_MLP_Model_3-lr_0.001-bs_64-20250917-081550/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9687 - loss: 0.1095 - val_accuracy: 0.9697 - val_loss: 0.1072\n",
      "Epoch 54/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9699 - loss: 0.1085\n",
      "Epoch 54: val_accuracy did not improve from 0.96967\n",
      "\n",
      "Epoch 54: val_accuracy did not improve from 0.96967\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9693 - loss: 0.1080 - val_accuracy: 0.9697 - val_loss: 0.1063\n",
      "Epoch 55/100\n",
      "\u001b[1m820/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9701 - loss: 0.1070\n",
      "Epoch 55: val_accuracy improved from 0.96967 to 0.97050, saving model to MLP_Models\\Base_MLP_Model_3-lr_0.001-bs_64-20250917-081550\\best_model_epoch-55_val_acc-0.9705.keras\n",
      "\n",
      "Epoch 55: val_accuracy improved from 0.96967 to 0.97050, saving model to wandb_models/Base_MLP_Model_3-lr_0.001-bs_64-20250917-081550/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9696 - loss: 0.1065 - val_accuracy: 0.9705 - val_loss: 0.1055\n",
      "Epoch 56/100\n",
      "\u001b[1m822/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9705 - loss: 0.1056\n",
      "Epoch 56: val_accuracy did not improve from 0.97050\n",
      "\n",
      "Epoch 56: val_accuracy did not improve from 0.97050\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9701 - loss: 0.1050 - val_accuracy: 0.9705 - val_loss: 0.1046\n",
      "Epoch 57/100\n",
      "\u001b[1m834/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9707 - loss: 0.1042\n",
      "Epoch 57: val_accuracy improved from 0.97050 to 0.97083, saving model to MLP_Models\\Base_MLP_Model_3-lr_0.001-bs_64-20250917-081550\\best_model_epoch-57_val_acc-0.9708.keras\n",
      "\n",
      "Epoch 57: val_accuracy improved from 0.97050 to 0.97083, saving model to wandb_models/Base_MLP_Model_3-lr_0.001-bs_64-20250917-081550/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9704 - loss: 0.1036 - val_accuracy: 0.9708 - val_loss: 0.1038\n",
      "Epoch 58/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9712 - loss: 0.1028\n",
      "Epoch 58: val_accuracy improved from 0.97083 to 0.97117, saving model to MLP_Models\\Base_MLP_Model_3-lr_0.001-bs_64-20250917-081550\\best_model_epoch-58_val_acc-0.9712.keras\n",
      "\n",
      "Epoch 58: val_accuracy improved from 0.97083 to 0.97117, saving model to wandb_models/Base_MLP_Model_3-lr_0.001-bs_64-20250917-081550/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9709 - loss: 0.1022 - val_accuracy: 0.9712 - val_loss: 0.1031\n",
      "Epoch 59/100\n",
      "\u001b[1m829/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9714 - loss: 0.1014\n",
      "Epoch 59: val_accuracy improved from 0.97117 to 0.97167, saving model to MLP_Models\\Base_MLP_Model_3-lr_0.001-bs_64-20250917-081550\\best_model_epoch-59_val_acc-0.9717.keras\n",
      "\n",
      "Epoch 59: val_accuracy improved from 0.97117 to 0.97167, saving model to wandb_models/Base_MLP_Model_3-lr_0.001-bs_64-20250917-081550/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9712 - loss: 0.1008 - val_accuracy: 0.9717 - val_loss: 0.1023\n",
      "Epoch 60/100\n",
      "\u001b[1m840/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9717 - loss: 0.1001\n",
      "Epoch 60: val_accuracy improved from 0.97167 to 0.97183, saving model to MLP_Models\\Base_MLP_Model_3-lr_0.001-bs_64-20250917-081550\\best_model_epoch-60_val_acc-0.9718.keras\n",
      "\n",
      "Epoch 60: val_accuracy improved from 0.97167 to 0.97183, saving model to wandb_models/Base_MLP_Model_3-lr_0.001-bs_64-20250917-081550/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9715 - loss: 0.0995 - val_accuracy: 0.9718 - val_loss: 0.1016\n",
      "Epoch 61/100\n",
      "\u001b[1m838/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9721 - loss: 0.0988\n",
      "Epoch 61: val_accuracy improved from 0.97183 to 0.97233, saving model to MLP_Models\\Base_MLP_Model_3-lr_0.001-bs_64-20250917-081550\\best_model_epoch-61_val_acc-0.9723.keras\n",
      "\n",
      "Epoch 61: val_accuracy improved from 0.97183 to 0.97233, saving model to wandb_models/Base_MLP_Model_3-lr_0.001-bs_64-20250917-081550/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9718 - loss: 0.0982 - val_accuracy: 0.9723 - val_loss: 0.1009\n",
      "Epoch 62/100\n",
      "\u001b[1m823/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9725 - loss: 0.0976\n",
      "Epoch 62: val_accuracy improved from 0.97233 to 0.97267, saving model to MLP_Models\\Base_MLP_Model_3-lr_0.001-bs_64-20250917-081550\\best_model_epoch-62_val_acc-0.9727.keras\n",
      "\n",
      "Epoch 62: val_accuracy improved from 0.97233 to 0.97267, saving model to wandb_models/Base_MLP_Model_3-lr_0.001-bs_64-20250917-081550/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9722 - loss: 0.0969 - val_accuracy: 0.9727 - val_loss: 0.1002\n",
      "Epoch 63/100\n",
      "\u001b[1m825/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9728 - loss: 0.0963\n",
      "Epoch 63: val_accuracy improved from 0.97267 to 0.97300, saving model to MLP_Models\\Base_MLP_Model_3-lr_0.001-bs_64-20250917-081550\\best_model_epoch-63_val_acc-0.9730.keras\n",
      "\n",
      "Epoch 63: val_accuracy improved from 0.97267 to 0.97300, saving model to wandb_models/Base_MLP_Model_3-lr_0.001-bs_64-20250917-081550/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9725 - loss: 0.0957 - val_accuracy: 0.9730 - val_loss: 0.0995\n",
      "Epoch 64/100\n",
      "\u001b[1m827/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9730 - loss: 0.0951\n",
      "Epoch 64: val_accuracy improved from 0.97300 to 0.97367, saving model to MLP_Models\\Base_MLP_Model_3-lr_0.001-bs_64-20250917-081550\\best_model_epoch-64_val_acc-0.9737.keras\n",
      "\n",
      "Epoch 64: val_accuracy improved from 0.97300 to 0.97367, saving model to wandb_models/Base_MLP_Model_3-lr_0.001-bs_64-20250917-081550/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9728 - loss: 0.0945 - val_accuracy: 0.9737 - val_loss: 0.0989\n",
      "Epoch 65/100\n",
      "\u001b[1m823/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9734 - loss: 0.0939\n",
      "Epoch 65: val_accuracy improved from 0.97367 to 0.97400, saving model to MLP_Models\\Base_MLP_Model_3-lr_0.001-bs_64-20250917-081550\\best_model_epoch-65_val_acc-0.9740.keras\n",
      "\n",
      "Epoch 65: val_accuracy improved from 0.97367 to 0.97400, saving model to wandb_models/Base_MLP_Model_3-lr_0.001-bs_64-20250917-081550/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9731 - loss: 0.0933 - val_accuracy: 0.9740 - val_loss: 0.0983\n",
      "Epoch 66/100\n",
      "\u001b[1m837/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9738 - loss: 0.0928\n",
      "Epoch 66: val_accuracy improved from 0.97400 to 0.97417, saving model to MLP_Models\\Base_MLP_Model_3-lr_0.001-bs_64-20250917-081550\\best_model_epoch-66_val_acc-0.9742.keras\n",
      "\n",
      "Epoch 66: val_accuracy improved from 0.97400 to 0.97417, saving model to wandb_models/Base_MLP_Model_3-lr_0.001-bs_64-20250917-081550/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9734 - loss: 0.0921 - val_accuracy: 0.9742 - val_loss: 0.0977\n",
      "Epoch 67/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9743 - loss: 0.0916\n",
      "Epoch 67: val_accuracy improved from 0.97417 to 0.97450, saving model to MLP_Models\\Base_MLP_Model_3-lr_0.001-bs_64-20250917-081550\\best_model_epoch-67_val_acc-0.9745.keras\n",
      "\n",
      "Epoch 67: val_accuracy improved from 0.97417 to 0.97450, saving model to wandb_models/Base_MLP_Model_3-lr_0.001-bs_64-20250917-081550/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9739 - loss: 0.0910 - val_accuracy: 0.9745 - val_loss: 0.0971\n",
      "Epoch 68/100\n",
      "\u001b[1m828/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9746 - loss: 0.0905\n",
      "Epoch 68: val_accuracy improved from 0.97450 to 0.97483, saving model to MLP_Models\\Base_MLP_Model_3-lr_0.001-bs_64-20250917-081550\\best_model_epoch-68_val_acc-0.9748.keras\n",
      "\n",
      "Epoch 68: val_accuracy improved from 0.97450 to 0.97483, saving model to wandb_models/Base_MLP_Model_3-lr_0.001-bs_64-20250917-081550/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9742 - loss: 0.0898 - val_accuracy: 0.9748 - val_loss: 0.0966\n",
      "Epoch 69/100\n",
      "\u001b[1m822/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9748 - loss: 0.0894\n",
      "Epoch 69: val_accuracy improved from 0.97483 to 0.97500, saving model to MLP_Models\\Base_MLP_Model_3-lr_0.001-bs_64-20250917-081550\\best_model_epoch-69_val_acc-0.9750.keras\n",
      "\n",
      "Epoch 69: val_accuracy improved from 0.97483 to 0.97500, saving model to wandb_models/Base_MLP_Model_3-lr_0.001-bs_64-20250917-081550/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9745 - loss: 0.0887 - val_accuracy: 0.9750 - val_loss: 0.0960\n",
      "Epoch 70/100\n",
      "\u001b[1m834/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9751 - loss: 0.0883\n",
      "Epoch 70: val_accuracy did not improve from 0.97500\n",
      "\n",
      "Epoch 70: val_accuracy did not improve from 0.97500\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9748 - loss: 0.0877 - val_accuracy: 0.9748 - val_loss: 0.0955\n",
      "Epoch 71/100\n",
      "\u001b[1m831/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9753 - loss: 0.0873\n",
      "Epoch 71: val_accuracy improved from 0.97500 to 0.97517, saving model to MLP_Models\\Base_MLP_Model_3-lr_0.001-bs_64-20250917-081550\\best_model_epoch-71_val_acc-0.9752.keras\n",
      "\n",
      "Epoch 71: val_accuracy improved from 0.97500 to 0.97517, saving model to wandb_models/Base_MLP_Model_3-lr_0.001-bs_64-20250917-081550/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9751 - loss: 0.0866 - val_accuracy: 0.9752 - val_loss: 0.0950\n",
      "Epoch 72/100\n",
      "\u001b[1m836/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9757 - loss: 0.0862\n",
      "Epoch 72: val_accuracy did not improve from 0.97517\n",
      "\n",
      "Epoch 72: val_accuracy did not improve from 0.97517\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9754 - loss: 0.0856 - val_accuracy: 0.9752 - val_loss: 0.0945\n",
      "Epoch 73/100\n",
      "\u001b[1m831/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9762 - loss: 0.0852\n",
      "Epoch 73: val_accuracy did not improve from 0.97517\n",
      "\n",
      "Epoch 73: val_accuracy did not improve from 0.97517\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9760 - loss: 0.0845 - val_accuracy: 0.9752 - val_loss: 0.0940\n",
      "Epoch 74/100\n",
      "\u001b[1m833/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9765 - loss: 0.0842\n",
      "Epoch 74: val_accuracy did not improve from 0.97517\n",
      "\n",
      "Epoch 74: val_accuracy did not improve from 0.97517\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9763 - loss: 0.0835 - val_accuracy: 0.9752 - val_loss: 0.0936\n",
      "Epoch 75/100\n",
      "\u001b[1m827/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9766 - loss: 0.0832\n",
      "Epoch 75: val_accuracy improved from 0.97517 to 0.97533, saving model to MLP_Models\\Base_MLP_Model_3-lr_0.001-bs_64-20250917-081550\\best_model_epoch-75_val_acc-0.9753.keras\n",
      "\n",
      "Epoch 75: val_accuracy improved from 0.97517 to 0.97533, saving model to wandb_models/Base_MLP_Model_3-lr_0.001-bs_64-20250917-081550/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9766 - loss: 0.0825 - val_accuracy: 0.9753 - val_loss: 0.0931\n",
      "Epoch 76/100\n",
      "\u001b[1m827/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9771 - loss: 0.0823\n",
      "Epoch 76: val_accuracy did not improve from 0.97533\n",
      "\n",
      "Epoch 76: val_accuracy did not improve from 0.97533\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.9770 - loss: 0.0816 - val_accuracy: 0.9753 - val_loss: 0.0927\n",
      "Epoch 77/100\n",
      "\u001b[1m835/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9774 - loss: 0.0813\n",
      "Epoch 77: val_accuracy did not improve from 0.97533\n",
      "\n",
      "Epoch 77: val_accuracy did not improve from 0.97533\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9773 - loss: 0.0806 - val_accuracy: 0.9753 - val_loss: 0.0922\n",
      "Epoch 78/100\n",
      "\u001b[1m832/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9775 - loss: 0.0804\n",
      "Epoch 78: val_accuracy improved from 0.97533 to 0.97567, saving model to MLP_Models\\Base_MLP_Model_3-lr_0.001-bs_64-20250917-081550\\best_model_epoch-78_val_acc-0.9757.keras\n",
      "\n",
      "Epoch 78: val_accuracy improved from 0.97533 to 0.97567, saving model to wandb_models/Base_MLP_Model_3-lr_0.001-bs_64-20250917-081550/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.9775 - loss: 0.0797 - val_accuracy: 0.9757 - val_loss: 0.0918\n",
      "Epoch 79/100\n",
      "\u001b[1m825/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9777 - loss: 0.0795\n",
      "Epoch 79: val_accuracy did not improve from 0.97567\n",
      "\n",
      "Epoch 79: val_accuracy did not improve from 0.97567\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9777 - loss: 0.0788 - val_accuracy: 0.9755 - val_loss: 0.0914\n",
      "Epoch 80/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9779 - loss: 0.0786\n",
      "Epoch 80: val_accuracy did not improve from 0.97567\n",
      "\n",
      "Epoch 80: val_accuracy did not improve from 0.97567\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.9779 - loss: 0.0779 - val_accuracy: 0.9753 - val_loss: 0.0910\n",
      "Epoch 81/100\n",
      "\u001b[1m838/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9784 - loss: 0.0777\n",
      "Epoch 81: val_accuracy did not improve from 0.97567\n",
      "\n",
      "Epoch 81: val_accuracy did not improve from 0.97567\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.9781 - loss: 0.0770 - val_accuracy: 0.9753 - val_loss: 0.0906\n",
      "Epoch 82/100\n",
      "\u001b[1m821/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9787 - loss: 0.0769\n",
      "Epoch 82: val_accuracy did not improve from 0.97567\n",
      "\n",
      "Epoch 82: val_accuracy did not improve from 0.97567\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9784 - loss: 0.0761 - val_accuracy: 0.9757 - val_loss: 0.0902\n",
      "Epoch 83/100\n",
      "\u001b[1m824/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9788 - loss: 0.0760\n",
      "Epoch 83: val_accuracy improved from 0.97567 to 0.97583, saving model to MLP_Models\\Base_MLP_Model_3-lr_0.001-bs_64-20250917-081550\\best_model_epoch-83_val_acc-0.9758.keras\n",
      "\n",
      "Epoch 83: val_accuracy improved from 0.97567 to 0.97583, saving model to wandb_models/Base_MLP_Model_3-lr_0.001-bs_64-20250917-081550/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.9785 - loss: 0.0753 - val_accuracy: 0.9758 - val_loss: 0.0899\n",
      "Epoch 84/100\n",
      "\u001b[1m831/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9790 - loss: 0.0751\n",
      "Epoch 84: val_accuracy did not improve from 0.97583\n",
      "\n",
      "Epoch 84: val_accuracy did not improve from 0.97583\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9787 - loss: 0.0744 - val_accuracy: 0.9758 - val_loss: 0.0895\n",
      "Epoch 85/100\n",
      "\u001b[1m829/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9792 - loss: 0.0743\n",
      "Epoch 85: val_accuracy did not improve from 0.97583\n",
      "\n",
      "Epoch 85: val_accuracy did not improve from 0.97583\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.9789 - loss: 0.0736 - val_accuracy: 0.9757 - val_loss: 0.0892\n",
      "Epoch 86/100\n",
      "\u001b[1m821/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9794 - loss: 0.0735\n",
      "Epoch 86: val_accuracy did not improve from 0.97583\n",
      "\n",
      "Epoch 86: val_accuracy did not improve from 0.97583\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.9792 - loss: 0.0728 - val_accuracy: 0.9755 - val_loss: 0.0889\n",
      "Epoch 87/100\n",
      "\u001b[1m834/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9796 - loss: 0.0727\n",
      "Epoch 87: val_accuracy did not improve from 0.97583\n",
      "\n",
      "Epoch 87: val_accuracy did not improve from 0.97583\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.9795 - loss: 0.0720 - val_accuracy: 0.9757 - val_loss: 0.0885\n",
      "Epoch 88/100\n",
      "\u001b[1m826/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9798 - loss: 0.0719\n",
      "Epoch 88: val_accuracy did not improve from 0.97583\n",
      "\n",
      "Epoch 88: val_accuracy did not improve from 0.97583\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.9797 - loss: 0.0712 - val_accuracy: 0.9758 - val_loss: 0.0882\n",
      "Epoch 89/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9799 - loss: 0.0711\n",
      "Epoch 89: val_accuracy did not improve from 0.97583\n",
      "\n",
      "Epoch 89: val_accuracy did not improve from 0.97583\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.9800 - loss: 0.0704 - val_accuracy: 0.9758 - val_loss: 0.0879\n",
      "Epoch 90/100\n",
      "\u001b[1m836/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9802 - loss: 0.0704\n",
      "Epoch 90: val_accuracy did not improve from 0.97583\n",
      "\n",
      "Epoch 90: val_accuracy did not improve from 0.97583\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9802 - loss: 0.0696 - val_accuracy: 0.9757 - val_loss: 0.0877\n",
      "Epoch 91/100\n",
      "\u001b[1m829/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9803 - loss: 0.0696\n",
      "Epoch 91: val_accuracy did not improve from 0.97583\n",
      "\n",
      "Epoch 91: val_accuracy did not improve from 0.97583\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.9804 - loss: 0.0689 - val_accuracy: 0.9757 - val_loss: 0.0874\n",
      "Epoch 92/100\n",
      "\u001b[1m829/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9804 - loss: 0.0689\n",
      "Epoch 92: val_accuracy did not improve from 0.97583\n",
      "\n",
      "Epoch 92: val_accuracy did not improve from 0.97583\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.9806 - loss: 0.0681 - val_accuracy: 0.9755 - val_loss: 0.0871\n",
      "Epoch 93/100\n",
      "\u001b[1m821/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9805 - loss: 0.0682\n",
      "Epoch 93: val_accuracy did not improve from 0.97583\n",
      "\n",
      "Epoch 93: val_accuracy did not improve from 0.97583\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9808 - loss: 0.0674 - val_accuracy: 0.9757 - val_loss: 0.0868\n",
      "\n",
      "Training history saved to: MLP_Models\\Base_MLP_Model_3-lr_0.001-bs_64-20250917-081550\\training_history.pkl\n",
      "\n",
      "--- Peak Performance Summary ---\n",
      "Best validation accuracy:           0.9758\n",
      "Associated training accuracy:       0.9785\n",
      "Occurred at epoch:                  83\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch/accuracy</td><td>▁▆▇▇▇▇▇▇▇███████████████████████████████</td></tr><tr><td>epoch/epoch</td><td>▁▁▁▁▂▂▂▂▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇██</td></tr><tr><td>epoch/learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/loss</td><td>█▅▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/val_accuracy</td><td>▁▆▆▆▇▇▇▇▇▇▇▇▇███████████████████████████</td></tr><tr><td>epoch/val_loss</td><td>█▄▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch/accuracy</td><td>0.9808</td></tr><tr><td>epoch/epoch</td><td>92</td></tr><tr><td>epoch/learning_rate</td><td>0.001</td></tr><tr><td>epoch/loss</td><td>0.0674</td></tr><tr><td>epoch/val_accuracy</td><td>0.97567</td></tr><tr><td>epoch/val_loss</td><td>0.08683</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Base_MLP_Model_3-lr_0.001-bs_64-20250917-081550</strong> at: <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/ege6dfg9' target=\"_blank\">https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/ege6dfg9</a><br> View project at: <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2' target=\"_blank\">https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2</a><br>Synced 5 W&B file(s), 0 media file(s), 138 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250917_081550-ege6dfg9\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Adapting the normalisation layer...\n",
      "Adaptation complete.\n",
      "\n",
      "\n",
      "--- Starting Experiment: Base_MLP_Model_3 ---\n",
      "\n",
      "--- Model Architecture ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"Base_MLP_Model_3\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"Base_MLP_Model_3\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ normalization_11                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Normalization</span>)                 │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_44 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">100,480</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_45 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">33,024</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_46 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,448</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_47 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">650</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ normalization_11                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m1\u001b[0m)      │             \u001b[38;5;34m3\u001b[0m │\n",
       "│ (\u001b[38;5;33mNormalization\u001b[0m)                 │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_11 (\u001b[38;5;33mFlatten\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m784\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_44 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m100,480\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_45 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │        \u001b[38;5;34m33,024\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_46 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m16,448\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_47 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │           \u001b[38;5;34m650\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">150,605</span> (588.30 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m150,605\u001b[0m (588.30 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">150,602</span> (588.29 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m150,602\u001b[0m (588.29 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3</span> (16.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m3\u001b[0m (16.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Hyperparameters ---\n",
      "optimiser           : SGD\n",
      "learning_rate       : 0.01\n",
      "epochs              : 100\n",
      "batch_size          : 64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "creating run (0.1s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\TimVos\\VSC Projects\\CSE5ML\\Assessment 2\\wandb\\run-20250917_082144-u2vcw1cg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/u2vcw1cg' target=\"_blank\">Base_MLP_Model_3-lr_0.01-bs_64-20250917-082144</a></strong> to <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2' target=\"_blank\">https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/u2vcw1cg' target=\"_blank\">https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/u2vcw1cg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m821/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7134 - loss: 0.9693\n",
      "Epoch 1: val_accuracy improved from None to 0.93433, saving model to MLP_Models\\Base_MLP_Model_3-lr_0.01-bs_64-20250917-082144\\best_model_epoch-01_val_acc-0.9343.keras\n",
      "\n",
      "Epoch 1: val_accuracy improved from None to 0.93433, saving model to wandb_models/Base_MLP_Model_3-lr_0.01-bs_64-20250917-082144/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.8415 - loss: 0.5516 - val_accuracy: 0.9343 - val_loss: 0.2248\n",
      "Epoch 2/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9272 - loss: 0.2547\n",
      "Epoch 2: val_accuracy improved from 0.93433 to 0.95183, saving model to MLP_Models\\Base_MLP_Model_3-lr_0.01-bs_64-20250917-082144\\best_model_epoch-02_val_acc-0.9518.keras\n",
      "\n",
      "Epoch 2: val_accuracy improved from 0.93433 to 0.95183, saving model to wandb_models/Base_MLP_Model_3-lr_0.01-bs_64-20250917-082144/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9297 - loss: 0.2388 - val_accuracy: 0.9518 - val_loss: 0.1674\n",
      "Epoch 3/100\n",
      "\u001b[1m828/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9439 - loss: 0.1949\n",
      "Epoch 3: val_accuracy improved from 0.95183 to 0.96217, saving model to MLP_Models\\Base_MLP_Model_3-lr_0.01-bs_64-20250917-082144\\best_model_epoch-03_val_acc-0.9622.keras\n",
      "\n",
      "Epoch 3: val_accuracy improved from 0.95183 to 0.96217, saving model to wandb_models/Base_MLP_Model_3-lr_0.01-bs_64-20250917-082144/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.9453 - loss: 0.1870 - val_accuracy: 0.9622 - val_loss: 0.1407\n",
      "Epoch 4/100\n",
      "\u001b[1m827/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9524 - loss: 0.1619\n",
      "Epoch 4: val_accuracy improved from 0.96217 to 0.96633, saving model to MLP_Models\\Base_MLP_Model_3-lr_0.01-bs_64-20250917-082144\\best_model_epoch-04_val_acc-0.9663.keras\n",
      "\n",
      "Epoch 4: val_accuracy improved from 0.96217 to 0.96633, saving model to wandb_models/Base_MLP_Model_3-lr_0.01-bs_64-20250917-082144/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.9538 - loss: 0.1567 - val_accuracy: 0.9663 - val_loss: 0.1250\n",
      "Epoch 5/100\n",
      "\u001b[1m821/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9599 - loss: 0.1396\n",
      "Epoch 5: val_accuracy improved from 0.96633 to 0.96900, saving model to MLP_Models\\Base_MLP_Model_3-lr_0.01-bs_64-20250917-082144\\best_model_epoch-05_val_acc-0.9690.keras\n",
      "\n",
      "Epoch 5: val_accuracy improved from 0.96633 to 0.96900, saving model to wandb_models/Base_MLP_Model_3-lr_0.01-bs_64-20250917-082144/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.9608 - loss: 0.1355 - val_accuracy: 0.9690 - val_loss: 0.1145\n",
      "Epoch 6/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9642 - loss: 0.1226\n",
      "Epoch 6: val_accuracy improved from 0.96900 to 0.97133, saving model to MLP_Models\\Base_MLP_Model_3-lr_0.01-bs_64-20250917-082144\\best_model_epoch-06_val_acc-0.9713.keras\n",
      "\n",
      "Epoch 6: val_accuracy improved from 0.96900 to 0.97133, saving model to wandb_models/Base_MLP_Model_3-lr_0.01-bs_64-20250917-082144/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.9649 - loss: 0.1194 - val_accuracy: 0.9713 - val_loss: 0.1078\n",
      "Epoch 7/100\n",
      "\u001b[1m826/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9684 - loss: 0.1091\n",
      "Epoch 7: val_accuracy improved from 0.97133 to 0.97267, saving model to MLP_Models\\Base_MLP_Model_3-lr_0.01-bs_64-20250917-082144\\best_model_epoch-07_val_acc-0.9727.keras\n",
      "\n",
      "Epoch 7: val_accuracy improved from 0.97133 to 0.97267, saving model to wandb_models/Base_MLP_Model_3-lr_0.01-bs_64-20250917-082144/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.9689 - loss: 0.1065 - val_accuracy: 0.9727 - val_loss: 0.1027\n",
      "Epoch 8/100\n",
      "\u001b[1m821/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9722 - loss: 0.0979\n",
      "Epoch 8: val_accuracy improved from 0.97267 to 0.97367, saving model to MLP_Models\\Base_MLP_Model_3-lr_0.01-bs_64-20250917-082144\\best_model_epoch-08_val_acc-0.9737.keras\n",
      "\n",
      "Epoch 8: val_accuracy improved from 0.97267 to 0.97367, saving model to wandb_models/Base_MLP_Model_3-lr_0.01-bs_64-20250917-082144/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9721 - loss: 0.0958 - val_accuracy: 0.9737 - val_loss: 0.0982\n",
      "Epoch 9/100\n",
      "\u001b[1m827/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9745 - loss: 0.0884\n",
      "Epoch 9: val_accuracy improved from 0.97367 to 0.97467, saving model to MLP_Models\\Base_MLP_Model_3-lr_0.01-bs_64-20250917-082144\\best_model_epoch-09_val_acc-0.9747.keras\n",
      "\n",
      "Epoch 9: val_accuracy improved from 0.97367 to 0.97467, saving model to wandb_models/Base_MLP_Model_3-lr_0.01-bs_64-20250917-082144/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.9747 - loss: 0.0868 - val_accuracy: 0.9747 - val_loss: 0.0950\n",
      "Epoch 10/100\n",
      "\u001b[1m826/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9774 - loss: 0.0802\n",
      "Epoch 10: val_accuracy improved from 0.97467 to 0.97550, saving model to MLP_Models\\Base_MLP_Model_3-lr_0.01-bs_64-20250917-082144\\best_model_epoch-10_val_acc-0.9755.keras\n",
      "\n",
      "Epoch 10: val_accuracy improved from 0.97467 to 0.97550, saving model to wandb_models/Base_MLP_Model_3-lr_0.01-bs_64-20250917-082144/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9776 - loss: 0.0790 - val_accuracy: 0.9755 - val_loss: 0.0926\n",
      "Epoch 11/100\n",
      "\u001b[1m835/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9795 - loss: 0.0732\n",
      "Epoch 11: val_accuracy improved from 0.97550 to 0.97583, saving model to MLP_Models\\Base_MLP_Model_3-lr_0.01-bs_64-20250917-082144\\best_model_epoch-11_val_acc-0.9758.keras\n",
      "\n",
      "Epoch 11: val_accuracy improved from 0.97550 to 0.97583, saving model to wandb_models/Base_MLP_Model_3-lr_0.01-bs_64-20250917-082144/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9798 - loss: 0.0722 - val_accuracy: 0.9758 - val_loss: 0.0906\n",
      "Epoch 12/100\n",
      "\u001b[1m834/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9814 - loss: 0.0670\n",
      "Epoch 12: val_accuracy improved from 0.97583 to 0.97617, saving model to MLP_Models\\Base_MLP_Model_3-lr_0.01-bs_64-20250917-082144\\best_model_epoch-12_val_acc-0.9762.keras\n",
      "\n",
      "Epoch 12: val_accuracy improved from 0.97583 to 0.97617, saving model to wandb_models/Base_MLP_Model_3-lr_0.01-bs_64-20250917-082144/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9814 - loss: 0.0662 - val_accuracy: 0.9762 - val_loss: 0.0893\n",
      "Epoch 13/100\n",
      "\u001b[1m835/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9831 - loss: 0.0616\n",
      "Epoch 13: val_accuracy improved from 0.97617 to 0.97667, saving model to MLP_Models\\Base_MLP_Model_3-lr_0.01-bs_64-20250917-082144\\best_model_epoch-13_val_acc-0.9767.keras\n",
      "\n",
      "Epoch 13: val_accuracy improved from 0.97617 to 0.97667, saving model to wandb_models/Base_MLP_Model_3-lr_0.01-bs_64-20250917-082144/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.9831 - loss: 0.0609 - val_accuracy: 0.9767 - val_loss: 0.0883\n",
      "Epoch 14/100\n",
      "\u001b[1m827/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9845 - loss: 0.0567\n",
      "Epoch 14: val_accuracy did not improve from 0.97667\n",
      "\n",
      "Epoch 14: val_accuracy did not improve from 0.97667\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.9846 - loss: 0.0560 - val_accuracy: 0.9767 - val_loss: 0.0876\n",
      "Epoch 15/100\n",
      "\u001b[1m833/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9857 - loss: 0.0522\n",
      "Epoch 15: val_accuracy improved from 0.97667 to 0.97683, saving model to MLP_Models\\Base_MLP_Model_3-lr_0.01-bs_64-20250917-082144\\best_model_epoch-15_val_acc-0.9768.keras\n",
      "\n",
      "Epoch 15: val_accuracy improved from 0.97667 to 0.97683, saving model to wandb_models/Base_MLP_Model_3-lr_0.01-bs_64-20250917-082144/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9861 - loss: 0.0516 - val_accuracy: 0.9768 - val_loss: 0.0870\n",
      "Epoch 16/100\n",
      "\u001b[1m819/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9871 - loss: 0.0483\n",
      "Epoch 16: val_accuracy did not improve from 0.97683\n",
      "\n",
      "Epoch 16: val_accuracy did not improve from 0.97683\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9873 - loss: 0.0477 - val_accuracy: 0.9767 - val_loss: 0.0865\n",
      "Epoch 17/100\n",
      "\u001b[1m825/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9881 - loss: 0.0446\n",
      "Epoch 17: val_accuracy did not improve from 0.97683\n",
      "\n",
      "Epoch 17: val_accuracy did not improve from 0.97683\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9885 - loss: 0.0440 - val_accuracy: 0.9767 - val_loss: 0.0864\n",
      "Epoch 18/100\n",
      "\u001b[1m819/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9892 - loss: 0.0411\n",
      "Epoch 18: val_accuracy did not improve from 0.97683\n",
      "\n",
      "Epoch 18: val_accuracy did not improve from 0.97683\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9895 - loss: 0.0406 - val_accuracy: 0.9768 - val_loss: 0.0859\n",
      "Epoch 19/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9902 - loss: 0.0380\n",
      "Epoch 19: val_accuracy did not improve from 0.97683\n",
      "\n",
      "Epoch 19: val_accuracy did not improve from 0.97683\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9904 - loss: 0.0375 - val_accuracy: 0.9768 - val_loss: 0.0857\n",
      "Epoch 20/100\n",
      "\u001b[1m833/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9917 - loss: 0.0352\n",
      "Epoch 20: val_accuracy improved from 0.97683 to 0.97717, saving model to MLP_Models\\Base_MLP_Model_3-lr_0.01-bs_64-20250917-082144\\best_model_epoch-20_val_acc-0.9772.keras\n",
      "\n",
      "Epoch 20: val_accuracy improved from 0.97683 to 0.97717, saving model to wandb_models/Base_MLP_Model_3-lr_0.01-bs_64-20250917-082144/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.9915 - loss: 0.0347 - val_accuracy: 0.9772 - val_loss: 0.0854\n",
      "Epoch 21/100\n",
      "\u001b[1m827/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9925 - loss: 0.0325\n",
      "Epoch 21: val_accuracy did not improve from 0.97717\n",
      "\n",
      "Epoch 21: val_accuracy did not improve from 0.97717\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9923 - loss: 0.0320 - val_accuracy: 0.9772 - val_loss: 0.0855\n",
      "Epoch 22/100\n",
      "\u001b[1m836/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9932 - loss: 0.0300\n",
      "Epoch 22: val_accuracy did not improve from 0.97717\n",
      "\n",
      "Epoch 22: val_accuracy did not improve from 0.97717\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9930 - loss: 0.0296 - val_accuracy: 0.9770 - val_loss: 0.0854\n",
      "Epoch 23/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9939 - loss: 0.0277\n",
      "Epoch 23: val_accuracy improved from 0.97717 to 0.97750, saving model to MLP_Models\\Base_MLP_Model_3-lr_0.01-bs_64-20250917-082144\\best_model_epoch-23_val_acc-0.9775.keras\n",
      "\n",
      "Epoch 23: val_accuracy improved from 0.97717 to 0.97750, saving model to wandb_models/Base_MLP_Model_3-lr_0.01-bs_64-20250917-082144/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.9938 - loss: 0.0273 - val_accuracy: 0.9775 - val_loss: 0.0853\n",
      "Epoch 24/100\n",
      "\u001b[1m835/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9947 - loss: 0.0255\n",
      "Epoch 24: val_accuracy improved from 0.97750 to 0.97767, saving model to MLP_Models\\Base_MLP_Model_3-lr_0.01-bs_64-20250917-082144\\best_model_epoch-24_val_acc-0.9777.keras\n",
      "\n",
      "Epoch 24: val_accuracy improved from 0.97750 to 0.97767, saving model to wandb_models/Base_MLP_Model_3-lr_0.01-bs_64-20250917-082144/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.9946 - loss: 0.0251 - val_accuracy: 0.9777 - val_loss: 0.0853\n",
      "Epoch 25/100\n",
      "\u001b[1m833/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9952 - loss: 0.0236\n",
      "Epoch 25: val_accuracy did not improve from 0.97767\n",
      "\n",
      "Epoch 25: val_accuracy did not improve from 0.97767\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.9952 - loss: 0.0232 - val_accuracy: 0.9775 - val_loss: 0.0853\n",
      "Epoch 26/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9959 - loss: 0.0218\n",
      "Epoch 26: val_accuracy did not improve from 0.97767\n",
      "\n",
      "Epoch 26: val_accuracy did not improve from 0.97767\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.9959 - loss: 0.0214 - val_accuracy: 0.9777 - val_loss: 0.0855\n",
      "Epoch 27/100\n",
      "\u001b[1m825/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9964 - loss: 0.0201\n",
      "Epoch 27: val_accuracy did not improve from 0.97767\n",
      "\n",
      "Epoch 27: val_accuracy did not improve from 0.97767\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.9964 - loss: 0.0197 - val_accuracy: 0.9773 - val_loss: 0.0856\n",
      "Epoch 28/100\n",
      "\u001b[1m837/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9969 - loss: 0.0185\n",
      "Epoch 28: val_accuracy did not improve from 0.97767\n",
      "\n",
      "Epoch 28: val_accuracy did not improve from 0.97767\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9969 - loss: 0.0182 - val_accuracy: 0.9773 - val_loss: 0.0857\n",
      "Epoch 29/100\n",
      "\u001b[1m830/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9974 - loss: 0.0172\n",
      "Epoch 29: val_accuracy improved from 0.97767 to 0.97800, saving model to MLP_Models\\Base_MLP_Model_3-lr_0.01-bs_64-20250917-082144\\best_model_epoch-29_val_acc-0.9780.keras\n",
      "\n",
      "Epoch 29: val_accuracy improved from 0.97767 to 0.97800, saving model to wandb_models/Base_MLP_Model_3-lr_0.01-bs_64-20250917-082144/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.9973 - loss: 0.0168 - val_accuracy: 0.9780 - val_loss: 0.0860\n",
      "Epoch 30/100\n",
      "\u001b[1m826/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9976 - loss: 0.0159\n",
      "Epoch 30: val_accuracy did not improve from 0.97800\n",
      "\n",
      "Epoch 30: val_accuracy did not improve from 0.97800\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.9976 - loss: 0.0155 - val_accuracy: 0.9778 - val_loss: 0.0864\n",
      "Epoch 31/100\n",
      "\u001b[1m839/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9979 - loss: 0.0146\n",
      "Epoch 31: val_accuracy did not improve from 0.97800\n",
      "\n",
      "Epoch 31: val_accuracy did not improve from 0.97800\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.9980 - loss: 0.0144 - val_accuracy: 0.9777 - val_loss: 0.0866\n",
      "Epoch 32/100\n",
      "\u001b[1m835/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9982 - loss: 0.0135\n",
      "Epoch 32: val_accuracy did not improve from 0.97800\n",
      "\n",
      "Epoch 32: val_accuracy did not improve from 0.97800\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.9982 - loss: 0.0133 - val_accuracy: 0.9778 - val_loss: 0.0869\n",
      "Epoch 33/100\n",
      "\u001b[1m824/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9986 - loss: 0.0125\n",
      "Epoch 33: val_accuracy improved from 0.97800 to 0.97833, saving model to MLP_Models\\Base_MLP_Model_3-lr_0.01-bs_64-20250917-082144\\best_model_epoch-33_val_acc-0.9783.keras\n",
      "\n",
      "Epoch 33: val_accuracy improved from 0.97800 to 0.97833, saving model to wandb_models/Base_MLP_Model_3-lr_0.01-bs_64-20250917-082144/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.9985 - loss: 0.0123 - val_accuracy: 0.9783 - val_loss: 0.0872\n",
      "Epoch 34/100\n",
      "\u001b[1m835/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9988 - loss: 0.0115\n",
      "Epoch 34: val_accuracy did not improve from 0.97833\n",
      "\n",
      "Epoch 34: val_accuracy did not improve from 0.97833\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9987 - loss: 0.0114 - val_accuracy: 0.9783 - val_loss: 0.0876\n",
      "Epoch 35/100\n",
      "\u001b[1m827/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9990 - loss: 0.0106\n",
      "Epoch 35: val_accuracy did not improve from 0.97833\n",
      "\n",
      "Epoch 35: val_accuracy did not improve from 0.97833\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9990 - loss: 0.0105 - val_accuracy: 0.9782 - val_loss: 0.0880\n",
      "Epoch 36/100\n",
      "\u001b[1m833/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9992 - loss: 0.0098\n",
      "Epoch 36: val_accuracy did not improve from 0.97833\n",
      "\n",
      "Epoch 36: val_accuracy did not improve from 0.97833\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9992 - loss: 0.0098 - val_accuracy: 0.9777 - val_loss: 0.0885\n",
      "Epoch 37/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9992 - loss: 0.0091\n",
      "Epoch 37: val_accuracy did not improve from 0.97833\n",
      "\n",
      "Epoch 37: val_accuracy did not improve from 0.97833\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9993 - loss: 0.0091 - val_accuracy: 0.9777 - val_loss: 0.0890\n",
      "Epoch 38/100\n",
      "\u001b[1m836/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9994 - loss: 0.0084\n",
      "Epoch 38: val_accuracy did not improve from 0.97833\n",
      "\n",
      "Epoch 38: val_accuracy did not improve from 0.97833\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.9994 - loss: 0.0084 - val_accuracy: 0.9777 - val_loss: 0.0894\n",
      "Epoch 39/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9994 - loss: 0.0078\n",
      "Epoch 39: val_accuracy did not improve from 0.97833\n",
      "\n",
      "Epoch 39: val_accuracy did not improve from 0.97833\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9995 - loss: 0.0078 - val_accuracy: 0.9780 - val_loss: 0.0900\n",
      "Epoch 40/100\n",
      "\u001b[1m839/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9996 - loss: 0.0072\n",
      "Epoch 40: val_accuracy improved from 0.97833 to 0.97850, saving model to MLP_Models\\Base_MLP_Model_3-lr_0.01-bs_64-20250917-082144\\best_model_epoch-40_val_acc-0.9785.keras\n",
      "\n",
      "Epoch 40: val_accuracy improved from 0.97833 to 0.97850, saving model to wandb_models/Base_MLP_Model_3-lr_0.01-bs_64-20250917-082144/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.9995 - loss: 0.0073 - val_accuracy: 0.9785 - val_loss: 0.0905\n",
      "Epoch 41/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9996 - loss: 0.0067\n",
      "Epoch 41: val_accuracy did not improve from 0.97850\n",
      "\n",
      "Epoch 41: val_accuracy did not improve from 0.97850\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9996 - loss: 0.0068 - val_accuracy: 0.9785 - val_loss: 0.0911\n",
      "Epoch 42/100\n",
      "\u001b[1m834/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9997 - loss: 0.0063\n",
      "Epoch 42: val_accuracy improved from 0.97850 to 0.97883, saving model to MLP_Models\\Base_MLP_Model_3-lr_0.01-bs_64-20250917-082144\\best_model_epoch-42_val_acc-0.9788.keras\n",
      "\n",
      "Epoch 42: val_accuracy improved from 0.97850 to 0.97883, saving model to wandb_models/Base_MLP_Model_3-lr_0.01-bs_64-20250917-082144/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.9996 - loss: 0.0064 - val_accuracy: 0.9788 - val_loss: 0.0916\n",
      "Epoch 43/100\n",
      "\u001b[1m840/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9998 - loss: 0.0059\n",
      "Epoch 43: val_accuracy did not improve from 0.97883\n",
      "\n",
      "Epoch 43: val_accuracy did not improve from 0.97883\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.9997 - loss: 0.0060 - val_accuracy: 0.9788 - val_loss: 0.0922\n",
      "Epoch 44/100\n",
      "\u001b[1m834/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9998 - loss: 0.0055\n",
      "Epoch 44: val_accuracy did not improve from 0.97883\n",
      "\n",
      "Epoch 44: val_accuracy did not improve from 0.97883\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9998 - loss: 0.0056 - val_accuracy: 0.9788 - val_loss: 0.0927\n",
      "Epoch 45/100\n",
      "\u001b[1m833/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9998 - loss: 0.0051\n",
      "Epoch 45: val_accuracy improved from 0.97883 to 0.97900, saving model to MLP_Models\\Base_MLP_Model_3-lr_0.01-bs_64-20250917-082144\\best_model_epoch-45_val_acc-0.9790.keras\n",
      "\n",
      "Epoch 45: val_accuracy improved from 0.97883 to 0.97900, saving model to wandb_models/Base_MLP_Model_3-lr_0.01-bs_64-20250917-082144/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9998 - loss: 0.0053 - val_accuracy: 0.9790 - val_loss: 0.0934\n",
      "Epoch 46/100\n",
      "\u001b[1m838/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9999 - loss: 0.0048\n",
      "Epoch 46: val_accuracy did not improve from 0.97900\n",
      "\n",
      "Epoch 46: val_accuracy did not improve from 0.97900\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.9998 - loss: 0.0050 - val_accuracy: 0.9787 - val_loss: 0.0939\n",
      "Epoch 47/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9999 - loss: 0.0046\n",
      "Epoch 47: val_accuracy did not improve from 0.97900\n",
      "\n",
      "Epoch 47: val_accuracy did not improve from 0.97900\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.9999 - loss: 0.0047 - val_accuracy: 0.9788 - val_loss: 0.0944\n",
      "Epoch 48/100\n",
      "\u001b[1m826/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9999 - loss: 0.0043\n",
      "Epoch 48: val_accuracy did not improve from 0.97900\n",
      "\n",
      "Epoch 48: val_accuracy did not improve from 0.97900\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9999 - loss: 0.0044 - val_accuracy: 0.9788 - val_loss: 0.0950\n",
      "Epoch 49/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9999 - loss: 0.0041\n",
      "Epoch 49: val_accuracy did not improve from 0.97900\n",
      "\n",
      "Epoch 49: val_accuracy did not improve from 0.97900\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9999 - loss: 0.0042 - val_accuracy: 0.9788 - val_loss: 0.0955\n",
      "Epoch 50/100\n",
      "\u001b[1m840/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9999 - loss: 0.0039\n",
      "Epoch 50: val_accuracy did not improve from 0.97900\n",
      "\n",
      "Epoch 50: val_accuracy did not improve from 0.97900\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.9999 - loss: 0.0040 - val_accuracy: 0.9788 - val_loss: 0.0960\n",
      "Epoch 51/100\n",
      "\u001b[1m826/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9999 - loss: 0.0037\n",
      "Epoch 51: val_accuracy did not improve from 0.97900\n",
      "\n",
      "Epoch 51: val_accuracy did not improve from 0.97900\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.9999 - loss: 0.0038 - val_accuracy: 0.9788 - val_loss: 0.0965\n",
      "Epoch 52/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9999 - loss: 0.0035\n",
      "Epoch 52: val_accuracy did not improve from 0.97900\n",
      "\n",
      "Epoch 52: val_accuracy did not improve from 0.97900\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9999 - loss: 0.0036 - val_accuracy: 0.9788 - val_loss: 0.0971\n",
      "Epoch 53/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9999 - loss: 0.0034\n",
      "Epoch 53: val_accuracy did not improve from 0.97900\n",
      "\n",
      "Epoch 53: val_accuracy did not improve from 0.97900\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9999 - loss: 0.0034 - val_accuracy: 0.9787 - val_loss: 0.0976\n",
      "Epoch 54/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9999 - loss: 0.0032\n",
      "Epoch 54: val_accuracy did not improve from 0.97900\n",
      "\n",
      "Epoch 54: val_accuracy did not improve from 0.97900\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9999 - loss: 0.0032 - val_accuracy: 0.9785 - val_loss: 0.0981\n",
      "Epoch 55/100\n",
      "\u001b[1m827/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 0.0031\n",
      "Epoch 55: val_accuracy did not improve from 0.97900\n",
      "\n",
      "Epoch 55: val_accuracy did not improve from 0.97900\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.0031 - val_accuracy: 0.9787 - val_loss: 0.0986\n",
      "\n",
      "Training history saved to: MLP_Models\\Base_MLP_Model_3-lr_0.01-bs_64-20250917-082144\\training_history.pkl\n",
      "\n",
      "--- Peak Performance Summary ---\n",
      "Best validation accuracy:           0.9790\n",
      "Associated training accuracy:       0.9998\n",
      "Occurred at epoch:                  45\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch/accuracy</td><td>▁▅▆▆▆▇▇▇▇▇▇▇▇▇██████████████████████████</td></tr><tr><td>epoch/epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇███</td></tr><tr><td>epoch/learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/loss</td><td>█▄▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/val_accuracy</td><td>▁▄▅▅▆▇▇▇▇▇▇▇▇▇▇█▇███████████████████████</td></tr><tr><td>epoch/val_loss</td><td>█▅▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch/accuracy</td><td>0.99996</td></tr><tr><td>epoch/epoch</td><td>54</td></tr><tr><td>epoch/learning_rate</td><td>0.01</td></tr><tr><td>epoch/loss</td><td>0.0031</td></tr><tr><td>epoch/val_accuracy</td><td>0.97867</td></tr><tr><td>epoch/val_loss</td><td>0.09859</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Base_MLP_Model_3-lr_0.01-bs_64-20250917-082144</strong> at: <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/u2vcw1cg' target=\"_blank\">https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/u2vcw1cg</a><br> View project at: <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2' target=\"_blank\">https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2</a><br>Synced 5 W&B file(s), 0 media file(s), 44 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250917_082144-u2vcw1cg\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Adapting the normalisation layer...\n",
      "Adaptation complete.\n",
      "\n",
      "\n",
      "--- Starting Experiment: Wide_MLP_Model ---\n",
      "\n",
      "--- Model Architecture ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"Wide_MLP_Model\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"Wide_MLP_Model\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ normalization_12                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Normalization</span>)                 │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_48 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">401,920</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_49 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │       <span style=\"color: #00af00; text-decoration-color: #00af00\">525,312</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_50 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">262,400</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_51 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,570</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ normalization_12                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m1\u001b[0m)      │             \u001b[38;5;34m3\u001b[0m │\n",
       "│ (\u001b[38;5;33mNormalization\u001b[0m)                 │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_12 (\u001b[38;5;33mFlatten\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m784\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_48 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │       \u001b[38;5;34m401,920\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_49 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │       \u001b[38;5;34m525,312\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_50 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m262,400\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_51 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │         \u001b[38;5;34m2,570\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,192,205</span> (4.55 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,192,205\u001b[0m (4.55 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,192,202</span> (4.55 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,192,202\u001b[0m (4.55 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3</span> (16.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m3\u001b[0m (16.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Hyperparameters ---\n",
      "optimiser           : adamw\n",
      "learning_rate       : 0.001\n",
      "epochs              : 100\n",
      "batch_size          : 64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "creating run (0.0s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\TimVos\\VSC Projects\\CSE5ML\\Assessment 2\\wandb\\run-20250917_082538-un8s928h</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/un8s928h' target=\"_blank\">Wide_MLP_Model-lr_0.001-bs_64-20250917-082538</a></strong> to <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2' target=\"_blank\">https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/un8s928h' target=\"_blank\">https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/un8s928h</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8934 - loss: 0.3411\n",
      "Epoch 1: val_accuracy improved from None to 0.96817, saving model to MLP_Models\\Wide_MLP_Model-lr_0.001-bs_64-20250917-082538\\best_model_epoch-01_val_acc-0.9682.keras\n",
      "\n",
      "Epoch 1: val_accuracy improved from None to 0.96817, saving model to wandb_models/Wide_MLP_Model-lr_0.001-bs_64-20250917-082538/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 16ms/step - accuracy: 0.9358 - loss: 0.2072 - val_accuracy: 0.9682 - val_loss: 0.1047\n",
      "Epoch 2/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9677 - loss: 0.1062\n",
      "Epoch 2: val_accuracy improved from 0.96817 to 0.97350, saving model to MLP_Models\\Wide_MLP_Model-lr_0.001-bs_64-20250917-082538\\best_model_epoch-02_val_acc-0.9735.keras\n",
      "\n",
      "Epoch 2: val_accuracy improved from 0.96817 to 0.97350, saving model to wandb_models/Wide_MLP_Model-lr_0.001-bs_64-20250917-082538/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 15ms/step - accuracy: 0.9721 - loss: 0.0923 - val_accuracy: 0.9735 - val_loss: 0.0867\n",
      "Epoch 3/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9780 - loss: 0.0696\n",
      "Epoch 3: val_accuracy did not improve from 0.97350\n",
      "\n",
      "Epoch 3: val_accuracy did not improve from 0.97350\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 15ms/step - accuracy: 0.9787 - loss: 0.0671 - val_accuracy: 0.9722 - val_loss: 0.0923\n",
      "Epoch 4/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9802 - loss: 0.0608\n",
      "Epoch 4: val_accuracy improved from 0.97350 to 0.97433, saving model to MLP_Models\\Wide_MLP_Model-lr_0.001-bs_64-20250917-082538\\best_model_epoch-04_val_acc-0.9743.keras\n",
      "\n",
      "Epoch 4: val_accuracy improved from 0.97350 to 0.97433, saving model to wandb_models/Wide_MLP_Model-lr_0.001-bs_64-20250917-082538/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 17ms/step - accuracy: 0.9818 - loss: 0.0566 - val_accuracy: 0.9743 - val_loss: 0.0968\n",
      "Epoch 5/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9873 - loss: 0.0444\n",
      "Epoch 5: val_accuracy improved from 0.97433 to 0.97683, saving model to MLP_Models\\Wide_MLP_Model-lr_0.001-bs_64-20250917-082538\\best_model_epoch-05_val_acc-0.9768.keras\n",
      "\n",
      "Epoch 5: val_accuracy improved from 0.97433 to 0.97683, saving model to wandb_models/Wide_MLP_Model-lr_0.001-bs_64-20250917-082538/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.9869 - loss: 0.0440 - val_accuracy: 0.9768 - val_loss: 0.0880\n",
      "Epoch 6/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9875 - loss: 0.0386\n",
      "Epoch 6: val_accuracy improved from 0.97683 to 0.97983, saving model to MLP_Models\\Wide_MLP_Model-lr_0.001-bs_64-20250917-082538\\best_model_epoch-06_val_acc-0.9798.keras\n",
      "\n",
      "Epoch 6: val_accuracy improved from 0.97683 to 0.97983, saving model to wandb_models/Wide_MLP_Model-lr_0.001-bs_64-20250917-082538/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.9872 - loss: 0.0398 - val_accuracy: 0.9798 - val_loss: 0.0857\n",
      "Epoch 7/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9895 - loss: 0.0333\n",
      "Epoch 7: val_accuracy did not improve from 0.97983\n",
      "\n",
      "Epoch 7: val_accuracy did not improve from 0.97983\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 15ms/step - accuracy: 0.9894 - loss: 0.0341 - val_accuracy: 0.9777 - val_loss: 0.0910\n",
      "Epoch 8/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9899 - loss: 0.0337\n",
      "Epoch 8: val_accuracy did not improve from 0.97983\n",
      "\n",
      "Epoch 8: val_accuracy did not improve from 0.97983\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 15ms/step - accuracy: 0.9902 - loss: 0.0325 - val_accuracy: 0.9783 - val_loss: 0.0992\n",
      "Epoch 9/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9914 - loss: 0.0289\n",
      "Epoch 9: val_accuracy did not improve from 0.97983\n",
      "\n",
      "Epoch 9: val_accuracy did not improve from 0.97983\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.9910 - loss: 0.0299 - val_accuracy: 0.9782 - val_loss: 0.1007\n",
      "Epoch 10/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9913 - loss: 0.0287\n",
      "Epoch 10: val_accuracy did not improve from 0.97983\n",
      "\n",
      "Epoch 10: val_accuracy did not improve from 0.97983\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 15ms/step - accuracy: 0.9912 - loss: 0.0308 - val_accuracy: 0.9785 - val_loss: 0.0926\n",
      "Epoch 11/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9927 - loss: 0.0247\n",
      "Epoch 11: val_accuracy did not improve from 0.97983\n",
      "\n",
      "Epoch 11: val_accuracy did not improve from 0.97983\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 15ms/step - accuracy: 0.9928 - loss: 0.0247 - val_accuracy: 0.9755 - val_loss: 0.1287\n",
      "Epoch 12/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9927 - loss: 0.0230\n",
      "Epoch 12: val_accuracy improved from 0.97983 to 0.98117, saving model to MLP_Models\\Wide_MLP_Model-lr_0.001-bs_64-20250917-082538\\best_model_epoch-12_val_acc-0.9812.keras\n",
      "\n",
      "Epoch 12: val_accuracy improved from 0.97983 to 0.98117, saving model to wandb_models/Wide_MLP_Model-lr_0.001-bs_64-20250917-082538/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 15ms/step - accuracy: 0.9933 - loss: 0.0218 - val_accuracy: 0.9812 - val_loss: 0.0821\n",
      "Epoch 13/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9924 - loss: 0.0268\n",
      "Epoch 13: val_accuracy did not improve from 0.98117\n",
      "\n",
      "Epoch 13: val_accuracy did not improve from 0.98117\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 15ms/step - accuracy: 0.9927 - loss: 0.0254 - val_accuracy: 0.9797 - val_loss: 0.1055\n",
      "Epoch 14/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9943 - loss: 0.0211\n",
      "Epoch 14: val_accuracy did not improve from 0.98117\n",
      "\n",
      "Epoch 14: val_accuracy did not improve from 0.98117\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 15ms/step - accuracy: 0.9940 - loss: 0.0221 - val_accuracy: 0.9793 - val_loss: 0.1226\n",
      "Epoch 15/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9930 - loss: 0.0244\n",
      "Epoch 15: val_accuracy did not improve from 0.98117\n",
      "\n",
      "Epoch 15: val_accuracy did not improve from 0.98117\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 15ms/step - accuracy: 0.9932 - loss: 0.0230 - val_accuracy: 0.9787 - val_loss: 0.1195\n",
      "Epoch 16/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9945 - loss: 0.0182\n",
      "Epoch 16: val_accuracy did not improve from 0.98117\n",
      "\n",
      "Epoch 16: val_accuracy did not improve from 0.98117\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 15ms/step - accuracy: 0.9941 - loss: 0.0200 - val_accuracy: 0.9808 - val_loss: 0.1024\n",
      "Epoch 17/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9955 - loss: 0.0158\n",
      "Epoch 17: val_accuracy did not improve from 0.98117\n",
      "\n",
      "Epoch 17: val_accuracy did not improve from 0.98117\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 15ms/step - accuracy: 0.9954 - loss: 0.0166 - val_accuracy: 0.9787 - val_loss: 0.1343\n",
      "Epoch 18/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9936 - loss: 0.0215\n",
      "Epoch 18: val_accuracy did not improve from 0.98117\n",
      "\n",
      "Epoch 18: val_accuracy did not improve from 0.98117\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.9941 - loss: 0.0194 - val_accuracy: 0.9805 - val_loss: 0.1136\n",
      "Epoch 19/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9946 - loss: 0.0204\n",
      "Epoch 19: val_accuracy improved from 0.98117 to 0.98200, saving model to MLP_Models\\Wide_MLP_Model-lr_0.001-bs_64-20250917-082538\\best_model_epoch-19_val_acc-0.9820.keras\n",
      "\n",
      "Epoch 19: val_accuracy improved from 0.98117 to 0.98200, saving model to wandb_models/Wide_MLP_Model-lr_0.001-bs_64-20250917-082538/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 16ms/step - accuracy: 0.9945 - loss: 0.0202 - val_accuracy: 0.9820 - val_loss: 0.1006\n",
      "Epoch 20/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9956 - loss: 0.0163\n",
      "Epoch 20: val_accuracy did not improve from 0.98200\n",
      "\n",
      "Epoch 20: val_accuracy did not improve from 0.98200\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 22ms/step - accuracy: 0.9954 - loss: 0.0172 - val_accuracy: 0.9790 - val_loss: 0.0956\n",
      "Epoch 21/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9959 - loss: 0.0148\n",
      "Epoch 21: val_accuracy did not improve from 0.98200\n",
      "\n",
      "Epoch 21: val_accuracy did not improve from 0.98200\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 18ms/step - accuracy: 0.9960 - loss: 0.0153 - val_accuracy: 0.9803 - val_loss: 0.1129\n",
      "Epoch 22/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9944 - loss: 0.0206\n",
      "Epoch 22: val_accuracy improved from 0.98200 to 0.98250, saving model to MLP_Models\\Wide_MLP_Model-lr_0.001-bs_64-20250917-082538\\best_model_epoch-22_val_acc-0.9825.keras\n",
      "\n",
      "Epoch 22: val_accuracy improved from 0.98200 to 0.98250, saving model to wandb_models/Wide_MLP_Model-lr_0.001-bs_64-20250917-082538/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 17ms/step - accuracy: 0.9945 - loss: 0.0200 - val_accuracy: 0.9825 - val_loss: 0.1037\n",
      "Epoch 23/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9950 - loss: 0.0179\n",
      "Epoch 23: val_accuracy did not improve from 0.98250\n",
      "\n",
      "Epoch 23: val_accuracy did not improve from 0.98250\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 16ms/step - accuracy: 0.9953 - loss: 0.0171 - val_accuracy: 0.9807 - val_loss: 0.1346\n",
      "Epoch 24/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9964 - loss: 0.0138\n",
      "Epoch 24: val_accuracy did not improve from 0.98250\n",
      "\n",
      "Epoch 24: val_accuracy did not improve from 0.98250\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 18ms/step - accuracy: 0.9962 - loss: 0.0137 - val_accuracy: 0.9785 - val_loss: 0.1386\n",
      "Epoch 25/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9946 - loss: 0.0205\n",
      "Epoch 25: val_accuracy improved from 0.98250 to 0.98383, saving model to MLP_Models\\Wide_MLP_Model-lr_0.001-bs_64-20250917-082538\\best_model_epoch-25_val_acc-0.9838.keras\n",
      "\n",
      "Epoch 25: val_accuracy improved from 0.98250 to 0.98383, saving model to wandb_models/Wide_MLP_Model-lr_0.001-bs_64-20250917-082538/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 14ms/step - accuracy: 0.9956 - loss: 0.0168 - val_accuracy: 0.9838 - val_loss: 0.1109\n",
      "Epoch 26/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9964 - loss: 0.0151\n",
      "Epoch 26: val_accuracy did not improve from 0.98383\n",
      "\n",
      "Epoch 26: val_accuracy did not improve from 0.98383\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 14ms/step - accuracy: 0.9961 - loss: 0.0156 - val_accuracy: 0.9822 - val_loss: 0.1117\n",
      "Epoch 27/100\n",
      "\u001b[1m840/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9966 - loss: 0.0164\n",
      "Epoch 27: val_accuracy did not improve from 0.98383\n",
      "\n",
      "Epoch 27: val_accuracy did not improve from 0.98383\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 14ms/step - accuracy: 0.9965 - loss: 0.0158 - val_accuracy: 0.9797 - val_loss: 0.1329\n",
      "Epoch 28/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9948 - loss: 0.0222\n",
      "Epoch 28: val_accuracy did not improve from 0.98383\n",
      "\n",
      "Epoch 28: val_accuracy did not improve from 0.98383\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 14ms/step - accuracy: 0.9955 - loss: 0.0179 - val_accuracy: 0.9822 - val_loss: 0.1046\n",
      "Epoch 29/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9976 - loss: 0.0096\n",
      "Epoch 29: val_accuracy did not improve from 0.98383\n",
      "\n",
      "Epoch 29: val_accuracy did not improve from 0.98383\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 14ms/step - accuracy: 0.9971 - loss: 0.0117 - val_accuracy: 0.9825 - val_loss: 0.1215\n",
      "Epoch 30/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9964 - loss: 0.0126\n",
      "Epoch 30: val_accuracy did not improve from 0.98383\n",
      "\n",
      "Epoch 30: val_accuracy did not improve from 0.98383\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 14ms/step - accuracy: 0.9966 - loss: 0.0128 - val_accuracy: 0.9807 - val_loss: 0.1400\n",
      "Epoch 31/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9954 - loss: 0.0218\n",
      "Epoch 31: val_accuracy did not improve from 0.98383\n",
      "\n",
      "Epoch 31: val_accuracy did not improve from 0.98383\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 14ms/step - accuracy: 0.9964 - loss: 0.0165 - val_accuracy: 0.9812 - val_loss: 0.1391\n",
      "Epoch 32/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9961 - loss: 0.0163\n",
      "Epoch 32: val_accuracy did not improve from 0.98383\n",
      "\n",
      "Epoch 32: val_accuracy did not improve from 0.98383\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 16ms/step - accuracy: 0.9967 - loss: 0.0134 - val_accuracy: 0.9820 - val_loss: 0.1469\n",
      "Epoch 33/100\n",
      "\u001b[1m840/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9969 - loss: 0.0127\n",
      "Epoch 33: val_accuracy did not improve from 0.98383\n",
      "\n",
      "Epoch 33: val_accuracy did not improve from 0.98383\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.9969 - loss: 0.0127 - val_accuracy: 0.9815 - val_loss: 0.1476\n",
      "Epoch 34/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9959 - loss: 0.0184\n",
      "Epoch 34: val_accuracy did not improve from 0.98383\n",
      "\n",
      "Epoch 34: val_accuracy did not improve from 0.98383\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 15ms/step - accuracy: 0.9961 - loss: 0.0175 - val_accuracy: 0.9805 - val_loss: 0.1227\n",
      "Epoch 35/100\n",
      "\u001b[1m840/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9963 - loss: 0.0132\n",
      "Epoch 35: val_accuracy did not improve from 0.98383\n",
      "\n",
      "Epoch 35: val_accuracy did not improve from 0.98383\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.9966 - loss: 0.0137 - val_accuracy: 0.9815 - val_loss: 0.1436\n",
      "\n",
      "Training history saved to: MLP_Models\\Wide_MLP_Model-lr_0.001-bs_64-20250917-082538\\training_history.pkl\n",
      "\n",
      "--- Peak Performance Summary ---\n",
      "Best validation accuracy:           0.9838\n",
      "Associated training accuracy:       0.9956\n",
      "Occurred at epoch:                  25\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch/accuracy</td><td>▁▅▆▆▇▇▇▇▇▇█████████████████████████</td></tr><tr><td>epoch/epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>epoch/learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/loss</td><td>█▄▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/val_accuracy</td><td>▁▃▃▄▅▆▅▆▅▆▄▇▆▆▆▇▆▇▇▆▆▇▇▆█▇▆▇▇▇▇▇▇▇▇</td></tr><tr><td>epoch/val_loss</td><td>▃▁▂▃▂▁▂▃▃▂▆▁▃▅▅▃▇▄▃▂▄▃▇▇▄▄▆▃▅▇▇██▅█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch/accuracy</td><td>0.99659</td></tr><tr><td>epoch/epoch</td><td>34</td></tr><tr><td>epoch/learning_rate</td><td>0.001</td></tr><tr><td>epoch/loss</td><td>0.01373</td></tr><tr><td>epoch/val_accuracy</td><td>0.9815</td></tr><tr><td>epoch/val_loss</td><td>0.14364</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Wide_MLP_Model-lr_0.001-bs_64-20250917-082538</strong> at: <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/un8s928h' target=\"_blank\">https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/un8s928h</a><br> View project at: <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2' target=\"_blank\">https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2</a><br>Synced 5 W&B file(s), 0 media file(s), 18 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250917_082538-un8s928h\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Adapting the normalisation layer...\n",
      "Adaptation complete.\n",
      "\n",
      "\n",
      "--- Starting Experiment: Wide_MLP_Model ---\n",
      "\n",
      "--- Model Architecture ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"Wide_MLP_Model\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"Wide_MLP_Model\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ normalization_13                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Normalization</span>)                 │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_52 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">401,920</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_53 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │       <span style=\"color: #00af00; text-decoration-color: #00af00\">525,312</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_54 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">262,400</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_55 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,570</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ normalization_13                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m1\u001b[0m)      │             \u001b[38;5;34m3\u001b[0m │\n",
       "│ (\u001b[38;5;33mNormalization\u001b[0m)                 │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_13 (\u001b[38;5;33mFlatten\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m784\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_52 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │       \u001b[38;5;34m401,920\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_53 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │       \u001b[38;5;34m525,312\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_54 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m262,400\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_55 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │         \u001b[38;5;34m2,570\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,192,205</span> (4.55 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,192,205\u001b[0m (4.55 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,192,202</span> (4.55 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,192,202\u001b[0m (4.55 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3</span> (16.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m3\u001b[0m (16.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Hyperparameters ---\n",
      "optimiser           : adamw\n",
      "learning_rate       : 0.01\n",
      "epochs              : 100\n",
      "batch_size          : 64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "creating run (0.1s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\TimVos\\VSC Projects\\CSE5ML\\Assessment 2\\wandb\\run-20250917_083319-uzuxcim5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/uzuxcim5' target=\"_blank\">Wide_MLP_Model-lr_0.01-bs_64-20250917-083319</a></strong> to <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2' target=\"_blank\">https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/uzuxcim5' target=\"_blank\">https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/uzuxcim5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8103 - loss: 0.9786\n",
      "Epoch 1: val_accuracy improved from None to 0.93100, saving model to MLP_Models\\Wide_MLP_Model-lr_0.01-bs_64-20250917-083319\\best_model_epoch-01_val_acc-0.9310.keras\n",
      "\n",
      "Epoch 1: val_accuracy improved from None to 0.93100, saving model to wandb_models/Wide_MLP_Model-lr_0.01-bs_64-20250917-083319/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 15ms/step - accuracy: 0.8782 - loss: 0.4879 - val_accuracy: 0.9310 - val_loss: 0.2454\n",
      "Epoch 2/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9161 - loss: 0.3228\n",
      "Epoch 2: val_accuracy improved from 0.93100 to 0.94050, saving model to MLP_Models\\Wide_MLP_Model-lr_0.01-bs_64-20250917-083319\\best_model_epoch-02_val_acc-0.9405.keras\n",
      "\n",
      "Epoch 2: val_accuracy improved from 0.93100 to 0.94050, saving model to wandb_models/Wide_MLP_Model-lr_0.01-bs_64-20250917-083319/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.9188 - loss: 0.3072 - val_accuracy: 0.9405 - val_loss: 0.2135\n",
      "Epoch 3/100\n",
      "\u001b[1m840/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9278 - loss: 0.2719\n",
      "Epoch 3: val_accuracy improved from 0.94050 to 0.94467, saving model to MLP_Models\\Wide_MLP_Model-lr_0.01-bs_64-20250917-083319\\best_model_epoch-03_val_acc-0.9447.keras\n",
      "\n",
      "Epoch 3: val_accuracy improved from 0.94050 to 0.94467, saving model to wandb_models/Wide_MLP_Model-lr_0.01-bs_64-20250917-083319/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.9279 - loss: 0.2657 - val_accuracy: 0.9447 - val_loss: 0.2282\n",
      "Epoch 4/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9337 - loss: 0.2494\n",
      "Epoch 4: val_accuracy improved from 0.94467 to 0.94900, saving model to MLP_Models\\Wide_MLP_Model-lr_0.01-bs_64-20250917-083319\\best_model_epoch-04_val_acc-0.9490.keras\n",
      "\n",
      "Epoch 4: val_accuracy improved from 0.94467 to 0.94900, saving model to wandb_models/Wide_MLP_Model-lr_0.01-bs_64-20250917-083319/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.9336 - loss: 0.2480 - val_accuracy: 0.9490 - val_loss: 0.2121\n",
      "Epoch 5/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9364 - loss: 0.2360\n",
      "Epoch 5: val_accuracy did not improve from 0.94900\n",
      "\n",
      "Epoch 5: val_accuracy did not improve from 0.94900\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 14ms/step - accuracy: 0.9355 - loss: 0.2394 - val_accuracy: 0.9473 - val_loss: 0.2074\n",
      "Epoch 6/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9415 - loss: 0.2265\n",
      "Epoch 6: val_accuracy improved from 0.94900 to 0.95317, saving model to MLP_Models\\Wide_MLP_Model-lr_0.01-bs_64-20250917-083319\\best_model_epoch-06_val_acc-0.9532.keras\n",
      "\n",
      "Epoch 6: val_accuracy improved from 0.94900 to 0.95317, saving model to wandb_models/Wide_MLP_Model-lr_0.01-bs_64-20250917-083319/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.9409 - loss: 0.2228 - val_accuracy: 0.9532 - val_loss: 0.1875\n",
      "Epoch 7/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9424 - loss: 0.2176\n",
      "Epoch 7: val_accuracy improved from 0.95317 to 0.95483, saving model to MLP_Models\\Wide_MLP_Model-lr_0.01-bs_64-20250917-083319\\best_model_epoch-07_val_acc-0.9548.keras\n",
      "\n",
      "Epoch 7: val_accuracy improved from 0.95317 to 0.95483, saving model to wandb_models/Wide_MLP_Model-lr_0.01-bs_64-20250917-083319/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 17ms/step - accuracy: 0.9429 - loss: 0.2142 - val_accuracy: 0.9548 - val_loss: 0.1869\n",
      "Epoch 8/100\n",
      "\u001b[1m840/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9409 - loss: 0.2277\n",
      "Epoch 8: val_accuracy improved from 0.95483 to 0.95750, saving model to MLP_Models\\Wide_MLP_Model-lr_0.01-bs_64-20250917-083319\\best_model_epoch-08_val_acc-0.9575.keras\n",
      "\n",
      "Epoch 8: val_accuracy improved from 0.95483 to 0.95750, saving model to wandb_models/Wide_MLP_Model-lr_0.01-bs_64-20250917-083319/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 15ms/step - accuracy: 0.9415 - loss: 0.2233 - val_accuracy: 0.9575 - val_loss: 0.1676\n",
      "Epoch 9/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9470 - loss: 0.2059\n",
      "Epoch 9: val_accuracy did not improve from 0.95750\n",
      "\n",
      "Epoch 9: val_accuracy did not improve from 0.95750\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 15ms/step - accuracy: 0.9474 - loss: 0.2057 - val_accuracy: 0.9423 - val_loss: 0.2395\n",
      "Epoch 10/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9444 - loss: 0.2125\n",
      "Epoch 10: val_accuracy did not improve from 0.95750\n",
      "\n",
      "Epoch 10: val_accuracy did not improve from 0.95750\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 14ms/step - accuracy: 0.9447 - loss: 0.2124 - val_accuracy: 0.9478 - val_loss: 0.1950\n",
      "Epoch 11/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9425 - loss: 0.2141\n",
      "Epoch 11: val_accuracy did not improve from 0.95750\n",
      "\n",
      "Epoch 11: val_accuracy did not improve from 0.95750\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 14ms/step - accuracy: 0.9442 - loss: 0.2127 - val_accuracy: 0.9472 - val_loss: 0.2148\n",
      "Epoch 12/100\n",
      "\u001b[1m840/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9517 - loss: 0.1943\n",
      "Epoch 12: val_accuracy did not improve from 0.95750\n",
      "\n",
      "Epoch 12: val_accuracy did not improve from 0.95750\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.9495 - loss: 0.2005 - val_accuracy: 0.9515 - val_loss: 0.2284\n",
      "Epoch 13/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9489 - loss: 0.1994\n",
      "Epoch 13: val_accuracy did not improve from 0.95750\n",
      "\n",
      "Epoch 13: val_accuracy did not improve from 0.95750\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 18ms/step - accuracy: 0.9501 - loss: 0.1900 - val_accuracy: 0.9560 - val_loss: 0.1774\n",
      "Epoch 14/100\n",
      "\u001b[1m840/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9516 - loss: 0.1900\n",
      "Epoch 14: val_accuracy did not improve from 0.95750\n",
      "\n",
      "Epoch 14: val_accuracy did not improve from 0.95750\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 17ms/step - accuracy: 0.9477 - loss: 0.2126 - val_accuracy: 0.9463 - val_loss: 0.2260\n",
      "Epoch 15/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9494 - loss: 0.1990\n",
      "Epoch 15: val_accuracy did not improve from 0.95750\n",
      "\n",
      "Epoch 15: val_accuracy did not improve from 0.95750\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 21ms/step - accuracy: 0.9518 - loss: 0.1893 - val_accuracy: 0.9533 - val_loss: 0.2054\n",
      "Epoch 16/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9513 - loss: 0.1889\n",
      "Epoch 16: val_accuracy did not improve from 0.95750\n",
      "\n",
      "Epoch 16: val_accuracy did not improve from 0.95750\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 18ms/step - accuracy: 0.9502 - loss: 0.1953 - val_accuracy: 0.9460 - val_loss: 0.2480\n",
      "Epoch 17/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9478 - loss: 0.2109\n",
      "Epoch 17: val_accuracy did not improve from 0.95750\n",
      "\n",
      "Epoch 17: val_accuracy did not improve from 0.95750\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 17ms/step - accuracy: 0.9514 - loss: 0.1942 - val_accuracy: 0.9538 - val_loss: 0.1961\n",
      "Epoch 18/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9547 - loss: 0.1773\n",
      "Epoch 18: val_accuracy did not improve from 0.95750\n",
      "\n",
      "Epoch 18: val_accuracy did not improve from 0.95750\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 17ms/step - accuracy: 0.9531 - loss: 0.1864 - val_accuracy: 0.9547 - val_loss: 0.1876\n",
      "\n",
      "Training history saved to: MLP_Models\\Wide_MLP_Model-lr_0.01-bs_64-20250917-083319\\training_history.pkl\n",
      "\n",
      "--- Peak Performance Summary ---\n",
      "Best validation accuracy:           0.9575\n",
      "Associated training accuracy:       0.9415\n",
      "Occurred at epoch:                  8\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch/accuracy</td><td>▁▅▆▆▆▇▇▇▇▇▇██▇████</td></tr><tr><td>epoch/epoch</td><td>▁▁▂▂▃▃▃▄▄▅▅▆▆▆▇▇██</td></tr><tr><td>epoch/learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/loss</td><td>█▄▃▂▂▂▂▂▁▂▂▁▁▂▁▁▁▁</td></tr><tr><td>epoch/val_accuracy</td><td>▁▄▅▆▅▇▇█▄▅▅▆█▅▇▅▇▇</td></tr><tr><td>epoch/val_loss</td><td>█▅▆▅▄▃▃▁▇▃▅▆▂▆▄█▃▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch/accuracy</td><td>0.95313</td></tr><tr><td>epoch/epoch</td><td>17</td></tr><tr><td>epoch/learning_rate</td><td>0.01</td></tr><tr><td>epoch/loss</td><td>0.1864</td></tr><tr><td>epoch/val_accuracy</td><td>0.95467</td></tr><tr><td>epoch/val_loss</td><td>0.18762</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Wide_MLP_Model-lr_0.01-bs_64-20250917-083319</strong> at: <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/uzuxcim5' target=\"_blank\">https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/uzuxcim5</a><br> View project at: <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2' target=\"_blank\">https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2</a><br>Synced 5 W&B file(s), 0 media file(s), 14 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250917_083319-uzuxcim5\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Adapting the normalisation layer...\n",
      "Adaptation complete.\n",
      "\n",
      "\n",
      "--- Starting Experiment: Wide_MLP_Model ---\n",
      "\n",
      "--- Model Architecture ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"Wide_MLP_Model\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"Wide_MLP_Model\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ normalization_14                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Normalization</span>)                 │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_56 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">401,920</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_57 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │       <span style=\"color: #00af00; text-decoration-color: #00af00\">525,312</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_58 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">262,400</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_59 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,570</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ normalization_14                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m1\u001b[0m)      │             \u001b[38;5;34m3\u001b[0m │\n",
       "│ (\u001b[38;5;33mNormalization\u001b[0m)                 │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_14 (\u001b[38;5;33mFlatten\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m784\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_56 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │       \u001b[38;5;34m401,920\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_57 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │       \u001b[38;5;34m525,312\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_58 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m262,400\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_59 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │         \u001b[38;5;34m2,570\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,192,205</span> (4.55 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,192,205\u001b[0m (4.55 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,192,202</span> (4.55 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,192,202\u001b[0m (4.55 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3</span> (16.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m3\u001b[0m (16.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Hyperparameters ---\n",
      "optimiser           : SGD\n",
      "learning_rate       : 0.001\n",
      "epochs              : 100\n",
      "batch_size          : 64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "creating run (0.1s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\TimVos\\VSC Projects\\CSE5ML\\Assessment 2\\wandb\\run-20250917_083729-6i5ld6xg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/6i5ld6xg' target=\"_blank\">Wide_MLP_Model-lr_0.001-bs_64-20250917-083729</a></strong> to <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2' target=\"_blank\">https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/6i5ld6xg' target=\"_blank\">https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/6i5ld6xg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5317 - loss: 1.6300\n",
      "Epoch 1: val_accuracy improved from None to 0.88433, saving model to MLP_Models\\Wide_MLP_Model-lr_0.001-bs_64-20250917-083729\\best_model_epoch-01_val_acc-0.8843.keras\n",
      "\n",
      "Epoch 1: val_accuracy improved from None to 0.88433, saving model to wandb_models/Wide_MLP_Model-lr_0.001-bs_64-20250917-083729/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 14ms/step - accuracy: 0.7052 - loss: 1.1885 - val_accuracy: 0.8843 - val_loss: 0.5726\n",
      "Epoch 2/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8583 - loss: 0.5722\n",
      "Epoch 2: val_accuracy improved from 0.88433 to 0.91133, saving model to MLP_Models\\Wide_MLP_Model-lr_0.001-bs_64-20250917-083729\\best_model_epoch-02_val_acc-0.9113.keras\n",
      "\n",
      "Epoch 2: val_accuracy improved from 0.88433 to 0.91133, saving model to wandb_models/Wide_MLP_Model-lr_0.001-bs_64-20250917-083729/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - accuracy: 0.8681 - loss: 0.5191 - val_accuracy: 0.9113 - val_loss: 0.3653\n",
      "Epoch 3/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8867 - loss: 0.4133\n",
      "Epoch 3: val_accuracy improved from 0.91133 to 0.92233, saving model to MLP_Models\\Wide_MLP_Model-lr_0.001-bs_64-20250917-083729\\best_model_epoch-03_val_acc-0.9223.keras\n",
      "\n",
      "Epoch 3: val_accuracy improved from 0.91133 to 0.92233, saving model to wandb_models/Wide_MLP_Model-lr_0.001-bs_64-20250917-083729/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 19ms/step - accuracy: 0.8913 - loss: 0.3965 - val_accuracy: 0.9223 - val_loss: 0.3002\n",
      "Epoch 4/100\n",
      "\u001b[1m837/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9006 - loss: 0.3512\n",
      "Epoch 4: val_accuracy improved from 0.92233 to 0.92850, saving model to MLP_Models\\Wide_MLP_Model-lr_0.001-bs_64-20250917-083729\\best_model_epoch-04_val_acc-0.9285.keras\n",
      "\n",
      "Epoch 4: val_accuracy improved from 0.92233 to 0.92850, saving model to wandb_models/Wide_MLP_Model-lr_0.001-bs_64-20250917-083729/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 9ms/step - accuracy: 0.9036 - loss: 0.3431 - val_accuracy: 0.9285 - val_loss: 0.2666\n",
      "Epoch 5/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9103 - loss: 0.3152\n",
      "Epoch 5: val_accuracy improved from 0.92850 to 0.93367, saving model to MLP_Models\\Wide_MLP_Model-lr_0.001-bs_64-20250917-083729\\best_model_epoch-05_val_acc-0.9337.keras\n",
      "\n",
      "Epoch 5: val_accuracy improved from 0.92850 to 0.93367, saving model to wandb_models/Wide_MLP_Model-lr_0.001-bs_64-20250917-083729/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.9121 - loss: 0.3105 - val_accuracy: 0.9337 - val_loss: 0.2447\n",
      "Epoch 6/100\n",
      "\u001b[1m839/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9173 - loss: 0.2903\n",
      "Epoch 6: val_accuracy improved from 0.93367 to 0.93600, saving model to MLP_Models\\Wide_MLP_Model-lr_0.001-bs_64-20250917-083729\\best_model_epoch-06_val_acc-0.9360.keras\n",
      "\n",
      "Epoch 6: val_accuracy improved from 0.93367 to 0.93600, saving model to wandb_models/Wide_MLP_Model-lr_0.001-bs_64-20250917-083729/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - accuracy: 0.9181 - loss: 0.2873 - val_accuracy: 0.9360 - val_loss: 0.2286\n",
      "Epoch 7/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9231 - loss: 0.2714\n",
      "Epoch 7: val_accuracy improved from 0.93600 to 0.93833, saving model to MLP_Models\\Wide_MLP_Model-lr_0.001-bs_64-20250917-083729\\best_model_epoch-07_val_acc-0.9383.keras\n",
      "\n",
      "Epoch 7: val_accuracy improved from 0.93600 to 0.93833, saving model to wandb_models/Wide_MLP_Model-lr_0.001-bs_64-20250917-083729/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 9ms/step - accuracy: 0.9234 - loss: 0.2694 - val_accuracy: 0.9383 - val_loss: 0.2159\n",
      "Epoch 8/100\n",
      "\u001b[1m840/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9276 - loss: 0.2561\n",
      "Epoch 8: val_accuracy improved from 0.93833 to 0.94250, saving model to MLP_Models\\Wide_MLP_Model-lr_0.001-bs_64-20250917-083729\\best_model_epoch-08_val_acc-0.9425.keras\n",
      "\n",
      "Epoch 8: val_accuracy improved from 0.93833 to 0.94250, saving model to wandb_models/Wide_MLP_Model-lr_0.001-bs_64-20250917-083729/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.9280 - loss: 0.2547 - val_accuracy: 0.9425 - val_loss: 0.2053\n",
      "Epoch 9/100\n",
      "\u001b[1m837/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9317 - loss: 0.2432\n",
      "Epoch 9: val_accuracy improved from 0.94250 to 0.94567, saving model to MLP_Models\\Wide_MLP_Model-lr_0.001-bs_64-20250917-083729\\best_model_epoch-09_val_acc-0.9457.keras\n",
      "\n",
      "Epoch 9: val_accuracy improved from 0.94250 to 0.94567, saving model to wandb_models/Wide_MLP_Model-lr_0.001-bs_64-20250917-083729/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 9ms/step - accuracy: 0.9316 - loss: 0.2422 - val_accuracy: 0.9457 - val_loss: 0.1962\n",
      "Epoch 10/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9351 - loss: 0.2321\n",
      "Epoch 10: val_accuracy improved from 0.94567 to 0.94950, saving model to MLP_Models\\Wide_MLP_Model-lr_0.001-bs_64-20250917-083729\\best_model_epoch-10_val_acc-0.9495.keras\n",
      "\n",
      "Epoch 10: val_accuracy improved from 0.94567 to 0.94950, saving model to wandb_models/Wide_MLP_Model-lr_0.001-bs_64-20250917-083729/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 9ms/step - accuracy: 0.9346 - loss: 0.2313 - val_accuracy: 0.9495 - val_loss: 0.1883\n",
      "Epoch 11/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9385 - loss: 0.2222\n",
      "Epoch 11: val_accuracy improved from 0.94950 to 0.95017, saving model to MLP_Models\\Wide_MLP_Model-lr_0.001-bs_64-20250917-083729\\best_model_epoch-11_val_acc-0.9502.keras\n",
      "\n",
      "Epoch 11: val_accuracy improved from 0.94950 to 0.95017, saving model to wandb_models/Wide_MLP_Model-lr_0.001-bs_64-20250917-083729/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 9ms/step - accuracy: 0.9374 - loss: 0.2217 - val_accuracy: 0.9502 - val_loss: 0.1812\n",
      "Epoch 12/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9409 - loss: 0.2134\n",
      "Epoch 12: val_accuracy improved from 0.95017 to 0.95217, saving model to MLP_Models\\Wide_MLP_Model-lr_0.001-bs_64-20250917-083729\\best_model_epoch-12_val_acc-0.9522.keras\n",
      "\n",
      "Epoch 12: val_accuracy improved from 0.95017 to 0.95217, saving model to wandb_models/Wide_MLP_Model-lr_0.001-bs_64-20250917-083729/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 9ms/step - accuracy: 0.9400 - loss: 0.2130 - val_accuracy: 0.9522 - val_loss: 0.1749\n",
      "Epoch 13/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9435 - loss: 0.2054\n",
      "Epoch 13: val_accuracy improved from 0.95217 to 0.95333, saving model to MLP_Models\\Wide_MLP_Model-lr_0.001-bs_64-20250917-083729\\best_model_epoch-13_val_acc-0.9533.keras\n",
      "\n",
      "Epoch 13: val_accuracy improved from 0.95217 to 0.95333, saving model to wandb_models/Wide_MLP_Model-lr_0.001-bs_64-20250917-083729/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 9ms/step - accuracy: 0.9422 - loss: 0.2051 - val_accuracy: 0.9533 - val_loss: 0.1692\n",
      "Epoch 14/100\n",
      "\u001b[1m836/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9455 - loss: 0.1982\n",
      "Epoch 14: val_accuracy improved from 0.95333 to 0.95467, saving model to MLP_Models\\Wide_MLP_Model-lr_0.001-bs_64-20250917-083729\\best_model_epoch-14_val_acc-0.9547.keras\n",
      "\n",
      "Epoch 14: val_accuracy improved from 0.95333 to 0.95467, saving model to wandb_models/Wide_MLP_Model-lr_0.001-bs_64-20250917-083729/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 9ms/step - accuracy: 0.9443 - loss: 0.1978 - val_accuracy: 0.9547 - val_loss: 0.1640\n",
      "Epoch 15/100\n",
      "\u001b[1m838/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9473 - loss: 0.1914\n",
      "Epoch 15: val_accuracy improved from 0.95467 to 0.95600, saving model to MLP_Models\\Wide_MLP_Model-lr_0.001-bs_64-20250917-083729\\best_model_epoch-15_val_acc-0.9560.keras\n",
      "\n",
      "Epoch 15: val_accuracy improved from 0.95467 to 0.95600, saving model to wandb_models/Wide_MLP_Model-lr_0.001-bs_64-20250917-083729/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 9ms/step - accuracy: 0.9460 - loss: 0.1911 - val_accuracy: 0.9560 - val_loss: 0.1593\n",
      "Epoch 16/100\n",
      "\u001b[1m838/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9488 - loss: 0.1852\n",
      "Epoch 16: val_accuracy improved from 0.95600 to 0.95800, saving model to MLP_Models\\Wide_MLP_Model-lr_0.001-bs_64-20250917-083729\\best_model_epoch-16_val_acc-0.9580.keras\n",
      "\n",
      "Epoch 16: val_accuracy improved from 0.95600 to 0.95800, saving model to wandb_models/Wide_MLP_Model-lr_0.001-bs_64-20250917-083729/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - accuracy: 0.9477 - loss: 0.1849 - val_accuracy: 0.9580 - val_loss: 0.1549\n",
      "Epoch 17/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9505 - loss: 0.1795\n",
      "Epoch 17: val_accuracy improved from 0.95800 to 0.95917, saving model to MLP_Models\\Wide_MLP_Model-lr_0.001-bs_64-20250917-083729\\best_model_epoch-17_val_acc-0.9592.keras\n",
      "\n",
      "Epoch 17: val_accuracy improved from 0.95800 to 0.95917, saving model to wandb_models/Wide_MLP_Model-lr_0.001-bs_64-20250917-083729/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - accuracy: 0.9494 - loss: 0.1792 - val_accuracy: 0.9592 - val_loss: 0.1509\n",
      "Epoch 18/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9517 - loss: 0.1741\n",
      "Epoch 18: val_accuracy improved from 0.95917 to 0.95983, saving model to MLP_Models\\Wide_MLP_Model-lr_0.001-bs_64-20250917-083729\\best_model_epoch-18_val_acc-0.9598.keras\n",
      "\n",
      "Epoch 18: val_accuracy improved from 0.95917 to 0.95983, saving model to wandb_models/Wide_MLP_Model-lr_0.001-bs_64-20250917-083729/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 12ms/step - accuracy: 0.9509 - loss: 0.1738 - val_accuracy: 0.9598 - val_loss: 0.1472\n",
      "Epoch 19/100\n",
      "\u001b[1m836/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9531 - loss: 0.1690\n",
      "Epoch 19: val_accuracy improved from 0.95983 to 0.96167, saving model to MLP_Models\\Wide_MLP_Model-lr_0.001-bs_64-20250917-083729\\best_model_epoch-19_val_acc-0.9617.keras\n",
      "\n",
      "Epoch 19: val_accuracy improved from 0.95983 to 0.96167, saving model to wandb_models/Wide_MLP_Model-lr_0.001-bs_64-20250917-083729/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 9ms/step - accuracy: 0.9524 - loss: 0.1688 - val_accuracy: 0.9617 - val_loss: 0.1437\n",
      "Epoch 20/100\n",
      "\u001b[1m838/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9542 - loss: 0.1643\n",
      "Epoch 20: val_accuracy did not improve from 0.96167\n",
      "\n",
      "Epoch 20: val_accuracy did not improve from 0.96167\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 9ms/step - accuracy: 0.9536 - loss: 0.1640 - val_accuracy: 0.9617 - val_loss: 0.1405\n",
      "Epoch 21/100\n",
      "\u001b[1m840/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9553 - loss: 0.1598\n",
      "Epoch 21: val_accuracy improved from 0.96167 to 0.96317, saving model to MLP_Models\\Wide_MLP_Model-lr_0.001-bs_64-20250917-083729\\best_model_epoch-21_val_acc-0.9632.keras\n",
      "\n",
      "Epoch 21: val_accuracy improved from 0.96167 to 0.96317, saving model to wandb_models/Wide_MLP_Model-lr_0.001-bs_64-20250917-083729/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - accuracy: 0.9548 - loss: 0.1595 - val_accuracy: 0.9632 - val_loss: 0.1375\n",
      "Epoch 22/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9569 - loss: 0.1556\n",
      "Epoch 22: val_accuracy improved from 0.96317 to 0.96383, saving model to MLP_Models\\Wide_MLP_Model-lr_0.001-bs_64-20250917-083729\\best_model_epoch-22_val_acc-0.9638.keras\n",
      "\n",
      "Epoch 22: val_accuracy improved from 0.96317 to 0.96383, saving model to wandb_models/Wide_MLP_Model-lr_0.001-bs_64-20250917-083729/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 11ms/step - accuracy: 0.9560 - loss: 0.1553 - val_accuracy: 0.9638 - val_loss: 0.1347\n",
      "Epoch 23/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9579 - loss: 0.1516\n",
      "Epoch 23: val_accuracy improved from 0.96383 to 0.96400, saving model to MLP_Models\\Wide_MLP_Model-lr_0.001-bs_64-20250917-083729\\best_model_epoch-23_val_acc-0.9640.keras\n",
      "\n",
      "Epoch 23: val_accuracy improved from 0.96383 to 0.96400, saving model to wandb_models/Wide_MLP_Model-lr_0.001-bs_64-20250917-083729/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - accuracy: 0.9572 - loss: 0.1513 - val_accuracy: 0.9640 - val_loss: 0.1320\n",
      "Epoch 24/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9591 - loss: 0.1478\n",
      "Epoch 24: val_accuracy improved from 0.96400 to 0.96483, saving model to MLP_Models\\Wide_MLP_Model-lr_0.001-bs_64-20250917-083729\\best_model_epoch-24_val_acc-0.9648.keras\n",
      "\n",
      "Epoch 24: val_accuracy improved from 0.96400 to 0.96483, saving model to wandb_models/Wide_MLP_Model-lr_0.001-bs_64-20250917-083729/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 11ms/step - accuracy: 0.9585 - loss: 0.1474 - val_accuracy: 0.9648 - val_loss: 0.1296\n",
      "Epoch 25/100\n",
      "\u001b[1m840/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9603 - loss: 0.1441\n",
      "Epoch 25: val_accuracy improved from 0.96483 to 0.96600, saving model to MLP_Models\\Wide_MLP_Model-lr_0.001-bs_64-20250917-083729\\best_model_epoch-25_val_acc-0.9660.keras\n",
      "\n",
      "Epoch 25: val_accuracy improved from 0.96483 to 0.96600, saving model to wandb_models/Wide_MLP_Model-lr_0.001-bs_64-20250917-083729/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 11ms/step - accuracy: 0.9595 - loss: 0.1438 - val_accuracy: 0.9660 - val_loss: 0.1272\n",
      "Epoch 26/100\n",
      "\u001b[1m838/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9611 - loss: 0.1407\n",
      "Epoch 26: val_accuracy improved from 0.96600 to 0.96683, saving model to MLP_Models\\Wide_MLP_Model-lr_0.001-bs_64-20250917-083729\\best_model_epoch-26_val_acc-0.9668.keras\n",
      "\n",
      "Epoch 26: val_accuracy improved from 0.96600 to 0.96683, saving model to wandb_models/Wide_MLP_Model-lr_0.001-bs_64-20250917-083729/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 9ms/step - accuracy: 0.9606 - loss: 0.1403 - val_accuracy: 0.9668 - val_loss: 0.1250\n",
      "Epoch 27/100\n",
      "\u001b[1m837/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9621 - loss: 0.1373\n",
      "Epoch 27: val_accuracy improved from 0.96683 to 0.96700, saving model to MLP_Models\\Wide_MLP_Model-lr_0.001-bs_64-20250917-083729\\best_model_epoch-27_val_acc-0.9670.keras\n",
      "\n",
      "Epoch 27: val_accuracy improved from 0.96683 to 0.96700, saving model to wandb_models/Wide_MLP_Model-lr_0.001-bs_64-20250917-083729/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 9ms/step - accuracy: 0.9618 - loss: 0.1370 - val_accuracy: 0.9670 - val_loss: 0.1229\n",
      "Epoch 28/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9631 - loss: 0.1341\n",
      "Epoch 28: val_accuracy improved from 0.96700 to 0.96767, saving model to MLP_Models\\Wide_MLP_Model-lr_0.001-bs_64-20250917-083729\\best_model_epoch-28_val_acc-0.9677.keras\n",
      "\n",
      "Epoch 28: val_accuracy improved from 0.96700 to 0.96767, saving model to wandb_models/Wide_MLP_Model-lr_0.001-bs_64-20250917-083729/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - accuracy: 0.9626 - loss: 0.1338 - val_accuracy: 0.9677 - val_loss: 0.1209\n",
      "Epoch 29/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9640 - loss: 0.1311\n",
      "Epoch 29: val_accuracy improved from 0.96767 to 0.96817, saving model to MLP_Models\\Wide_MLP_Model-lr_0.001-bs_64-20250917-083729\\best_model_epoch-29_val_acc-0.9682.keras\n",
      "\n",
      "Epoch 29: val_accuracy improved from 0.96767 to 0.96817, saving model to wandb_models/Wide_MLP_Model-lr_0.001-bs_64-20250917-083729/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 9ms/step - accuracy: 0.9636 - loss: 0.1308 - val_accuracy: 0.9682 - val_loss: 0.1191\n",
      "Epoch 30/100\n",
      "\u001b[1m840/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9650 - loss: 0.1282\n",
      "Epoch 30: val_accuracy improved from 0.96817 to 0.96900, saving model to MLP_Models\\Wide_MLP_Model-lr_0.001-bs_64-20250917-083729\\best_model_epoch-30_val_acc-0.9690.keras\n",
      "\n",
      "Epoch 30: val_accuracy improved from 0.96817 to 0.96900, saving model to wandb_models/Wide_MLP_Model-lr_0.001-bs_64-20250917-083729/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 12ms/step - accuracy: 0.9644 - loss: 0.1279 - val_accuracy: 0.9690 - val_loss: 0.1173\n",
      "Epoch 31/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9658 - loss: 0.1254\n",
      "Epoch 31: val_accuracy improved from 0.96900 to 0.96983, saving model to MLP_Models\\Wide_MLP_Model-lr_0.001-bs_64-20250917-083729\\best_model_epoch-31_val_acc-0.9698.keras\n",
      "\n",
      "Epoch 31: val_accuracy improved from 0.96900 to 0.96983, saving model to wandb_models/Wide_MLP_Model-lr_0.001-bs_64-20250917-083729/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 12ms/step - accuracy: 0.9653 - loss: 0.1250 - val_accuracy: 0.9698 - val_loss: 0.1156\n",
      "Epoch 32/100\n",
      "\u001b[1m838/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9664 - loss: 0.1227\n",
      "Epoch 32: val_accuracy did not improve from 0.96983\n",
      "\n",
      "Epoch 32: val_accuracy did not improve from 0.96983\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - accuracy: 0.9659 - loss: 0.1223 - val_accuracy: 0.9697 - val_loss: 0.1140\n",
      "Epoch 33/100\n",
      "\u001b[1m839/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9669 - loss: 0.1201\n",
      "Epoch 33: val_accuracy did not improve from 0.96983\n",
      "\n",
      "Epoch 33: val_accuracy did not improve from 0.96983\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 11ms/step - accuracy: 0.9666 - loss: 0.1197 - val_accuracy: 0.9697 - val_loss: 0.1125\n",
      "Epoch 34/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9674 - loss: 0.1175\n",
      "Epoch 34: val_accuracy improved from 0.96983 to 0.97000, saving model to MLP_Models\\Wide_MLP_Model-lr_0.001-bs_64-20250917-083729\\best_model_epoch-34_val_acc-0.9700.keras\n",
      "\n",
      "Epoch 34: val_accuracy improved from 0.96983 to 0.97000, saving model to wandb_models/Wide_MLP_Model-lr_0.001-bs_64-20250917-083729/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 11ms/step - accuracy: 0.9673 - loss: 0.1172 - val_accuracy: 0.9700 - val_loss: 0.1110\n",
      "Epoch 35/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9684 - loss: 0.1151\n",
      "Epoch 35: val_accuracy improved from 0.97000 to 0.97067, saving model to MLP_Models\\Wide_MLP_Model-lr_0.001-bs_64-20250917-083729\\best_model_epoch-35_val_acc-0.9707.keras\n",
      "\n",
      "Epoch 35: val_accuracy improved from 0.97000 to 0.97067, saving model to wandb_models/Wide_MLP_Model-lr_0.001-bs_64-20250917-083729/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - accuracy: 0.9681 - loss: 0.1148 - val_accuracy: 0.9707 - val_loss: 0.1097\n",
      "Epoch 36/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9688 - loss: 0.1128\n",
      "Epoch 36: val_accuracy improved from 0.97067 to 0.97117, saving model to MLP_Models\\Wide_MLP_Model-lr_0.001-bs_64-20250917-083729\\best_model_epoch-36_val_acc-0.9712.keras\n",
      "\n",
      "Epoch 36: val_accuracy improved from 0.97067 to 0.97117, saving model to wandb_models/Wide_MLP_Model-lr_0.001-bs_64-20250917-083729/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 11ms/step - accuracy: 0.9685 - loss: 0.1125 - val_accuracy: 0.9712 - val_loss: 0.1083\n",
      "Epoch 37/100\n",
      "\u001b[1m840/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9697 - loss: 0.1105\n",
      "Epoch 37: val_accuracy improved from 0.97117 to 0.97217, saving model to MLP_Models\\Wide_MLP_Model-lr_0.001-bs_64-20250917-083729\\best_model_epoch-37_val_acc-0.9722.keras\n",
      "\n",
      "Epoch 37: val_accuracy improved from 0.97117 to 0.97217, saving model to wandb_models/Wide_MLP_Model-lr_0.001-bs_64-20250917-083729/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 11ms/step - accuracy: 0.9692 - loss: 0.1102 - val_accuracy: 0.9722 - val_loss: 0.1071\n",
      "Epoch 38/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9702 - loss: 0.1083\n",
      "Epoch 38: val_accuracy did not improve from 0.97217\n",
      "\n",
      "Epoch 38: val_accuracy did not improve from 0.97217\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 9ms/step - accuracy: 0.9697 - loss: 0.1080 - val_accuracy: 0.9718 - val_loss: 0.1058\n",
      "Epoch 39/100\n",
      "\u001b[1m839/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9706 - loss: 0.1062\n",
      "Epoch 39: val_accuracy did not improve from 0.97217\n",
      "\n",
      "Epoch 39: val_accuracy did not improve from 0.97217\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.9703 - loss: 0.1059 - val_accuracy: 0.9720 - val_loss: 0.1047\n",
      "Epoch 40/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9712 - loss: 0.1041\n",
      "Epoch 40: val_accuracy improved from 0.97217 to 0.97267, saving model to MLP_Models\\Wide_MLP_Model-lr_0.001-bs_64-20250917-083729\\best_model_epoch-40_val_acc-0.9727.keras\n",
      "\n",
      "Epoch 40: val_accuracy improved from 0.97217 to 0.97267, saving model to wandb_models/Wide_MLP_Model-lr_0.001-bs_64-20250917-083729/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 11ms/step - accuracy: 0.9709 - loss: 0.1038 - val_accuracy: 0.9727 - val_loss: 0.1035\n",
      "Epoch 41/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9716 - loss: 0.1021\n",
      "Epoch 41: val_accuracy did not improve from 0.97267\n",
      "\n",
      "Epoch 41: val_accuracy did not improve from 0.97267\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - accuracy: 0.9714 - loss: 0.1018 - val_accuracy: 0.9727 - val_loss: 0.1024\n",
      "Epoch 42/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9723 - loss: 0.1001\n",
      "Epoch 42: val_accuracy improved from 0.97267 to 0.97300, saving model to MLP_Models\\Wide_MLP_Model-lr_0.001-bs_64-20250917-083729\\best_model_epoch-42_val_acc-0.9730.keras\n",
      "\n",
      "Epoch 42: val_accuracy improved from 0.97267 to 0.97300, saving model to wandb_models/Wide_MLP_Model-lr_0.001-bs_64-20250917-083729/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.9720 - loss: 0.0999 - val_accuracy: 0.9730 - val_loss: 0.1013\n",
      "Epoch 43/100\n",
      "\u001b[1m840/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9729 - loss: 0.0983\n",
      "Epoch 43: val_accuracy improved from 0.97300 to 0.97317, saving model to MLP_Models\\Wide_MLP_Model-lr_0.001-bs_64-20250917-083729\\best_model_epoch-43_val_acc-0.9732.keras\n",
      "\n",
      "Epoch 43: val_accuracy improved from 0.97300 to 0.97317, saving model to wandb_models/Wide_MLP_Model-lr_0.001-bs_64-20250917-083729/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - accuracy: 0.9726 - loss: 0.0980 - val_accuracy: 0.9732 - val_loss: 0.1003\n",
      "Epoch 44/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9732 - loss: 0.0964\n",
      "Epoch 44: val_accuracy improved from 0.97317 to 0.97367, saving model to MLP_Models\\Wide_MLP_Model-lr_0.001-bs_64-20250917-083729\\best_model_epoch-44_val_acc-0.9737.keras\n",
      "\n",
      "Epoch 44: val_accuracy improved from 0.97317 to 0.97367, saving model to wandb_models/Wide_MLP_Model-lr_0.001-bs_64-20250917-083729/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - accuracy: 0.9730 - loss: 0.0962 - val_accuracy: 0.9737 - val_loss: 0.0994\n",
      "Epoch 45/100\n",
      "\u001b[1m839/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9740 - loss: 0.0946\n",
      "Epoch 45: val_accuracy improved from 0.97367 to 0.97383, saving model to MLP_Models\\Wide_MLP_Model-lr_0.001-bs_64-20250917-083729\\best_model_epoch-45_val_acc-0.9738.keras\n",
      "\n",
      "Epoch 45: val_accuracy improved from 0.97367 to 0.97383, saving model to wandb_models/Wide_MLP_Model-lr_0.001-bs_64-20250917-083729/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 8ms/step - accuracy: 0.9737 - loss: 0.0944 - val_accuracy: 0.9738 - val_loss: 0.0984\n",
      "Epoch 46/100\n",
      "\u001b[1m840/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9746 - loss: 0.0929\n",
      "Epoch 46: val_accuracy improved from 0.97383 to 0.97400, saving model to MLP_Models\\Wide_MLP_Model-lr_0.001-bs_64-20250917-083729\\best_model_epoch-46_val_acc-0.9740.keras\n",
      "\n",
      "Epoch 46: val_accuracy improved from 0.97383 to 0.97400, saving model to wandb_models/Wide_MLP_Model-lr_0.001-bs_64-20250917-083729/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - accuracy: 0.9744 - loss: 0.0926 - val_accuracy: 0.9740 - val_loss: 0.0975\n",
      "Epoch 47/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9751 - loss: 0.0912\n",
      "Epoch 47: val_accuracy improved from 0.97400 to 0.97433, saving model to MLP_Models\\Wide_MLP_Model-lr_0.001-bs_64-20250917-083729\\best_model_epoch-47_val_acc-0.9743.keras\n",
      "\n",
      "Epoch 47: val_accuracy improved from 0.97400 to 0.97433, saving model to wandb_models/Wide_MLP_Model-lr_0.001-bs_64-20250917-083729/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 9ms/step - accuracy: 0.9749 - loss: 0.0910 - val_accuracy: 0.9743 - val_loss: 0.0967\n",
      "Epoch 48/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9759 - loss: 0.0896\n",
      "Epoch 48: val_accuracy did not improve from 0.97433\n",
      "\n",
      "Epoch 48: val_accuracy did not improve from 0.97433\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.9755 - loss: 0.0893 - val_accuracy: 0.9743 - val_loss: 0.0958\n",
      "Epoch 49/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9764 - loss: 0.0880\n",
      "Epoch 49: val_accuracy improved from 0.97433 to 0.97450, saving model to MLP_Models\\Wide_MLP_Model-lr_0.001-bs_64-20250917-083729\\best_model_epoch-49_val_acc-0.9745.keras\n",
      "\n",
      "Epoch 49: val_accuracy improved from 0.97433 to 0.97450, saving model to wandb_models/Wide_MLP_Model-lr_0.001-bs_64-20250917-083729/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 12ms/step - accuracy: 0.9760 - loss: 0.0877 - val_accuracy: 0.9745 - val_loss: 0.0950\n",
      "Epoch 50/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9766 - loss: 0.0864\n",
      "Epoch 50: val_accuracy did not improve from 0.97450\n",
      "\n",
      "Epoch 50: val_accuracy did not improve from 0.97450\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 17ms/step - accuracy: 0.9764 - loss: 0.0862 - val_accuracy: 0.9745 - val_loss: 0.0942\n",
      "Epoch 51/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9772 - loss: 0.0849\n",
      "Epoch 51: val_accuracy improved from 0.97450 to 0.97483, saving model to MLP_Models\\Wide_MLP_Model-lr_0.001-bs_64-20250917-083729\\best_model_epoch-51_val_acc-0.9748.keras\n",
      "\n",
      "Epoch 51: val_accuracy improved from 0.97450 to 0.97483, saving model to wandb_models/Wide_MLP_Model-lr_0.001-bs_64-20250917-083729/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.9769 - loss: 0.0847 - val_accuracy: 0.9748 - val_loss: 0.0935\n",
      "Epoch 52/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9774 - loss: 0.0835\n",
      "Epoch 52: val_accuracy improved from 0.97483 to 0.97517, saving model to MLP_Models\\Wide_MLP_Model-lr_0.001-bs_64-20250917-083729\\best_model_epoch-52_val_acc-0.9752.keras\n",
      "\n",
      "Epoch 52: val_accuracy improved from 0.97483 to 0.97517, saving model to wandb_models/Wide_MLP_Model-lr_0.001-bs_64-20250917-083729/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 11ms/step - accuracy: 0.9773 - loss: 0.0832 - val_accuracy: 0.9752 - val_loss: 0.0927\n",
      "Epoch 53/100\n",
      "\u001b[1m838/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9776 - loss: 0.0820\n",
      "Epoch 53: val_accuracy improved from 0.97517 to 0.97567, saving model to MLP_Models\\Wide_MLP_Model-lr_0.001-bs_64-20250917-083729\\best_model_epoch-53_val_acc-0.9757.keras\n",
      "\n",
      "Epoch 53: val_accuracy improved from 0.97517 to 0.97567, saving model to wandb_models/Wide_MLP_Model-lr_0.001-bs_64-20250917-083729/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 11ms/step - accuracy: 0.9776 - loss: 0.0818 - val_accuracy: 0.9757 - val_loss: 0.0920\n",
      "Epoch 54/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9779 - loss: 0.0806\n",
      "Epoch 54: val_accuracy did not improve from 0.97567\n",
      "\n",
      "Epoch 54: val_accuracy did not improve from 0.97567\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 12ms/step - accuracy: 0.9778 - loss: 0.0804 - val_accuracy: 0.9757 - val_loss: 0.0913\n",
      "Epoch 55/100\n",
      "\u001b[1m840/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9783 - loss: 0.0793\n",
      "Epoch 55: val_accuracy did not improve from 0.97567\n",
      "\n",
      "Epoch 55: val_accuracy did not improve from 0.97567\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - accuracy: 0.9783 - loss: 0.0791 - val_accuracy: 0.9757 - val_loss: 0.0907\n",
      "Epoch 56/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9789 - loss: 0.0779\n",
      "Epoch 56: val_accuracy improved from 0.97567 to 0.97617, saving model to MLP_Models\\Wide_MLP_Model-lr_0.001-bs_64-20250917-083729\\best_model_epoch-56_val_acc-0.9762.keras\n",
      "\n",
      "Epoch 56: val_accuracy improved from 0.97567 to 0.97617, saving model to wandb_models/Wide_MLP_Model-lr_0.001-bs_64-20250917-083729/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 12ms/step - accuracy: 0.9788 - loss: 0.0777 - val_accuracy: 0.9762 - val_loss: 0.0900\n",
      "Epoch 57/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9793 - loss: 0.0766\n",
      "Epoch 57: val_accuracy did not improve from 0.97617\n",
      "\n",
      "Epoch 57: val_accuracy did not improve from 0.97617\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 12ms/step - accuracy: 0.9793 - loss: 0.0764 - val_accuracy: 0.9762 - val_loss: 0.0894\n",
      "Epoch 58/100\n",
      "\u001b[1m840/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9796 - loss: 0.0754\n",
      "Epoch 58: val_accuracy improved from 0.97617 to 0.97650, saving model to MLP_Models\\Wide_MLP_Model-lr_0.001-bs_64-20250917-083729\\best_model_epoch-58_val_acc-0.9765.keras\n",
      "\n",
      "Epoch 58: val_accuracy improved from 0.97617 to 0.97650, saving model to wandb_models/Wide_MLP_Model-lr_0.001-bs_64-20250917-083729/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 11ms/step - accuracy: 0.9795 - loss: 0.0752 - val_accuracy: 0.9765 - val_loss: 0.0888\n",
      "Epoch 59/100\n",
      "\u001b[1m838/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9798 - loss: 0.0741\n",
      "Epoch 59: val_accuracy improved from 0.97650 to 0.97667, saving model to MLP_Models\\Wide_MLP_Model-lr_0.001-bs_64-20250917-083729\\best_model_epoch-59_val_acc-0.9767.keras\n",
      "\n",
      "Epoch 59: val_accuracy improved from 0.97650 to 0.97667, saving model to wandb_models/Wide_MLP_Model-lr_0.001-bs_64-20250917-083729/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 9ms/step - accuracy: 0.9798 - loss: 0.0739 - val_accuracy: 0.9767 - val_loss: 0.0882\n",
      "Epoch 60/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9801 - loss: 0.0729\n",
      "Epoch 60: val_accuracy improved from 0.97667 to 0.97717, saving model to MLP_Models\\Wide_MLP_Model-lr_0.001-bs_64-20250917-083729\\best_model_epoch-60_val_acc-0.9772.keras\n",
      "\n",
      "Epoch 60: val_accuracy improved from 0.97667 to 0.97717, saving model to wandb_models/Wide_MLP_Model-lr_0.001-bs_64-20250917-083729/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 9ms/step - accuracy: 0.9801 - loss: 0.0727 - val_accuracy: 0.9772 - val_loss: 0.0877\n",
      "Epoch 61/100\n",
      "\u001b[1m839/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9807 - loss: 0.0717\n",
      "Epoch 61: val_accuracy improved from 0.97717 to 0.97733, saving model to MLP_Models\\Wide_MLP_Model-lr_0.001-bs_64-20250917-083729\\best_model_epoch-61_val_acc-0.9773.keras\n",
      "\n",
      "Epoch 61: val_accuracy improved from 0.97717 to 0.97733, saving model to wandb_models/Wide_MLP_Model-lr_0.001-bs_64-20250917-083729/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 11ms/step - accuracy: 0.9806 - loss: 0.0716 - val_accuracy: 0.9773 - val_loss: 0.0871\n",
      "Epoch 62/100\n",
      "\u001b[1m839/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9811 - loss: 0.0706\n",
      "Epoch 62: val_accuracy did not improve from 0.97733\n",
      "\n",
      "Epoch 62: val_accuracy did not improve from 0.97733\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 9ms/step - accuracy: 0.9809 - loss: 0.0704 - val_accuracy: 0.9773 - val_loss: 0.0866\n",
      "Epoch 63/100\n",
      "\u001b[1m839/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9815 - loss: 0.0695\n",
      "Epoch 63: val_accuracy did not improve from 0.97733\n",
      "\n",
      "Epoch 63: val_accuracy did not improve from 0.97733\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 9ms/step - accuracy: 0.9814 - loss: 0.0693 - val_accuracy: 0.9773 - val_loss: 0.0861\n",
      "Epoch 64/100\n",
      "\u001b[1m839/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9818 - loss: 0.0683\n",
      "Epoch 64: val_accuracy did not improve from 0.97733\n",
      "\n",
      "Epoch 64: val_accuracy did not improve from 0.97733\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - accuracy: 0.9819 - loss: 0.0682 - val_accuracy: 0.9773 - val_loss: 0.0856\n",
      "Epoch 65/100\n",
      "\u001b[1m838/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9820 - loss: 0.0673\n",
      "Epoch 65: val_accuracy did not improve from 0.97733\n",
      "\n",
      "Epoch 65: val_accuracy did not improve from 0.97733\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 11ms/step - accuracy: 0.9821 - loss: 0.0671 - val_accuracy: 0.9773 - val_loss: 0.0852\n",
      "Epoch 66/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9824 - loss: 0.0662\n",
      "Epoch 66: val_accuracy improved from 0.97733 to 0.97767, saving model to MLP_Models\\Wide_MLP_Model-lr_0.001-bs_64-20250917-083729\\best_model_epoch-66_val_acc-0.9777.keras\n",
      "\n",
      "Epoch 66: val_accuracy improved from 0.97733 to 0.97767, saving model to wandb_models/Wide_MLP_Model-lr_0.001-bs_64-20250917-083729/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 19ms/step - accuracy: 0.9826 - loss: 0.0660 - val_accuracy: 0.9777 - val_loss: 0.0847\n",
      "Epoch 67/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9828 - loss: 0.0652\n",
      "Epoch 67: val_accuracy did not improve from 0.97767\n",
      "\n",
      "Epoch 67: val_accuracy did not improve from 0.97767\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 11ms/step - accuracy: 0.9829 - loss: 0.0650 - val_accuracy: 0.9777 - val_loss: 0.0842\n",
      "Epoch 68/100\n",
      "\u001b[1m839/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9833 - loss: 0.0642\n",
      "Epoch 68: val_accuracy did not improve from 0.97767\n",
      "\n",
      "Epoch 68: val_accuracy did not improve from 0.97767\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 9ms/step - accuracy: 0.9834 - loss: 0.0640 - val_accuracy: 0.9777 - val_loss: 0.0838\n",
      "Epoch 69/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9837 - loss: 0.0632\n",
      "Epoch 69: val_accuracy did not improve from 0.97767\n",
      "\n",
      "Epoch 69: val_accuracy did not improve from 0.97767\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - accuracy: 0.9837 - loss: 0.0630 - val_accuracy: 0.9775 - val_loss: 0.0834\n",
      "Epoch 70/100\n",
      "\u001b[1m838/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9839 - loss: 0.0622\n",
      "Epoch 70: val_accuracy did not improve from 0.97767\n",
      "\n",
      "Epoch 70: val_accuracy did not improve from 0.97767\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 11ms/step - accuracy: 0.9840 - loss: 0.0620 - val_accuracy: 0.9777 - val_loss: 0.0830\n",
      "Epoch 71/100\n",
      "\u001b[1m840/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9840 - loss: 0.0612\n",
      "Epoch 71: val_accuracy did not improve from 0.97767\n",
      "\n",
      "Epoch 71: val_accuracy did not improve from 0.97767\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 11ms/step - accuracy: 0.9842 - loss: 0.0611 - val_accuracy: 0.9777 - val_loss: 0.0826\n",
      "Epoch 72/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9843 - loss: 0.0603\n",
      "Epoch 72: val_accuracy improved from 0.97767 to 0.97817, saving model to MLP_Models\\Wide_MLP_Model-lr_0.001-bs_64-20250917-083729\\best_model_epoch-72_val_acc-0.9782.keras\n",
      "\n",
      "Epoch 72: val_accuracy improved from 0.97767 to 0.97817, saving model to wandb_models/Wide_MLP_Model-lr_0.001-bs_64-20250917-083729/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 12ms/step - accuracy: 0.9845 - loss: 0.0601 - val_accuracy: 0.9782 - val_loss: 0.0822\n",
      "Epoch 73/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9847 - loss: 0.0594\n",
      "Epoch 73: val_accuracy did not improve from 0.97817\n",
      "\n",
      "Epoch 73: val_accuracy did not improve from 0.97817\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 8ms/step - accuracy: 0.9849 - loss: 0.0592 - val_accuracy: 0.9782 - val_loss: 0.0818\n",
      "Epoch 74/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9850 - loss: 0.0585\n",
      "Epoch 74: val_accuracy improved from 0.97817 to 0.97850, saving model to MLP_Models\\Wide_MLP_Model-lr_0.001-bs_64-20250917-083729\\best_model_epoch-74_val_acc-0.9785.keras\n",
      "\n",
      "Epoch 74: val_accuracy improved from 0.97817 to 0.97850, saving model to wandb_models/Wide_MLP_Model-lr_0.001-bs_64-20250917-083729/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 8ms/step - accuracy: 0.9851 - loss: 0.0583 - val_accuracy: 0.9785 - val_loss: 0.0814\n",
      "Epoch 75/100\n",
      "\u001b[1m839/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9853 - loss: 0.0576\n",
      "Epoch 75: val_accuracy improved from 0.97850 to 0.97867, saving model to MLP_Models\\Wide_MLP_Model-lr_0.001-bs_64-20250917-083729\\best_model_epoch-75_val_acc-0.9787.keras\n",
      "\n",
      "Epoch 75: val_accuracy improved from 0.97850 to 0.97867, saving model to wandb_models/Wide_MLP_Model-lr_0.001-bs_64-20250917-083729/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - accuracy: 0.9854 - loss: 0.0574 - val_accuracy: 0.9787 - val_loss: 0.0811\n",
      "Epoch 76/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9855 - loss: 0.0567\n",
      "Epoch 76: val_accuracy did not improve from 0.97867\n",
      "\n",
      "Epoch 76: val_accuracy did not improve from 0.97867\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 9ms/step - accuracy: 0.9856 - loss: 0.0566 - val_accuracy: 0.9787 - val_loss: 0.0807\n",
      "Epoch 77/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9858 - loss: 0.0559\n",
      "Epoch 77: val_accuracy did not improve from 0.97867\n",
      "\n",
      "Epoch 77: val_accuracy did not improve from 0.97867\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 8ms/step - accuracy: 0.9859 - loss: 0.0557 - val_accuracy: 0.9787 - val_loss: 0.0804\n",
      "Epoch 78/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9861 - loss: 0.0551\n",
      "Epoch 78: val_accuracy improved from 0.97867 to 0.97883, saving model to MLP_Models\\Wide_MLP_Model-lr_0.001-bs_64-20250917-083729\\best_model_epoch-78_val_acc-0.9788.keras\n",
      "\n",
      "Epoch 78: val_accuracy improved from 0.97867 to 0.97883, saving model to wandb_models/Wide_MLP_Model-lr_0.001-bs_64-20250917-083729/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 8ms/step - accuracy: 0.9862 - loss: 0.0549 - val_accuracy: 0.9788 - val_loss: 0.0800\n",
      "Epoch 79/100\n",
      "\u001b[1m836/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9863 - loss: 0.0543\n",
      "Epoch 79: val_accuracy did not improve from 0.97883\n",
      "\n",
      "Epoch 79: val_accuracy did not improve from 0.97883\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 8ms/step - accuracy: 0.9864 - loss: 0.0541 - val_accuracy: 0.9788 - val_loss: 0.0797\n",
      "Epoch 80/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9865 - loss: 0.0535\n",
      "Epoch 80: val_accuracy did not improve from 0.97883\n",
      "\n",
      "Epoch 80: val_accuracy did not improve from 0.97883\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.9866 - loss: 0.0533 - val_accuracy: 0.9788 - val_loss: 0.0794\n",
      "Epoch 81/100\n",
      "\u001b[1m838/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9867 - loss: 0.0527\n",
      "Epoch 81: val_accuracy improved from 0.97883 to 0.97900, saving model to MLP_Models\\Wide_MLP_Model-lr_0.001-bs_64-20250917-083729\\best_model_epoch-81_val_acc-0.9790.keras\n",
      "\n",
      "Epoch 81: val_accuracy improved from 0.97883 to 0.97900, saving model to wandb_models/Wide_MLP_Model-lr_0.001-bs_64-20250917-083729/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.9869 - loss: 0.0525 - val_accuracy: 0.9790 - val_loss: 0.0791\n",
      "Epoch 82/100\n",
      "\u001b[1m836/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9869 - loss: 0.0519\n",
      "Epoch 82: val_accuracy improved from 0.97900 to 0.97917, saving model to MLP_Models\\Wide_MLP_Model-lr_0.001-bs_64-20250917-083729\\best_model_epoch-82_val_acc-0.9792.keras\n",
      "\n",
      "Epoch 82: val_accuracy improved from 0.97900 to 0.97917, saving model to wandb_models/Wide_MLP_Model-lr_0.001-bs_64-20250917-083729/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 8ms/step - accuracy: 0.9872 - loss: 0.0517 - val_accuracy: 0.9792 - val_loss: 0.0788\n",
      "Epoch 83/100\n",
      "\u001b[1m839/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9873 - loss: 0.0512\n",
      "Epoch 83: val_accuracy did not improve from 0.97917\n",
      "\n",
      "Epoch 83: val_accuracy did not improve from 0.97917\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 8ms/step - accuracy: 0.9874 - loss: 0.0510 - val_accuracy: 0.9792 - val_loss: 0.0785\n",
      "Epoch 84/100\n",
      "\u001b[1m840/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9875 - loss: 0.0504\n",
      "Epoch 84: val_accuracy improved from 0.97917 to 0.97933, saving model to MLP_Models\\Wide_MLP_Model-lr_0.001-bs_64-20250917-083729\\best_model_epoch-84_val_acc-0.9793.keras\n",
      "\n",
      "Epoch 84: val_accuracy improved from 0.97917 to 0.97933, saving model to wandb_models/Wide_MLP_Model-lr_0.001-bs_64-20250917-083729/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 8ms/step - accuracy: 0.9877 - loss: 0.0502 - val_accuracy: 0.9793 - val_loss: 0.0782\n",
      "Epoch 85/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9877 - loss: 0.0497\n",
      "Epoch 85: val_accuracy improved from 0.97933 to 0.97950, saving model to MLP_Models\\Wide_MLP_Model-lr_0.001-bs_64-20250917-083729\\best_model_epoch-85_val_acc-0.9795.keras\n",
      "\n",
      "Epoch 85: val_accuracy improved from 0.97933 to 0.97950, saving model to wandb_models/Wide_MLP_Model-lr_0.001-bs_64-20250917-083729/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.9879 - loss: 0.0495 - val_accuracy: 0.9795 - val_loss: 0.0780\n",
      "Epoch 86/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9880 - loss: 0.0490\n",
      "Epoch 86: val_accuracy did not improve from 0.97950\n",
      "\n",
      "Epoch 86: val_accuracy did not improve from 0.97950\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - accuracy: 0.9881 - loss: 0.0488 - val_accuracy: 0.9795 - val_loss: 0.0777\n",
      "Epoch 87/100\n",
      "\u001b[1m837/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9883 - loss: 0.0483\n",
      "Epoch 87: val_accuracy did not improve from 0.97950\n",
      "\n",
      "Epoch 87: val_accuracy did not improve from 0.97950\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - accuracy: 0.9884 - loss: 0.0481 - val_accuracy: 0.9795 - val_loss: 0.0774\n",
      "Epoch 88/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9886 - loss: 0.0476\n",
      "Epoch 88: val_accuracy improved from 0.97950 to 0.97983, saving model to MLP_Models\\Wide_MLP_Model-lr_0.001-bs_64-20250917-083729\\best_model_epoch-88_val_acc-0.9798.keras\n",
      "\n",
      "Epoch 88: val_accuracy improved from 0.97950 to 0.97983, saving model to wandb_models/Wide_MLP_Model-lr_0.001-bs_64-20250917-083729/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 9ms/step - accuracy: 0.9886 - loss: 0.0474 - val_accuracy: 0.9798 - val_loss: 0.0772\n",
      "Epoch 89/100\n",
      "\u001b[1m838/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9891 - loss: 0.0469\n",
      "Epoch 89: val_accuracy did not improve from 0.97983\n",
      "\n",
      "Epoch 89: val_accuracy did not improve from 0.97983\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 9ms/step - accuracy: 0.9889 - loss: 0.0467 - val_accuracy: 0.9798 - val_loss: 0.0769\n",
      "Epoch 90/100\n",
      "\u001b[1m839/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9894 - loss: 0.0462\n",
      "Epoch 90: val_accuracy did not improve from 0.97983\n",
      "\n",
      "Epoch 90: val_accuracy did not improve from 0.97983\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 9ms/step - accuracy: 0.9891 - loss: 0.0460 - val_accuracy: 0.9798 - val_loss: 0.0767\n",
      "Epoch 91/100\n",
      "\u001b[1m840/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9896 - loss: 0.0456\n",
      "Epoch 91: val_accuracy improved from 0.97983 to 0.98000, saving model to MLP_Models\\Wide_MLP_Model-lr_0.001-bs_64-20250917-083729\\best_model_epoch-91_val_acc-0.9800.keras\n",
      "\n",
      "Epoch 91: val_accuracy improved from 0.97983 to 0.98000, saving model to wandb_models/Wide_MLP_Model-lr_0.001-bs_64-20250917-083729/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - accuracy: 0.9894 - loss: 0.0454 - val_accuracy: 0.9800 - val_loss: 0.0765\n",
      "Epoch 92/100\n",
      "\u001b[1m840/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9897 - loss: 0.0449\n",
      "Epoch 92: val_accuracy did not improve from 0.98000\n",
      "\n",
      "Epoch 92: val_accuracy did not improve from 0.98000\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - accuracy: 0.9896 - loss: 0.0447 - val_accuracy: 0.9800 - val_loss: 0.0762\n",
      "Epoch 93/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9899 - loss: 0.0443\n",
      "Epoch 93: val_accuracy did not improve from 0.98000\n",
      "\n",
      "Epoch 93: val_accuracy did not improve from 0.98000\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10ms/step - accuracy: 0.9899 - loss: 0.0441 - val_accuracy: 0.9800 - val_loss: 0.0760\n",
      "Epoch 94/100\n",
      "\u001b[1m838/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9902 - loss: 0.0437\n",
      "Epoch 94: val_accuracy did not improve from 0.98000\n",
      "\n",
      "Epoch 94: val_accuracy did not improve from 0.98000\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 11ms/step - accuracy: 0.9901 - loss: 0.0435 - val_accuracy: 0.9798 - val_loss: 0.0758\n",
      "Epoch 95/100\n",
      "\u001b[1m839/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9904 - loss: 0.0430\n",
      "Epoch 95: val_accuracy did not improve from 0.98000\n",
      "\n",
      "Epoch 95: val_accuracy did not improve from 0.98000\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - accuracy: 0.9903 - loss: 0.0429 - val_accuracy: 0.9800 - val_loss: 0.0756\n",
      "Epoch 96/100\n",
      "\u001b[1m838/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9907 - loss: 0.0424\n",
      "Epoch 96: val_accuracy improved from 0.98000 to 0.98017, saving model to MLP_Models\\Wide_MLP_Model-lr_0.001-bs_64-20250917-083729\\best_model_epoch-96_val_acc-0.9802.keras\n",
      "\n",
      "Epoch 96: val_accuracy improved from 0.98000 to 0.98017, saving model to wandb_models/Wide_MLP_Model-lr_0.001-bs_64-20250917-083729/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.9905 - loss: 0.0423 - val_accuracy: 0.9802 - val_loss: 0.0754\n",
      "Epoch 97/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9909 - loss: 0.0418\n",
      "Epoch 97: val_accuracy improved from 0.98017 to 0.98050, saving model to MLP_Models\\Wide_MLP_Model-lr_0.001-bs_64-20250917-083729\\best_model_epoch-97_val_acc-0.9805.keras\n",
      "\n",
      "Epoch 97: val_accuracy improved from 0.98017 to 0.98050, saving model to wandb_models/Wide_MLP_Model-lr_0.001-bs_64-20250917-083729/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - accuracy: 0.9908 - loss: 0.0417 - val_accuracy: 0.9805 - val_loss: 0.0752\n",
      "Epoch 98/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9910 - loss: 0.0413\n",
      "Epoch 98: val_accuracy did not improve from 0.98050\n",
      "\n",
      "Epoch 98: val_accuracy did not improve from 0.98050\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - accuracy: 0.9910 - loss: 0.0411 - val_accuracy: 0.9805 - val_loss: 0.0750\n",
      "Epoch 99/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9912 - loss: 0.0407\n",
      "Epoch 99: val_accuracy did not improve from 0.98050\n",
      "\n",
      "Epoch 99: val_accuracy did not improve from 0.98050\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 9ms/step - accuracy: 0.9912 - loss: 0.0405 - val_accuracy: 0.9805 - val_loss: 0.0748\n",
      "Epoch 100/100\n",
      "\u001b[1m840/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9914 - loss: 0.0401\n",
      "Epoch 100: val_accuracy improved from 0.98050 to 0.98067, saving model to MLP_Models\\Wide_MLP_Model-lr_0.001-bs_64-20250917-083729\\best_model_epoch-100_val_acc-0.9807.keras\n",
      "\n",
      "Epoch 100: val_accuracy improved from 0.98050 to 0.98067, saving model to wandb_models/Wide_MLP_Model-lr_0.001-bs_64-20250917-083729/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 9ms/step - accuracy: 0.9914 - loss: 0.0399 - val_accuracy: 0.9807 - val_loss: 0.0746\n",
      "\n",
      "Training history saved to: MLP_Models\\Wide_MLP_Model-lr_0.001-bs_64-20250917-083729\\training_history.pkl\n",
      "\n",
      "--- Peak Performance Summary ---\n",
      "Best validation accuracy:           0.9807\n",
      "Associated training accuracy:       0.9914\n",
      "Occurred at epoch:                  100\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch/accuracy</td><td>▁▅▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇█████████████████████</td></tr><tr><td>epoch/epoch</td><td>▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇███</td></tr><tr><td>epoch/learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/loss</td><td>█▆▅▅▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/val_accuracy</td><td>▁▄▅▅▆▆▆▆▇▇▇▇▇▇▇▇▇███████████████████████</td></tr><tr><td>epoch/val_loss</td><td>█▇▆▅▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch/accuracy</td><td>0.99141</td></tr><tr><td>epoch/epoch</td><td>99</td></tr><tr><td>epoch/learning_rate</td><td>0.001</td></tr><tr><td>epoch/loss</td><td>0.03993</td></tr><tr><td>epoch/val_accuracy</td><td>0.98067</td></tr><tr><td>epoch/val_loss</td><td>0.07461</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Wide_MLP_Model-lr_0.001-bs_64-20250917-083729</strong> at: <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/6i5ld6xg' target=\"_blank\">https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/6i5ld6xg</a><br> View project at: <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2' target=\"_blank\">https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2</a><br>Synced 5 W&B file(s), 0 media file(s), 128 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250917_083729-6i5ld6xg\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Adapting the normalisation layer...\n",
      "Adaptation complete.\n",
      "\n",
      "\n",
      "--- Starting Experiment: Wide_MLP_Model ---\n",
      "\n",
      "--- Model Architecture ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"Wide_MLP_Model\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"Wide_MLP_Model\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ normalization_15                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Normalization</span>)                 │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_60 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">401,920</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_61 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │       <span style=\"color: #00af00; text-decoration-color: #00af00\">525,312</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_62 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">262,400</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_63 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,570</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ normalization_15                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m1\u001b[0m)      │             \u001b[38;5;34m3\u001b[0m │\n",
       "│ (\u001b[38;5;33mNormalization\u001b[0m)                 │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_15 (\u001b[38;5;33mFlatten\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m784\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_60 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │       \u001b[38;5;34m401,920\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_61 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │       \u001b[38;5;34m525,312\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_62 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m262,400\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_63 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │         \u001b[38;5;34m2,570\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,192,205</span> (4.55 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,192,205\u001b[0m (4.55 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,192,202</span> (4.55 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,192,202\u001b[0m (4.55 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3</span> (16.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m3\u001b[0m (16.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Hyperparameters ---\n",
      "optimiser           : SGD\n",
      "learning_rate       : 0.01\n",
      "epochs              : 100\n",
      "batch_size          : 64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "creating run (0.3s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\TimVos\\VSC Projects\\CSE5ML\\Assessment 2\\wandb\\run-20250917_085200-8tekzt7d</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/8tekzt7d' target=\"_blank\">Wide_MLP_Model-lr_0.01-bs_64-20250917-085200</a></strong> to <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2' target=\"_blank\">https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/8tekzt7d' target=\"_blank\">https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/8tekzt7d</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m837/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8023 - loss: 0.7221\n",
      "Epoch 1: val_accuracy improved from None to 0.94167, saving model to MLP_Models\\Wide_MLP_Model-lr_0.01-bs_64-20250917-085200\\best_model_epoch-01_val_acc-0.9417.keras\n",
      "\n",
      "Epoch 1: val_accuracy improved from None to 0.94167, saving model to wandb_models/Wide_MLP_Model-lr_0.01-bs_64-20250917-085200/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 9ms/step - accuracy: 0.8829 - loss: 0.4222 - val_accuracy: 0.9417 - val_loss: 0.1938\n",
      "Epoch 2/100\n",
      "\u001b[1m840/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9373 - loss: 0.2162\n",
      "Epoch 2: val_accuracy improved from 0.94167 to 0.95983, saving model to MLP_Models\\Wide_MLP_Model-lr_0.01-bs_64-20250917-085200\\best_model_epoch-02_val_acc-0.9598.keras\n",
      "\n",
      "Epoch 2: val_accuracy improved from 0.94167 to 0.95983, saving model to wandb_models/Wide_MLP_Model-lr_0.01-bs_64-20250917-085200/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - accuracy: 0.9409 - loss: 0.2007 - val_accuracy: 0.9598 - val_loss: 0.1418\n",
      "Epoch 3/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9535 - loss: 0.1604\n",
      "Epoch 3: val_accuracy improved from 0.95983 to 0.96683, saving model to MLP_Models\\Wide_MLP_Model-lr_0.01-bs_64-20250917-085200\\best_model_epoch-03_val_acc-0.9668.keras\n",
      "\n",
      "Epoch 3: val_accuracy improved from 0.95983 to 0.96683, saving model to wandb_models/Wide_MLP_Model-lr_0.01-bs_64-20250917-085200/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - accuracy: 0.9552 - loss: 0.1524 - val_accuracy: 0.9668 - val_loss: 0.1172\n",
      "Epoch 4/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9631 - loss: 0.1283\n",
      "Epoch 4: val_accuracy improved from 0.96683 to 0.97150, saving model to MLP_Models\\Wide_MLP_Model-lr_0.01-bs_64-20250917-085200\\best_model_epoch-04_val_acc-0.9715.keras\n",
      "\n",
      "Epoch 4: val_accuracy improved from 0.96683 to 0.97150, saving model to wandb_models/Wide_MLP_Model-lr_0.01-bs_64-20250917-085200/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 18ms/step - accuracy: 0.9640 - loss: 0.1231 - val_accuracy: 0.9715 - val_loss: 0.1027\n",
      "Epoch 5/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9700 - loss: 0.1064\n",
      "Epoch 5: val_accuracy improved from 0.97150 to 0.97333, saving model to MLP_Models\\Wide_MLP_Model-lr_0.01-bs_64-20250917-085200\\best_model_epoch-05_val_acc-0.9733.keras\n",
      "\n",
      "Epoch 5: val_accuracy improved from 0.97150 to 0.97333, saving model to wandb_models/Wide_MLP_Model-lr_0.01-bs_64-20250917-085200/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 11ms/step - accuracy: 0.9708 - loss: 0.1027 - val_accuracy: 0.9733 - val_loss: 0.0932\n",
      "Epoch 6/100\n",
      "\u001b[1m840/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9757 - loss: 0.0901\n",
      "Epoch 6: val_accuracy improved from 0.97333 to 0.97483, saving model to MLP_Models\\Wide_MLP_Model-lr_0.01-bs_64-20250917-085200\\best_model_epoch-06_val_acc-0.9748.keras\n",
      "\n",
      "Epoch 6: val_accuracy improved from 0.97333 to 0.97483, saving model to wandb_models/Wide_MLP_Model-lr_0.01-bs_64-20250917-085200/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - accuracy: 0.9758 - loss: 0.0873 - val_accuracy: 0.9748 - val_loss: 0.0866\n",
      "Epoch 7/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9791 - loss: 0.0774\n",
      "Epoch 7: val_accuracy improved from 0.97483 to 0.97600, saving model to MLP_Models\\Wide_MLP_Model-lr_0.01-bs_64-20250917-085200\\best_model_epoch-07_val_acc-0.9760.keras\n",
      "\n",
      "Epoch 7: val_accuracy improved from 0.97483 to 0.97600, saving model to wandb_models/Wide_MLP_Model-lr_0.01-bs_64-20250917-085200/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 11ms/step - accuracy: 0.9791 - loss: 0.0750 - val_accuracy: 0.9760 - val_loss: 0.0819\n",
      "Epoch 8/100\n",
      "\u001b[1m839/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9822 - loss: 0.0672\n",
      "Epoch 8: val_accuracy improved from 0.97600 to 0.97700, saving model to MLP_Models\\Wide_MLP_Model-lr_0.01-bs_64-20250917-085200\\best_model_epoch-08_val_acc-0.9770.keras\n",
      "\n",
      "Epoch 8: val_accuracy improved from 0.97600 to 0.97700, saving model to wandb_models/Wide_MLP_Model-lr_0.01-bs_64-20250917-085200/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 11ms/step - accuracy: 0.9821 - loss: 0.0651 - val_accuracy: 0.9770 - val_loss: 0.0783\n",
      "Epoch 9/100\n",
      "\u001b[1m839/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9842 - loss: 0.0587\n",
      "Epoch 9: val_accuracy improved from 0.97700 to 0.97783, saving model to MLP_Models\\Wide_MLP_Model-lr_0.01-bs_64-20250917-085200\\best_model_epoch-09_val_acc-0.9778.keras\n",
      "\n",
      "Epoch 9: val_accuracy improved from 0.97700 to 0.97783, saving model to wandb_models/Wide_MLP_Model-lr_0.01-bs_64-20250917-085200/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 9ms/step - accuracy: 0.9846 - loss: 0.0568 - val_accuracy: 0.9778 - val_loss: 0.0756\n",
      "Epoch 10/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9866 - loss: 0.0515\n",
      "Epoch 10: val_accuracy improved from 0.97783 to 0.97850, saving model to MLP_Models\\Wide_MLP_Model-lr_0.01-bs_64-20250917-085200\\best_model_epoch-10_val_acc-0.9785.keras\n",
      "\n",
      "Epoch 10: val_accuracy improved from 0.97783 to 0.97850, saving model to wandb_models/Wide_MLP_Model-lr_0.01-bs_64-20250917-085200/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - accuracy: 0.9867 - loss: 0.0497 - val_accuracy: 0.9785 - val_loss: 0.0736\n",
      "Epoch 11/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9887 - loss: 0.0453\n",
      "Epoch 11: val_accuracy improved from 0.97850 to 0.97917, saving model to MLP_Models\\Wide_MLP_Model-lr_0.01-bs_64-20250917-085200\\best_model_epoch-11_val_acc-0.9792.keras\n",
      "\n",
      "Epoch 11: val_accuracy improved from 0.97850 to 0.97917, saving model to wandb_models/Wide_MLP_Model-lr_0.01-bs_64-20250917-085200/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - accuracy: 0.9890 - loss: 0.0437 - val_accuracy: 0.9792 - val_loss: 0.0720\n",
      "Epoch 12/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9901 - loss: 0.0400\n",
      "Epoch 12: val_accuracy improved from 0.97917 to 0.97983, saving model to MLP_Models\\Wide_MLP_Model-lr_0.01-bs_64-20250917-085200\\best_model_epoch-12_val_acc-0.9798.keras\n",
      "\n",
      "Epoch 12: val_accuracy improved from 0.97917 to 0.97983, saving model to wandb_models/Wide_MLP_Model-lr_0.01-bs_64-20250917-085200/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - accuracy: 0.9906 - loss: 0.0384 - val_accuracy: 0.9798 - val_loss: 0.0708\n",
      "Epoch 13/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9916 - loss: 0.0353\n",
      "Epoch 13: val_accuracy improved from 0.97983 to 0.98033, saving model to MLP_Models\\Wide_MLP_Model-lr_0.01-bs_64-20250917-085200\\best_model_epoch-13_val_acc-0.9803.keras\n",
      "\n",
      "Epoch 13: val_accuracy improved from 0.97983 to 0.98033, saving model to wandb_models/Wide_MLP_Model-lr_0.01-bs_64-20250917-085200/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - accuracy: 0.9921 - loss: 0.0338 - val_accuracy: 0.9803 - val_loss: 0.0700\n",
      "Epoch 14/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9933 - loss: 0.0312\n",
      "Epoch 14: val_accuracy did not improve from 0.98033\n",
      "\n",
      "Epoch 14: val_accuracy did not improve from 0.98033\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 11ms/step - accuracy: 0.9936 - loss: 0.0297 - val_accuracy: 0.9802 - val_loss: 0.0694\n",
      "Epoch 15/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9949 - loss: 0.0276\n",
      "Epoch 15: val_accuracy improved from 0.98033 to 0.98050, saving model to MLP_Models\\Wide_MLP_Model-lr_0.01-bs_64-20250917-085200\\best_model_epoch-15_val_acc-0.9805.keras\n",
      "\n",
      "Epoch 15: val_accuracy improved from 0.98033 to 0.98050, saving model to wandb_models/Wide_MLP_Model-lr_0.01-bs_64-20250917-085200/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - accuracy: 0.9950 - loss: 0.0263 - val_accuracy: 0.9805 - val_loss: 0.0690\n",
      "Epoch 16/100\n",
      "\u001b[1m838/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9957 - loss: 0.0245\n",
      "Epoch 16: val_accuracy improved from 0.98050 to 0.98083, saving model to MLP_Models\\Wide_MLP_Model-lr_0.01-bs_64-20250917-085200\\best_model_epoch-16_val_acc-0.9808.keras\n",
      "\n",
      "Epoch 16: val_accuracy improved from 0.98050 to 0.98083, saving model to wandb_models/Wide_MLP_Model-lr_0.01-bs_64-20250917-085200/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 9ms/step - accuracy: 0.9959 - loss: 0.0232 - val_accuracy: 0.9808 - val_loss: 0.0687\n",
      "Epoch 17/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9964 - loss: 0.0218\n",
      "Epoch 17: val_accuracy did not improve from 0.98083\n",
      "\n",
      "Epoch 17: val_accuracy did not improve from 0.98083\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - accuracy: 0.9967 - loss: 0.0206 - val_accuracy: 0.9807 - val_loss: 0.0685\n",
      "Epoch 18/100\n",
      "\u001b[1m838/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9972 - loss: 0.0194\n",
      "Epoch 18: val_accuracy did not improve from 0.98083\n",
      "\n",
      "Epoch 18: val_accuracy did not improve from 0.98083\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 9ms/step - accuracy: 0.9974 - loss: 0.0183 - val_accuracy: 0.9808 - val_loss: 0.0683\n",
      "Epoch 19/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9977 - loss: 0.0173\n",
      "Epoch 19: val_accuracy did not improve from 0.98083\n",
      "\n",
      "Epoch 19: val_accuracy did not improve from 0.98083\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 9ms/step - accuracy: 0.9979 - loss: 0.0163 - val_accuracy: 0.9808 - val_loss: 0.0682\n",
      "Epoch 20/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9983 - loss: 0.0155\n",
      "Epoch 20: val_accuracy improved from 0.98083 to 0.98100, saving model to MLP_Models\\Wide_MLP_Model-lr_0.01-bs_64-20250917-085200\\best_model_epoch-20_val_acc-0.9810.keras\n",
      "\n",
      "Epoch 20: val_accuracy improved from 0.98083 to 0.98100, saving model to wandb_models/Wide_MLP_Model-lr_0.01-bs_64-20250917-085200/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - accuracy: 0.9984 - loss: 0.0145 - val_accuracy: 0.9810 - val_loss: 0.0681\n",
      "Epoch 21/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9986 - loss: 0.0139\n",
      "Epoch 21: val_accuracy did not improve from 0.98100\n",
      "\n",
      "Epoch 21: val_accuracy did not improve from 0.98100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 11ms/step - accuracy: 0.9987 - loss: 0.0130 - val_accuracy: 0.9807 - val_loss: 0.0681\n",
      "Epoch 22/100\n",
      "\u001b[1m840/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9988 - loss: 0.0124\n",
      "Epoch 22: val_accuracy improved from 0.98100 to 0.98117, saving model to MLP_Models\\Wide_MLP_Model-lr_0.01-bs_64-20250917-085200\\best_model_epoch-22_val_acc-0.9812.keras\n",
      "\n",
      "Epoch 22: val_accuracy improved from 0.98100 to 0.98117, saving model to wandb_models/Wide_MLP_Model-lr_0.01-bs_64-20250917-085200/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 11ms/step - accuracy: 0.9990 - loss: 0.0117 - val_accuracy: 0.9812 - val_loss: 0.0681\n",
      "Epoch 23/100\n",
      "\u001b[1m838/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9990 - loss: 0.0112\n",
      "Epoch 23: val_accuracy did not improve from 0.98117\n",
      "\n",
      "Epoch 23: val_accuracy did not improve from 0.98117\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 14ms/step - accuracy: 0.9992 - loss: 0.0105 - val_accuracy: 0.9812 - val_loss: 0.0681\n",
      "Epoch 24/100\n",
      "\u001b[1m838/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9991 - loss: 0.0100\n",
      "Epoch 24: val_accuracy did not improve from 0.98117\n",
      "\n",
      "Epoch 24: val_accuracy did not improve from 0.98117\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - accuracy: 0.9993 - loss: 0.0095 - val_accuracy: 0.9807 - val_loss: 0.0682\n",
      "Epoch 25/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9993 - loss: 0.0090\n",
      "Epoch 25: val_accuracy did not improve from 0.98117\n",
      "\n",
      "Epoch 25: val_accuracy did not improve from 0.98117\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 12ms/step - accuracy: 0.9994 - loss: 0.0086 - val_accuracy: 0.9807 - val_loss: 0.0683\n",
      "Epoch 26/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9994 - loss: 0.0081\n",
      "Epoch 26: val_accuracy did not improve from 0.98117\n",
      "\n",
      "Epoch 26: val_accuracy did not improve from 0.98117\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 11ms/step - accuracy: 0.9996 - loss: 0.0078 - val_accuracy: 0.9807 - val_loss: 0.0684\n",
      "Epoch 27/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9995 - loss: 0.0074\n",
      "Epoch 27: val_accuracy did not improve from 0.98117\n",
      "\n",
      "Epoch 27: val_accuracy did not improve from 0.98117\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - accuracy: 0.9997 - loss: 0.0071 - val_accuracy: 0.9810 - val_loss: 0.0686\n",
      "Epoch 28/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9997 - loss: 0.0067\n",
      "Epoch 28: val_accuracy did not improve from 0.98117\n",
      "\n",
      "Epoch 28: val_accuracy did not improve from 0.98117\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 9ms/step - accuracy: 0.9998 - loss: 0.0065 - val_accuracy: 0.9812 - val_loss: 0.0688\n",
      "Epoch 29/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9997 - loss: 0.0061\n",
      "Epoch 29: val_accuracy improved from 0.98117 to 0.98133, saving model to MLP_Models\\Wide_MLP_Model-lr_0.01-bs_64-20250917-085200\\best_model_epoch-29_val_acc-0.9813.keras\n",
      "\n",
      "Epoch 29: val_accuracy improved from 0.98117 to 0.98133, saving model to wandb_models/Wide_MLP_Model-lr_0.01-bs_64-20250917-085200/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.9998 - loss: 0.0060 - val_accuracy: 0.9813 - val_loss: 0.0690\n",
      "Epoch 30/100\n",
      "\u001b[1m839/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9998 - loss: 0.0056\n",
      "Epoch 30: val_accuracy did not improve from 0.98133\n",
      "\n",
      "Epoch 30: val_accuracy did not improve from 0.98133\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - accuracy: 0.9998 - loss: 0.0055 - val_accuracy: 0.9813 - val_loss: 0.0693\n",
      "Epoch 31/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9999 - loss: 0.0052\n",
      "Epoch 31: val_accuracy improved from 0.98133 to 0.98150, saving model to MLP_Models\\Wide_MLP_Model-lr_0.01-bs_64-20250917-085200\\best_model_epoch-31_val_acc-0.9815.keras\n",
      "\n",
      "Epoch 31: val_accuracy improved from 0.98133 to 0.98150, saving model to wandb_models/Wide_MLP_Model-lr_0.01-bs_64-20250917-085200/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - accuracy: 0.9999 - loss: 0.0051 - val_accuracy: 0.9815 - val_loss: 0.0695\n",
      "Epoch 32/100\n",
      "\u001b[1m838/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9999 - loss: 0.0048\n",
      "Epoch 32: val_accuracy improved from 0.98150 to 0.98167, saving model to MLP_Models\\Wide_MLP_Model-lr_0.01-bs_64-20250917-085200\\best_model_epoch-32_val_acc-0.9817.keras\n",
      "\n",
      "Epoch 32: val_accuracy improved from 0.98150 to 0.98167, saving model to wandb_models/Wide_MLP_Model-lr_0.01-bs_64-20250917-085200/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - accuracy: 0.9999 - loss: 0.0047 - val_accuracy: 0.9817 - val_loss: 0.0698\n",
      "Epoch 33/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9999 - loss: 0.0045\n",
      "Epoch 33: val_accuracy did not improve from 0.98167\n",
      "\n",
      "Epoch 33: val_accuracy did not improve from 0.98167\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 9ms/step - accuracy: 0.9999 - loss: 0.0044 - val_accuracy: 0.9815 - val_loss: 0.0701\n",
      "Epoch 34/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9999 - loss: 0.0042\n",
      "Epoch 34: val_accuracy improved from 0.98167 to 0.98183, saving model to MLP_Models\\Wide_MLP_Model-lr_0.01-bs_64-20250917-085200\\best_model_epoch-34_val_acc-0.9818.keras\n",
      "\n",
      "Epoch 34: val_accuracy improved from 0.98167 to 0.98183, saving model to wandb_models/Wide_MLP_Model-lr_0.01-bs_64-20250917-085200/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 11ms/step - accuracy: 0.9999 - loss: 0.0041 - val_accuracy: 0.9818 - val_loss: 0.0704\n",
      "Epoch 35/100\n",
      "\u001b[1m840/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0039\n",
      "Epoch 35: val_accuracy improved from 0.98183 to 0.98217, saving model to MLP_Models\\Wide_MLP_Model-lr_0.01-bs_64-20250917-085200\\best_model_epoch-35_val_acc-0.9822.keras\n",
      "\n",
      "Epoch 35: val_accuracy improved from 0.98183 to 0.98217, saving model to wandb_models/Wide_MLP_Model-lr_0.01-bs_64-20250917-085200/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 0.0038 - val_accuracy: 0.9822 - val_loss: 0.0707\n",
      "Epoch 36/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0036\n",
      "Epoch 36: val_accuracy did not improve from 0.98217\n",
      "\n",
      "Epoch 36: val_accuracy did not improve from 0.98217\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0036 - val_accuracy: 0.9822 - val_loss: 0.0710\n",
      "Epoch 37/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0034\n",
      "Epoch 37: val_accuracy did not improve from 0.98217\n",
      "\n",
      "Epoch 37: val_accuracy did not improve from 0.98217\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0034 - val_accuracy: 0.9822 - val_loss: 0.0713\n",
      "Epoch 38/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0032\n",
      "Epoch 38: val_accuracy did not improve from 0.98217\n",
      "\n",
      "Epoch 38: val_accuracy did not improve from 0.98217\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 0.0032 - val_accuracy: 0.9822 - val_loss: 0.0716\n",
      "Epoch 39/100\n",
      "\u001b[1m840/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0031\n",
      "Epoch 39: val_accuracy improved from 0.98217 to 0.98233, saving model to MLP_Models\\Wide_MLP_Model-lr_0.01-bs_64-20250917-085200\\best_model_epoch-39_val_acc-0.9823.keras\n",
      "\n",
      "Epoch 39: val_accuracy improved from 0.98217 to 0.98233, saving model to wandb_models/Wide_MLP_Model-lr_0.01-bs_64-20250917-085200/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0030 - val_accuracy: 0.9823 - val_loss: 0.0719\n",
      "Epoch 40/100\n",
      "\u001b[1m838/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0029\n",
      "Epoch 40: val_accuracy did not improve from 0.98233\n",
      "\n",
      "Epoch 40: val_accuracy did not improve from 0.98233\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0029 - val_accuracy: 0.9822 - val_loss: 0.0722\n",
      "Epoch 41/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0027\n",
      "Epoch 41: val_accuracy did not improve from 0.98233\n",
      "\n",
      "Epoch 41: val_accuracy did not improve from 0.98233\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0027 - val_accuracy: 0.9823 - val_loss: 0.0725\n",
      "Epoch 42/100\n",
      "\u001b[1m837/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0026\n",
      "Epoch 42: val_accuracy did not improve from 0.98233\n",
      "\n",
      "Epoch 42: val_accuracy did not improve from 0.98233\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0026 - val_accuracy: 0.9823 - val_loss: 0.0728\n",
      "Epoch 43/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0025\n",
      "Epoch 43: val_accuracy did not improve from 0.98233\n",
      "\n",
      "Epoch 43: val_accuracy did not improve from 0.98233\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 0.0025 - val_accuracy: 0.9823 - val_loss: 0.0732\n",
      "Epoch 44/100\n",
      "\u001b[1m839/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0024\n",
      "Epoch 44: val_accuracy did not improve from 0.98233\n",
      "\n",
      "Epoch 44: val_accuracy did not improve from 0.98233\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0023 - val_accuracy: 0.9823 - val_loss: 0.0734\n",
      "Epoch 45/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0023\n",
      "Epoch 45: val_accuracy did not improve from 0.98233\n",
      "\n",
      "Epoch 45: val_accuracy did not improve from 0.98233\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 0.0022 - val_accuracy: 0.9823 - val_loss: 0.0738\n",
      "Epoch 46/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0022\n",
      "Epoch 46: val_accuracy did not improve from 0.98233\n",
      "\n",
      "Epoch 46: val_accuracy did not improve from 0.98233\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 0.0021 - val_accuracy: 0.9822 - val_loss: 0.0740\n",
      "Epoch 47/100\n",
      "\u001b[1m837/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0021\n",
      "Epoch 47: val_accuracy did not improve from 0.98233\n",
      "\n",
      "Epoch 47: val_accuracy did not improve from 0.98233\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 0.0021 - val_accuracy: 0.9822 - val_loss: 0.0743\n",
      "Epoch 48/100\n",
      "\u001b[1m839/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0020\n",
      "Epoch 48: val_accuracy did not improve from 0.98233\n",
      "\n",
      "Epoch 48: val_accuracy did not improve from 0.98233\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0020 - val_accuracy: 0.9822 - val_loss: 0.0746\n",
      "Epoch 49/100\n",
      "\u001b[1m838/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0019\n",
      "Epoch 49: val_accuracy did not improve from 0.98233\n",
      "\n",
      "Epoch 49: val_accuracy did not improve from 0.98233\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0019 - val_accuracy: 0.9822 - val_loss: 0.0749\n",
      "\n",
      "Training history saved to: MLP_Models\\Wide_MLP_Model-lr_0.01-bs_64-20250917-085200\\training_history.pkl\n",
      "\n",
      "--- Peak Performance Summary ---\n",
      "Best validation accuracy:           0.9823\n",
      "Associated training accuracy:       1.0000\n",
      "Occurred at epoch:                  39\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch/accuracy</td><td>▁▄▅▆▇▇▇▇▇▇██████████████████████████████</td></tr><tr><td>epoch/epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>epoch/learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/loss</td><td>█▆▅▅▄▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/val_accuracy</td><td>▁▄▅▆▆▇▇▇▇███████████████████████████████</td></tr><tr><td>epoch/val_loss</td><td>█▅▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch/accuracy</td><td>0.99998</td></tr><tr><td>epoch/epoch</td><td>48</td></tr><tr><td>epoch/learning_rate</td><td>0.01</td></tr><tr><td>epoch/loss</td><td>0.0019</td></tr><tr><td>epoch/val_accuracy</td><td>0.98217</td></tr><tr><td>epoch/val_loss</td><td>0.07485</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Wide_MLP_Model-lr_0.01-bs_64-20250917-085200</strong> at: <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/8tekzt7d' target=\"_blank\">https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/8tekzt7d</a><br> View project at: <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2' target=\"_blank\">https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2</a><br>Synced 5 W&B file(s), 0 media file(s), 46 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250917_085200-8tekzt7d\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Adapting the normalisation layer...\n",
      "Adaptation complete.\n",
      "\n",
      "\n",
      "--- Starting Experiment: Deep_MLP_Model ---\n",
      "\n",
      "--- Model Architecture ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"Deep_MLP_Model\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"Deep_MLP_Model\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ normalization_16                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Normalization</span>)                 │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_64 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">100,480</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_65 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">33,024</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_66 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">65,792</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_67 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">65,792</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_68 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">65,792</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_69 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,448</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_70 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_71 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">330</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ normalization_16                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m1\u001b[0m)      │             \u001b[38;5;34m3\u001b[0m │\n",
       "│ (\u001b[38;5;33mNormalization\u001b[0m)                 │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_16 (\u001b[38;5;33mFlatten\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m784\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_64 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m100,480\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_65 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │        \u001b[38;5;34m33,024\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_66 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │        \u001b[38;5;34m65,792\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_67 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │        \u001b[38;5;34m65,792\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_68 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │        \u001b[38;5;34m65,792\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_69 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m16,448\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_70 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m2,080\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_71 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │           \u001b[38;5;34m330\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">349,741</span> (1.33 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m349,741\u001b[0m (1.33 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">349,738</span> (1.33 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m349,738\u001b[0m (1.33 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3</span> (16.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m3\u001b[0m (16.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Hyperparameters ---\n",
      "optimiser           : adamw\n",
      "learning_rate       : 0.001\n",
      "epochs              : 100\n",
      "batch_size          : 64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "creating run (0.0s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\TimVos\\VSC Projects\\CSE5ML\\Assessment 2\\wandb\\run-20250917_085915-qoig61tl</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/qoig61tl' target=\"_blank\">Deep_MLP_Model-lr_0.001-bs_64-20250917-085915</a></strong> to <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2' target=\"_blank\">https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/qoig61tl' target=\"_blank\">https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/qoig61tl</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8360 - loss: 0.5058\n",
      "Epoch 1: val_accuracy improved from None to 0.96033, saving model to MLP_Models\\Deep_MLP_Model-lr_0.001-bs_64-20250917-085915\\best_model_epoch-01_val_acc-0.9603.keras\n",
      "\n",
      "Epoch 1: val_accuracy improved from None to 0.96033, saving model to wandb_models/Deep_MLP_Model-lr_0.001-bs_64-20250917-085915/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - accuracy: 0.9132 - loss: 0.2817 - val_accuracy: 0.9603 - val_loss: 0.1355\n",
      "Epoch 2/100\n",
      "\u001b[1m838/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9609 - loss: 0.1389\n",
      "Epoch 2: val_accuracy improved from 0.96033 to 0.97350, saving model to MLP_Models\\Deep_MLP_Model-lr_0.001-bs_64-20250917-085915\\best_model_epoch-02_val_acc-0.9735.keras\n",
      "\n",
      "Epoch 2: val_accuracy improved from 0.96033 to 0.97350, saving model to wandb_models/Deep_MLP_Model-lr_0.001-bs_64-20250917-085915/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.9635 - loss: 0.1268 - val_accuracy: 0.9735 - val_loss: 0.0945\n",
      "Epoch 3/100\n",
      "\u001b[1m838/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9700 - loss: 0.1019\n",
      "Epoch 3: val_accuracy did not improve from 0.97350\n",
      "\n",
      "Epoch 3: val_accuracy did not improve from 0.97350\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.9714 - loss: 0.0965 - val_accuracy: 0.9717 - val_loss: 0.1031\n",
      "Epoch 4/100\n",
      "\u001b[1m837/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9754 - loss: 0.0842\n",
      "Epoch 4: val_accuracy did not improve from 0.97350\n",
      "\n",
      "Epoch 4: val_accuracy did not improve from 0.97350\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.9764 - loss: 0.0787 - val_accuracy: 0.9713 - val_loss: 0.1052\n",
      "Epoch 5/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9794 - loss: 0.0704\n",
      "Epoch 5: val_accuracy did not improve from 0.97350\n",
      "\n",
      "Epoch 5: val_accuracy did not improve from 0.97350\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - accuracy: 0.9809 - loss: 0.0655 - val_accuracy: 0.9723 - val_loss: 0.1048\n",
      "Epoch 6/100\n",
      "\u001b[1m839/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9809 - loss: 0.0657\n",
      "Epoch 6: val_accuracy improved from 0.97350 to 0.97767, saving model to MLP_Models\\Deep_MLP_Model-lr_0.001-bs_64-20250917-085915\\best_model_epoch-06_val_acc-0.9777.keras\n",
      "\n",
      "Epoch 6: val_accuracy improved from 0.97350 to 0.97767, saving model to wandb_models/Deep_MLP_Model-lr_0.001-bs_64-20250917-085915/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 11ms/step - accuracy: 0.9819 - loss: 0.0623 - val_accuracy: 0.9777 - val_loss: 0.0963\n",
      "Epoch 7/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9848 - loss: 0.0529\n",
      "Epoch 7: val_accuracy did not improve from 0.97767\n",
      "\n",
      "Epoch 7: val_accuracy did not improve from 0.97767\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 9ms/step - accuracy: 0.9855 - loss: 0.0503 - val_accuracy: 0.9753 - val_loss: 0.0962\n",
      "Epoch 8/100\n",
      "\u001b[1m838/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9859 - loss: 0.0469\n",
      "Epoch 8: val_accuracy did not improve from 0.97767\n",
      "\n",
      "Epoch 8: val_accuracy did not improve from 0.97767\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.9862 - loss: 0.0468 - val_accuracy: 0.9737 - val_loss: 0.0933\n",
      "Epoch 9/100\n",
      "\u001b[1m834/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9871 - loss: 0.0431\n",
      "Epoch 9: val_accuracy did not improve from 0.97767\n",
      "\n",
      "Epoch 9: val_accuracy did not improve from 0.97767\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.9875 - loss: 0.0423 - val_accuracy: 0.9753 - val_loss: 0.1026\n",
      "Epoch 10/100\n",
      "\u001b[1m839/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9887 - loss: 0.0382\n",
      "Epoch 10: val_accuracy improved from 0.97767 to 0.97867, saving model to MLP_Models\\Deep_MLP_Model-lr_0.001-bs_64-20250917-085915\\best_model_epoch-10_val_acc-0.9787.keras\n",
      "\n",
      "Epoch 10: val_accuracy improved from 0.97767 to 0.97867, saving model to wandb_models/Deep_MLP_Model-lr_0.001-bs_64-20250917-085915/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.9889 - loss: 0.0382 - val_accuracy: 0.9787 - val_loss: 0.0845\n",
      "Epoch 11/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9898 - loss: 0.0351\n",
      "Epoch 11: val_accuracy did not improve from 0.97867\n",
      "\n",
      "Epoch 11: val_accuracy did not improve from 0.97867\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.9894 - loss: 0.0364 - val_accuracy: 0.9758 - val_loss: 0.1140\n",
      "Epoch 12/100\n",
      "\u001b[1m838/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9902 - loss: 0.0372\n",
      "Epoch 12: val_accuracy did not improve from 0.97867\n",
      "\n",
      "Epoch 12: val_accuracy did not improve from 0.97867\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.9909 - loss: 0.0330 - val_accuracy: 0.9760 - val_loss: 0.1189\n",
      "Epoch 13/100\n",
      "\u001b[1m835/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9902 - loss: 0.0350\n",
      "Epoch 13: val_accuracy improved from 0.97867 to 0.98000, saving model to MLP_Models\\Deep_MLP_Model-lr_0.001-bs_64-20250917-085915\\best_model_epoch-13_val_acc-0.9800.keras\n",
      "\n",
      "Epoch 13: val_accuracy improved from 0.97867 to 0.98000, saving model to wandb_models/Deep_MLP_Model-lr_0.001-bs_64-20250917-085915/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 8ms/step - accuracy: 0.9904 - loss: 0.0334 - val_accuracy: 0.9800 - val_loss: 0.0945\n",
      "Epoch 14/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9919 - loss: 0.0292\n",
      "Epoch 14: val_accuracy did not improve from 0.98000\n",
      "\n",
      "Epoch 14: val_accuracy did not improve from 0.98000\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.9917 - loss: 0.0297 - val_accuracy: 0.9800 - val_loss: 0.0882\n",
      "Epoch 15/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9924 - loss: 0.0262\n",
      "Epoch 15: val_accuracy did not improve from 0.98000\n",
      "\n",
      "Epoch 15: val_accuracy did not improve from 0.98000\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.9915 - loss: 0.0300 - val_accuracy: 0.9767 - val_loss: 0.1094\n",
      "Epoch 16/100\n",
      "\u001b[1m839/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9925 - loss: 0.0293\n",
      "Epoch 16: val_accuracy improved from 0.98000 to 0.98050, saving model to MLP_Models\\Deep_MLP_Model-lr_0.001-bs_64-20250917-085915\\best_model_epoch-16_val_acc-0.9805.keras\n",
      "\n",
      "Epoch 16: val_accuracy improved from 0.98000 to 0.98050, saving model to wandb_models/Deep_MLP_Model-lr_0.001-bs_64-20250917-085915/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 8ms/step - accuracy: 0.9929 - loss: 0.0256 - val_accuracy: 0.9805 - val_loss: 0.0917\n",
      "Epoch 17/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9927 - loss: 0.0246\n",
      "Epoch 17: val_accuracy improved from 0.98050 to 0.98067, saving model to MLP_Models\\Deep_MLP_Model-lr_0.001-bs_64-20250917-085915\\best_model_epoch-17_val_acc-0.9807.keras\n",
      "\n",
      "Epoch 17: val_accuracy improved from 0.98050 to 0.98067, saving model to wandb_models/Deep_MLP_Model-lr_0.001-bs_64-20250917-085915/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.9921 - loss: 0.0281 - val_accuracy: 0.9807 - val_loss: 0.0853\n",
      "Epoch 18/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9940 - loss: 0.0200\n",
      "Epoch 18: val_accuracy improved from 0.98067 to 0.98267, saving model to MLP_Models\\Deep_MLP_Model-lr_0.001-bs_64-20250917-085915\\best_model_epoch-18_val_acc-0.9827.keras\n",
      "\n",
      "Epoch 18: val_accuracy improved from 0.98067 to 0.98267, saving model to wandb_models/Deep_MLP_Model-lr_0.001-bs_64-20250917-085915/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 8ms/step - accuracy: 0.9936 - loss: 0.0222 - val_accuracy: 0.9827 - val_loss: 0.1126\n",
      "Epoch 19/100\n",
      "\u001b[1m838/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9934 - loss: 0.0247\n",
      "Epoch 19: val_accuracy did not improve from 0.98267\n",
      "\n",
      "Epoch 19: val_accuracy did not improve from 0.98267\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.9936 - loss: 0.0236 - val_accuracy: 0.9805 - val_loss: 0.1177\n",
      "Epoch 20/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9931 - loss: 0.0254\n",
      "Epoch 20: val_accuracy did not improve from 0.98267\n",
      "\n",
      "Epoch 20: val_accuracy did not improve from 0.98267\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.9930 - loss: 0.0265 - val_accuracy: 0.9775 - val_loss: 0.0991\n",
      "Epoch 21/100\n",
      "\u001b[1m835/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9939 - loss: 0.0227\n",
      "Epoch 21: val_accuracy improved from 0.98267 to 0.98350, saving model to MLP_Models\\Deep_MLP_Model-lr_0.001-bs_64-20250917-085915\\best_model_epoch-21_val_acc-0.9835.keras\n",
      "\n",
      "Epoch 21: val_accuracy improved from 0.98267 to 0.98350, saving model to wandb_models/Deep_MLP_Model-lr_0.001-bs_64-20250917-085915/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 8ms/step - accuracy: 0.9936 - loss: 0.0236 - val_accuracy: 0.9835 - val_loss: 0.0963\n",
      "Epoch 22/100\n",
      "\u001b[1m834/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9931 - loss: 0.0246\n",
      "Epoch 22: val_accuracy did not improve from 0.98350\n",
      "\n",
      "Epoch 22: val_accuracy did not improve from 0.98350\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 8ms/step - accuracy: 0.9944 - loss: 0.0198 - val_accuracy: 0.9813 - val_loss: 0.1134\n",
      "Epoch 23/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9952 - loss: 0.0192\n",
      "Epoch 23: val_accuracy did not improve from 0.98350\n",
      "\n",
      "Epoch 23: val_accuracy did not improve from 0.98350\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 8ms/step - accuracy: 0.9946 - loss: 0.0205 - val_accuracy: 0.9833 - val_loss: 0.0998\n",
      "Epoch 24/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9957 - loss: 0.0160\n",
      "Epoch 24: val_accuracy did not improve from 0.98350\n",
      "\n",
      "Epoch 24: val_accuracy did not improve from 0.98350\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 8ms/step - accuracy: 0.9949 - loss: 0.0204 - val_accuracy: 0.9800 - val_loss: 0.1082\n",
      "Epoch 25/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9949 - loss: 0.0199\n",
      "Epoch 25: val_accuracy did not improve from 0.98350\n",
      "\n",
      "Epoch 25: val_accuracy did not improve from 0.98350\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 8ms/step - accuracy: 0.9951 - loss: 0.0193 - val_accuracy: 0.9833 - val_loss: 0.1035\n",
      "Epoch 26/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9956 - loss: 0.0170\n",
      "Epoch 26: val_accuracy did not improve from 0.98350\n",
      "\n",
      "Epoch 26: val_accuracy did not improve from 0.98350\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 9ms/step - accuracy: 0.9951 - loss: 0.0199 - val_accuracy: 0.9823 - val_loss: 0.1030\n",
      "Epoch 27/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9950 - loss: 0.0204\n",
      "Epoch 27: val_accuracy did not improve from 0.98350\n",
      "\n",
      "Epoch 27: val_accuracy did not improve from 0.98350\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 8ms/step - accuracy: 0.9953 - loss: 0.0183 - val_accuracy: 0.9802 - val_loss: 0.1085\n",
      "Epoch 28/100\n",
      "\u001b[1m837/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9957 - loss: 0.0172\n",
      "Epoch 28: val_accuracy did not improve from 0.98350\n",
      "\n",
      "Epoch 28: val_accuracy did not improve from 0.98350\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 8ms/step - accuracy: 0.9953 - loss: 0.0190 - val_accuracy: 0.9813 - val_loss: 0.1204\n",
      "Epoch 29/100\n",
      "\u001b[1m839/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9950 - loss: 0.0195\n",
      "Epoch 29: val_accuracy did not improve from 0.98350\n",
      "\n",
      "Epoch 29: val_accuracy did not improve from 0.98350\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.9957 - loss: 0.0157 - val_accuracy: 0.9828 - val_loss: 0.1120\n",
      "Epoch 30/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9965 - loss: 0.0144\n",
      "Epoch 30: val_accuracy did not improve from 0.98350\n",
      "\n",
      "Epoch 30: val_accuracy did not improve from 0.98350\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.9951 - loss: 0.0200 - val_accuracy: 0.9808 - val_loss: 0.0913\n",
      "Epoch 31/100\n",
      "\u001b[1m837/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9956 - loss: 0.0177\n",
      "Epoch 31: val_accuracy did not improve from 0.98350\n",
      "\n",
      "Epoch 31: val_accuracy did not improve from 0.98350\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.9957 - loss: 0.0175 - val_accuracy: 0.9818 - val_loss: 0.1023\n",
      "\n",
      "Training history saved to: MLP_Models\\Deep_MLP_Model-lr_0.001-bs_64-20250917-085915\\training_history.pkl\n",
      "\n",
      "--- Peak Performance Summary ---\n",
      "Best validation accuracy:           0.9835\n",
      "Associated training accuracy:       0.9936\n",
      "Occurred at epoch:                  21\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch/accuracy</td><td>▁▅▆▆▇▇▇▇▇▇▇████████████████████</td></tr><tr><td>epoch/epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇███</td></tr><tr><td>epoch/learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/loss</td><td>█▄▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/val_accuracy</td><td>▁▅▄▄▅▆▆▅▆▇▆▆▇▇▆▇▇█▇▆█▇█▇██▇▇█▇▇</td></tr><tr><td>epoch/val_loss</td><td>█▂▄▄▄▃▃▂▃▁▅▆▂▁▄▂▁▅▆▃▃▅▃▄▄▄▄▆▅▂▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch/accuracy</td><td>0.9957</td></tr><tr><td>epoch/epoch</td><td>30</td></tr><tr><td>epoch/learning_rate</td><td>0.001</td></tr><tr><td>epoch/loss</td><td>0.01749</td></tr><tr><td>epoch/val_accuracy</td><td>0.98183</td></tr><tr><td>epoch/val_loss</td><td>0.10228</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Deep_MLP_Model-lr_0.001-bs_64-20250917-085915</strong> at: <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/qoig61tl' target=\"_blank\">https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/qoig61tl</a><br> View project at: <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2' target=\"_blank\">https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2</a><br>Synced 5 W&B file(s), 0 media file(s), 18 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250917_085915-qoig61tl\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Adapting the normalisation layer...\n",
      "Adaptation complete.\n",
      "\n",
      "\n",
      "--- Starting Experiment: Deep_MLP_Model ---\n",
      "\n",
      "--- Model Architecture ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"Deep_MLP_Model\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"Deep_MLP_Model\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ normalization_17                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Normalization</span>)                 │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_72 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">100,480</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_73 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">33,024</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_74 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">65,792</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_75 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">65,792</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_76 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">65,792</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_77 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,448</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_78 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_79 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">330</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ normalization_17                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m1\u001b[0m)      │             \u001b[38;5;34m3\u001b[0m │\n",
       "│ (\u001b[38;5;33mNormalization\u001b[0m)                 │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_17 (\u001b[38;5;33mFlatten\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m784\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_72 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m100,480\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_73 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │        \u001b[38;5;34m33,024\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_74 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │        \u001b[38;5;34m65,792\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_75 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │        \u001b[38;5;34m65,792\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_76 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │        \u001b[38;5;34m65,792\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_77 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m16,448\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_78 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m2,080\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_79 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │           \u001b[38;5;34m330\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">349,741</span> (1.33 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m349,741\u001b[0m (1.33 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">349,738</span> (1.33 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m349,738\u001b[0m (1.33 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3</span> (16.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m3\u001b[0m (16.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Hyperparameters ---\n",
      "optimiser           : adamw\n",
      "learning_rate       : 0.01\n",
      "epochs              : 100\n",
      "batch_size          : 64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "creating run (0.0s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\TimVos\\VSC Projects\\CSE5ML\\Assessment 2\\wandb\\run-20250917_090257-zve245no</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/zve245no' target=\"_blank\">Deep_MLP_Model-lr_0.01-bs_64-20250917-090257</a></strong> to <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2' target=\"_blank\">https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/zve245no' target=\"_blank\">https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/zve245no</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m836/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7097 - loss: 0.9138\n",
      "Epoch 1: val_accuracy improved from None to 0.92617, saving model to MLP_Models\\Deep_MLP_Model-lr_0.01-bs_64-20250917-090257\\best_model_epoch-01_val_acc-0.9262.keras\n",
      "\n",
      "Epoch 1: val_accuracy improved from None to 0.92617, saving model to wandb_models/Deep_MLP_Model-lr_0.01-bs_64-20250917-090257/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10ms/step - accuracy: 0.8366 - loss: 0.5932 - val_accuracy: 0.9262 - val_loss: 0.3355\n",
      "Epoch 2/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9100 - loss: 0.3876\n",
      "Epoch 2: val_accuracy improved from 0.92617 to 0.92633, saving model to MLP_Models\\Deep_MLP_Model-lr_0.01-bs_64-20250917-090257\\best_model_epoch-02_val_acc-0.9263.keras\n",
      "\n",
      "Epoch 2: val_accuracy improved from 0.92617 to 0.92633, saving model to wandb_models/Deep_MLP_Model-lr_0.01-bs_64-20250917-090257/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 9ms/step - accuracy: 0.9097 - loss: 0.3795 - val_accuracy: 0.9263 - val_loss: 0.3180\n",
      "Epoch 3/100\n",
      "\u001b[1m840/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9207 - loss: 0.3303\n",
      "Epoch 3: val_accuracy did not improve from 0.92633\n",
      "\n",
      "Epoch 3: val_accuracy did not improve from 0.92633\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 8ms/step - accuracy: 0.9143 - loss: 0.3741 - val_accuracy: 0.9182 - val_loss: 0.3836\n",
      "Epoch 4/100\n",
      "\u001b[1m840/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8654 - loss: 0.5448\n",
      "Epoch 4: val_accuracy did not improve from 0.92633\n",
      "\n",
      "Epoch 4: val_accuracy did not improve from 0.92633\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.8629 - loss: 0.5414 - val_accuracy: 0.8863 - val_loss: 0.4449\n",
      "Epoch 5/100\n",
      "\u001b[1m839/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8724 - loss: 0.4964\n",
      "Epoch 5: val_accuracy did not improve from 0.92633\n",
      "\n",
      "Epoch 5: val_accuracy did not improve from 0.92633\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 8ms/step - accuracy: 0.8679 - loss: 0.5335 - val_accuracy: 0.8852 - val_loss: 0.4832\n",
      "Epoch 6/100\n",
      "\u001b[1m835/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8086 - loss: 0.7122\n",
      "Epoch 6: val_accuracy did not improve from 0.92633\n",
      "\n",
      "Epoch 6: val_accuracy did not improve from 0.92633\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 8ms/step - accuracy: 0.7935 - loss: 0.7678 - val_accuracy: 0.8285 - val_loss: 0.5951\n",
      "Epoch 7/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5884 - loss: 1.2017\n",
      "Epoch 7: val_accuracy did not improve from 0.92633\n",
      "\n",
      "Epoch 7: val_accuracy did not improve from 0.92633\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 8ms/step - accuracy: 0.4537 - loss: 1.4739 - val_accuracy: 0.3650 - val_loss: 1.6115\n",
      "Epoch 8/100\n",
      "\u001b[1m840/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3675 - loss: 1.6202\n",
      "Epoch 8: val_accuracy did not improve from 0.92633\n",
      "\n",
      "Epoch 8: val_accuracy did not improve from 0.92633\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.3755 - loss: 1.5982 - val_accuracy: 0.3460 - val_loss: 1.4986\n",
      "Epoch 9/100\n",
      "\u001b[1m834/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.3535 - loss: 1.6597\n",
      "Epoch 9: val_accuracy did not improve from 0.92633\n",
      "\n",
      "Epoch 9: val_accuracy did not improve from 0.92633\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.4172 - loss: 1.5229 - val_accuracy: 0.5672 - val_loss: 1.2619\n",
      "Epoch 10/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6129 - loss: 1.1719\n",
      "Epoch 10: val_accuracy did not improve from 0.92633\n",
      "\n",
      "Epoch 10: val_accuracy did not improve from 0.92633\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - accuracy: 0.6297 - loss: 1.1391 - val_accuracy: 0.7442 - val_loss: 0.8708\n",
      "Epoch 11/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7217 - loss: 0.9400\n",
      "Epoch 11: val_accuracy did not improve from 0.92633\n",
      "\n",
      "Epoch 11: val_accuracy did not improve from 0.92633\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 8ms/step - accuracy: 0.7460 - loss: 0.8845 - val_accuracy: 0.8548 - val_loss: 0.6265\n",
      "Epoch 12/100\n",
      "\u001b[1m840/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8297 - loss: 0.6802\n",
      "Epoch 12: val_accuracy did not improve from 0.92633\n",
      "\n",
      "Epoch 12: val_accuracy did not improve from 0.92633\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 8ms/step - accuracy: 0.8411 - loss: 0.6378 - val_accuracy: 0.8907 - val_loss: 0.4738\n",
      "\n",
      "Training history saved to: MLP_Models\\Deep_MLP_Model-lr_0.01-bs_64-20250917-090257\\training_history.pkl\n",
      "\n",
      "--- Peak Performance Summary ---\n",
      "Best validation accuracy:           0.9263\n",
      "Associated training accuracy:       0.9097\n",
      "Occurred at epoch:                  2\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch/accuracy</td><td>▇██▇▇▆▂▁▂▄▆▇</td></tr><tr><td>epoch/epoch</td><td>▁▂▂▃▄▄▅▅▆▇▇█</td></tr><tr><td>epoch/learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/loss</td><td>▂▁▁▂▂▃▇██▅▄▃</td></tr><tr><td>epoch/val_accuracy</td><td>█████▇▁▁▄▆▇█</td></tr><tr><td>epoch/val_loss</td><td>▁▁▁▂▂▂█▇▆▄▃▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch/accuracy</td><td>0.84111</td></tr><tr><td>epoch/epoch</td><td>11</td></tr><tr><td>epoch/learning_rate</td><td>0.01</td></tr><tr><td>epoch/loss</td><td>0.63781</td></tr><tr><td>epoch/val_accuracy</td><td>0.89067</td></tr><tr><td>epoch/val_loss</td><td>0.47381</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Deep_MLP_Model-lr_0.01-bs_64-20250917-090257</strong> at: <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/zve245no' target=\"_blank\">https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/zve245no</a><br> View project at: <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2' target=\"_blank\">https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2</a><br>Synced 5 W&B file(s), 0 media file(s), 4 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250917_090257-zve245no\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Adapting the normalisation layer...\n",
      "Adaptation complete.\n",
      "\n",
      "\n",
      "--- Starting Experiment: Deep_MLP_Model ---\n",
      "\n",
      "--- Model Architecture ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"Deep_MLP_Model\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"Deep_MLP_Model\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ normalization_18                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Normalization</span>)                 │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_80 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">100,480</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_81 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">33,024</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_82 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">65,792</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_83 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">65,792</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_84 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">65,792</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_85 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,448</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_86 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_87 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">330</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ normalization_18                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m1\u001b[0m)      │             \u001b[38;5;34m3\u001b[0m │\n",
       "│ (\u001b[38;5;33mNormalization\u001b[0m)                 │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_18 (\u001b[38;5;33mFlatten\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m784\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_80 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m100,480\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_81 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │        \u001b[38;5;34m33,024\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_82 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │        \u001b[38;5;34m65,792\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_83 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │        \u001b[38;5;34m65,792\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_84 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │        \u001b[38;5;34m65,792\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_85 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m16,448\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_86 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m2,080\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_87 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │           \u001b[38;5;34m330\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">349,741</span> (1.33 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m349,741\u001b[0m (1.33 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">349,738</span> (1.33 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m349,738\u001b[0m (1.33 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3</span> (16.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m3\u001b[0m (16.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Hyperparameters ---\n",
      "optimiser           : SGD\n",
      "learning_rate       : 0.001\n",
      "epochs              : 100\n",
      "batch_size          : 64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "creating run (0.5s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\TimVos\\VSC Projects\\CSE5ML\\Assessment 2\\wandb\\run-20250917_090423-odwg102b</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/odwg102b' target=\"_blank\">Deep_MLP_Model-lr_0.001-bs_64-20250917-090423</a></strong> to <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2' target=\"_blank\">https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/odwg102b' target=\"_blank\">https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/odwg102b</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m834/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1416 - loss: 2.2499\n",
      "Epoch 1: val_accuracy improved from None to 0.37483, saving model to MLP_Models\\Deep_MLP_Model-lr_0.001-bs_64-20250917-090423\\best_model_epoch-01_val_acc-0.3748.keras\n",
      "\n",
      "Epoch 1: val_accuracy improved from None to 0.37483, saving model to wandb_models/Deep_MLP_Model-lr_0.001-bs_64-20250917-090423/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - accuracy: 0.2128 - loss: 2.1742 - val_accuracy: 0.3748 - val_loss: 1.9534\n",
      "Epoch 2/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4497 - loss: 1.7803\n",
      "Epoch 2: val_accuracy improved from 0.37483 to 0.74133, saving model to MLP_Models\\Deep_MLP_Model-lr_0.001-bs_64-20250917-090423\\best_model_epoch-02_val_acc-0.7413.keras\n",
      "\n",
      "Epoch 2: val_accuracy improved from 0.37483 to 0.74133, saving model to wandb_models/Deep_MLP_Model-lr_0.001-bs_64-20250917-090423/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.5289 - loss: 1.5728 - val_accuracy: 0.7413 - val_loss: 1.0773\n",
      "Epoch 3/100\n",
      "\u001b[1m840/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7511 - loss: 0.9487\n",
      "Epoch 3: val_accuracy improved from 0.74133 to 0.86067, saving model to MLP_Models\\Deep_MLP_Model-lr_0.001-bs_64-20250917-090423\\best_model_epoch-03_val_acc-0.8607.keras\n",
      "\n",
      "Epoch 3: val_accuracy improved from 0.74133 to 0.86067, saving model to wandb_models/Deep_MLP_Model-lr_0.001-bs_64-20250917-090423/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.7802 - loss: 0.8174 - val_accuracy: 0.8607 - val_loss: 0.5384\n",
      "Epoch 4/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8389 - loss: 0.5680\n",
      "Epoch 4: val_accuracy improved from 0.86067 to 0.88550, saving model to MLP_Models\\Deep_MLP_Model-lr_0.001-bs_64-20250917-090423\\best_model_epoch-04_val_acc-0.8855.keras\n",
      "\n",
      "Epoch 4: val_accuracy improved from 0.86067 to 0.88550, saving model to wandb_models/Deep_MLP_Model-lr_0.001-bs_64-20250917-090423/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 8ms/step - accuracy: 0.8459 - loss: 0.5337 - val_accuracy: 0.8855 - val_loss: 0.3978\n",
      "Epoch 5/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8674 - loss: 0.4490\n",
      "Epoch 5: val_accuracy improved from 0.88550 to 0.89950, saving model to MLP_Models\\Deep_MLP_Model-lr_0.001-bs_64-20250917-090423\\best_model_epoch-05_val_acc-0.8995.keras\n",
      "\n",
      "Epoch 5: val_accuracy improved from 0.88550 to 0.89950, saving model to wandb_models/Deep_MLP_Model-lr_0.001-bs_64-20250917-090423/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 8ms/step - accuracy: 0.8721 - loss: 0.4351 - val_accuracy: 0.8995 - val_loss: 0.3392\n",
      "Epoch 6/100\n",
      "\u001b[1m840/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8856 - loss: 0.3894\n",
      "Epoch 6: val_accuracy improved from 0.89950 to 0.91017, saving model to MLP_Models\\Deep_MLP_Model-lr_0.001-bs_64-20250917-090423\\best_model_epoch-06_val_acc-0.9102.keras\n",
      "\n",
      "Epoch 6: val_accuracy improved from 0.89950 to 0.91017, saving model to wandb_models/Deep_MLP_Model-lr_0.001-bs_64-20250917-090423/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 8ms/step - accuracy: 0.8877 - loss: 0.3819 - val_accuracy: 0.9102 - val_loss: 0.3028\n",
      "Epoch 7/100\n",
      "\u001b[1m837/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8970 - loss: 0.3498\n",
      "Epoch 7: val_accuracy improved from 0.91017 to 0.91817, saving model to MLP_Models\\Deep_MLP_Model-lr_0.001-bs_64-20250917-090423\\best_model_epoch-07_val_acc-0.9182.keras\n",
      "\n",
      "Epoch 7: val_accuracy improved from 0.91017 to 0.91817, saving model to wandb_models/Deep_MLP_Model-lr_0.001-bs_64-20250917-090423/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.8979 - loss: 0.3454 - val_accuracy: 0.9182 - val_loss: 0.2762\n",
      "Epoch 8/100\n",
      "\u001b[1m839/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9060 - loss: 0.3202\n",
      "Epoch 8: val_accuracy improved from 0.91817 to 0.92317, saving model to MLP_Models\\Deep_MLP_Model-lr_0.001-bs_64-20250917-090423\\best_model_epoch-08_val_acc-0.9232.keras\n",
      "\n",
      "Epoch 8: val_accuracy improved from 0.91817 to 0.92317, saving model to wandb_models/Deep_MLP_Model-lr_0.001-bs_64-20250917-090423/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - accuracy: 0.9063 - loss: 0.3176 - val_accuracy: 0.9232 - val_loss: 0.2555\n",
      "Epoch 9/100\n",
      "\u001b[1m835/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9122 - loss: 0.2966\n",
      "Epoch 9: val_accuracy improved from 0.92317 to 0.92900, saving model to MLP_Models\\Deep_MLP_Model-lr_0.001-bs_64-20250917-090423\\best_model_epoch-09_val_acc-0.9290.keras\n",
      "\n",
      "Epoch 9: val_accuracy improved from 0.92317 to 0.92900, saving model to wandb_models/Deep_MLP_Model-lr_0.001-bs_64-20250917-090423/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.9120 - loss: 0.2952 - val_accuracy: 0.9290 - val_loss: 0.2385\n",
      "Epoch 10/100\n",
      "\u001b[1m839/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9184 - loss: 0.2770\n",
      "Epoch 10: val_accuracy improved from 0.92900 to 0.93433, saving model to MLP_Models\\Deep_MLP_Model-lr_0.001-bs_64-20250917-090423\\best_model_epoch-10_val_acc-0.9343.keras\n",
      "\n",
      "Epoch 10: val_accuracy improved from 0.92900 to 0.93433, saving model to wandb_models/Deep_MLP_Model-lr_0.001-bs_64-20250917-090423/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.9176 - loss: 0.2763 - val_accuracy: 0.9343 - val_loss: 0.2243\n",
      "Epoch 11/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9224 - loss: 0.2602\n",
      "Epoch 11: val_accuracy improved from 0.93433 to 0.93783, saving model to MLP_Models\\Deep_MLP_Model-lr_0.001-bs_64-20250917-090423\\best_model_epoch-11_val_acc-0.9378.keras\n",
      "\n",
      "Epoch 11: val_accuracy improved from 0.93433 to 0.93783, saving model to wandb_models/Deep_MLP_Model-lr_0.001-bs_64-20250917-090423/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 8ms/step - accuracy: 0.9219 - loss: 0.2600 - val_accuracy: 0.9378 - val_loss: 0.2120\n",
      "Epoch 12/100\n",
      "\u001b[1m836/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9266 - loss: 0.2455\n",
      "Epoch 12: val_accuracy improved from 0.93783 to 0.94217, saving model to MLP_Models\\Deep_MLP_Model-lr_0.001-bs_64-20250917-090423\\best_model_epoch-12_val_acc-0.9422.keras\n",
      "\n",
      "Epoch 12: val_accuracy improved from 0.93783 to 0.94217, saving model to wandb_models/Deep_MLP_Model-lr_0.001-bs_64-20250917-090423/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9262 - loss: 0.2456 - val_accuracy: 0.9422 - val_loss: 0.2012\n",
      "Epoch 13/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9303 - loss: 0.2325\n",
      "Epoch 13: val_accuracy improved from 0.94217 to 0.94467, saving model to MLP_Models\\Deep_MLP_Model-lr_0.001-bs_64-20250917-090423\\best_model_epoch-13_val_acc-0.9447.keras\n",
      "\n",
      "Epoch 13: val_accuracy improved from 0.94217 to 0.94467, saving model to wandb_models/Deep_MLP_Model-lr_0.001-bs_64-20250917-090423/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9302 - loss: 0.2330 - val_accuracy: 0.9447 - val_loss: 0.1914\n",
      "Epoch 14/100\n",
      "\u001b[1m833/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9344 - loss: 0.2210\n",
      "Epoch 14: val_accuracy improved from 0.94467 to 0.94733, saving model to MLP_Models\\Deep_MLP_Model-lr_0.001-bs_64-20250917-090423\\best_model_epoch-14_val_acc-0.9473.keras\n",
      "\n",
      "Epoch 14: val_accuracy improved from 0.94467 to 0.94733, saving model to wandb_models/Deep_MLP_Model-lr_0.001-bs_64-20250917-090423/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9339 - loss: 0.2217 - val_accuracy: 0.9473 - val_loss: 0.1830\n",
      "Epoch 15/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9369 - loss: 0.2107\n",
      "Epoch 15: val_accuracy improved from 0.94733 to 0.95017, saving model to MLP_Models\\Deep_MLP_Model-lr_0.001-bs_64-20250917-090423\\best_model_epoch-15_val_acc-0.9502.keras\n",
      "\n",
      "Epoch 15: val_accuracy improved from 0.94733 to 0.95017, saving model to wandb_models/Deep_MLP_Model-lr_0.001-bs_64-20250917-090423/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.9365 - loss: 0.2116 - val_accuracy: 0.9502 - val_loss: 0.1756\n",
      "Epoch 16/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9399 - loss: 0.2014\n",
      "Epoch 16: val_accuracy improved from 0.95017 to 0.95167, saving model to MLP_Models\\Deep_MLP_Model-lr_0.001-bs_64-20250917-090423\\best_model_epoch-16_val_acc-0.9517.keras\n",
      "\n",
      "Epoch 16: val_accuracy improved from 0.95017 to 0.95167, saving model to wandb_models/Deep_MLP_Model-lr_0.001-bs_64-20250917-090423/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9395 - loss: 0.2026 - val_accuracy: 0.9517 - val_loss: 0.1691\n",
      "Epoch 17/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9425 - loss: 0.1931\n",
      "Epoch 17: val_accuracy improved from 0.95167 to 0.95467, saving model to MLP_Models\\Deep_MLP_Model-lr_0.001-bs_64-20250917-090423\\best_model_epoch-17_val_acc-0.9547.keras\n",
      "\n",
      "Epoch 17: val_accuracy improved from 0.95167 to 0.95467, saving model to wandb_models/Deep_MLP_Model-lr_0.001-bs_64-20250917-090423/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.9420 - loss: 0.1944 - val_accuracy: 0.9547 - val_loss: 0.1634\n",
      "Epoch 18/100\n",
      "\u001b[1m832/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9446 - loss: 0.1856\n",
      "Epoch 18: val_accuracy improved from 0.95467 to 0.95700, saving model to MLP_Models\\Deep_MLP_Model-lr_0.001-bs_64-20250917-090423\\best_model_epoch-18_val_acc-0.9570.keras\n",
      "\n",
      "Epoch 18: val_accuracy improved from 0.95467 to 0.95700, saving model to wandb_models/Deep_MLP_Model-lr_0.001-bs_64-20250917-090423/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9439 - loss: 0.1870 - val_accuracy: 0.9570 - val_loss: 0.1581\n",
      "Epoch 19/100\n",
      "\u001b[1m839/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9466 - loss: 0.1788\n",
      "Epoch 19: val_accuracy improved from 0.95700 to 0.95767, saving model to MLP_Models\\Deep_MLP_Model-lr_0.001-bs_64-20250917-090423\\best_model_epoch-19_val_acc-0.9577.keras\n",
      "\n",
      "Epoch 19: val_accuracy improved from 0.95700 to 0.95767, saving model to wandb_models/Deep_MLP_Model-lr_0.001-bs_64-20250917-090423/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9458 - loss: 0.1802 - val_accuracy: 0.9577 - val_loss: 0.1535\n",
      "Epoch 20/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9484 - loss: 0.1726\n",
      "Epoch 20: val_accuracy improved from 0.95767 to 0.95867, saving model to MLP_Models\\Deep_MLP_Model-lr_0.001-bs_64-20250917-090423\\best_model_epoch-20_val_acc-0.9587.keras\n",
      "\n",
      "Epoch 20: val_accuracy improved from 0.95767 to 0.95867, saving model to wandb_models/Deep_MLP_Model-lr_0.001-bs_64-20250917-090423/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9477 - loss: 0.1739 - val_accuracy: 0.9587 - val_loss: 0.1493\n",
      "Epoch 21/100\n",
      "\u001b[1m830/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9499 - loss: 0.1668\n",
      "Epoch 21: val_accuracy improved from 0.95867 to 0.95967, saving model to MLP_Models\\Deep_MLP_Model-lr_0.001-bs_64-20250917-090423\\best_model_epoch-21_val_acc-0.9597.keras\n",
      "\n",
      "Epoch 21: val_accuracy improved from 0.95867 to 0.95967, saving model to wandb_models/Deep_MLP_Model-lr_0.001-bs_64-20250917-090423/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9494 - loss: 0.1681 - val_accuracy: 0.9597 - val_loss: 0.1455\n",
      "Epoch 22/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9515 - loss: 0.1614\n",
      "Epoch 22: val_accuracy improved from 0.95967 to 0.96067, saving model to MLP_Models\\Deep_MLP_Model-lr_0.001-bs_64-20250917-090423\\best_model_epoch-22_val_acc-0.9607.keras\n",
      "\n",
      "Epoch 22: val_accuracy improved from 0.95967 to 0.96067, saving model to wandb_models/Deep_MLP_Model-lr_0.001-bs_64-20250917-090423/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9511 - loss: 0.1627 - val_accuracy: 0.9607 - val_loss: 0.1420\n",
      "Epoch 23/100\n",
      "\u001b[1m829/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9531 - loss: 0.1564\n",
      "Epoch 23: val_accuracy improved from 0.96067 to 0.96217, saving model to MLP_Models\\Deep_MLP_Model-lr_0.001-bs_64-20250917-090423\\best_model_epoch-23_val_acc-0.9622.keras\n",
      "\n",
      "Epoch 23: val_accuracy improved from 0.96067 to 0.96217, saving model to wandb_models/Deep_MLP_Model-lr_0.001-bs_64-20250917-090423/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9529 - loss: 0.1577 - val_accuracy: 0.9622 - val_loss: 0.1388\n",
      "Epoch 24/100\n",
      "\u001b[1m834/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9545 - loss: 0.1518\n",
      "Epoch 24: val_accuracy improved from 0.96217 to 0.96267, saving model to MLP_Models\\Deep_MLP_Model-lr_0.001-bs_64-20250917-090423\\best_model_epoch-24_val_acc-0.9627.keras\n",
      "\n",
      "Epoch 24: val_accuracy improved from 0.96217 to 0.96267, saving model to wandb_models/Deep_MLP_Model-lr_0.001-bs_64-20250917-090423/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9543 - loss: 0.1530 - val_accuracy: 0.9627 - val_loss: 0.1358\n",
      "Epoch 25/100\n",
      "\u001b[1m834/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9557 - loss: 0.1474\n",
      "Epoch 25: val_accuracy improved from 0.96267 to 0.96350, saving model to MLP_Models\\Deep_MLP_Model-lr_0.001-bs_64-20250917-090423\\best_model_epoch-25_val_acc-0.9635.keras\n",
      "\n",
      "Epoch 25: val_accuracy improved from 0.96267 to 0.96350, saving model to wandb_models/Deep_MLP_Model-lr_0.001-bs_64-20250917-090423/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9555 - loss: 0.1485 - val_accuracy: 0.9635 - val_loss: 0.1330\n",
      "Epoch 26/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9566 - loss: 0.1433\n",
      "Epoch 26: val_accuracy improved from 0.96350 to 0.96467, saving model to MLP_Models\\Deep_MLP_Model-lr_0.001-bs_64-20250917-090423\\best_model_epoch-26_val_acc-0.9647.keras\n",
      "\n",
      "Epoch 26: val_accuracy improved from 0.96350 to 0.96467, saving model to wandb_models/Deep_MLP_Model-lr_0.001-bs_64-20250917-090423/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9566 - loss: 0.1443 - val_accuracy: 0.9647 - val_loss: 0.1305\n",
      "Epoch 27/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9575 - loss: 0.1394\n",
      "Epoch 27: val_accuracy improved from 0.96467 to 0.96517, saving model to MLP_Models\\Deep_MLP_Model-lr_0.001-bs_64-20250917-090423\\best_model_epoch-27_val_acc-0.9652.keras\n",
      "\n",
      "Epoch 27: val_accuracy improved from 0.96467 to 0.96517, saving model to wandb_models/Deep_MLP_Model-lr_0.001-bs_64-20250917-090423/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9576 - loss: 0.1403 - val_accuracy: 0.9652 - val_loss: 0.1282\n",
      "Epoch 28/100\n",
      "\u001b[1m839/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9588 - loss: 0.1356\n",
      "Epoch 28: val_accuracy improved from 0.96517 to 0.96583, saving model to MLP_Models\\Deep_MLP_Model-lr_0.001-bs_64-20250917-090423\\best_model_epoch-28_val_acc-0.9658.keras\n",
      "\n",
      "Epoch 28: val_accuracy improved from 0.96517 to 0.96583, saving model to wandb_models/Deep_MLP_Model-lr_0.001-bs_64-20250917-090423/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9587 - loss: 0.1365 - val_accuracy: 0.9658 - val_loss: 0.1259\n",
      "Epoch 29/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9603 - loss: 0.1321\n",
      "Epoch 29: val_accuracy did not improve from 0.96583\n",
      "\n",
      "Epoch 29: val_accuracy did not improve from 0.96583\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9601 - loss: 0.1328 - val_accuracy: 0.9658 - val_loss: 0.1238\n",
      "Epoch 30/100\n",
      "\u001b[1m837/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9613 - loss: 0.1287\n",
      "Epoch 30: val_accuracy improved from 0.96583 to 0.96633, saving model to MLP_Models\\Deep_MLP_Model-lr_0.001-bs_64-20250917-090423\\best_model_epoch-30_val_acc-0.9663.keras\n",
      "\n",
      "Epoch 30: val_accuracy improved from 0.96583 to 0.96633, saving model to wandb_models/Deep_MLP_Model-lr_0.001-bs_64-20250917-090423/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9611 - loss: 0.1294 - val_accuracy: 0.9663 - val_loss: 0.1218\n",
      "Epoch 31/100\n",
      "\u001b[1m834/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9622 - loss: 0.1254\n",
      "Epoch 31: val_accuracy improved from 0.96633 to 0.96683, saving model to MLP_Models\\Deep_MLP_Model-lr_0.001-bs_64-20250917-090423\\best_model_epoch-31_val_acc-0.9668.keras\n",
      "\n",
      "Epoch 31: val_accuracy improved from 0.96633 to 0.96683, saving model to wandb_models/Deep_MLP_Model-lr_0.001-bs_64-20250917-090423/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9620 - loss: 0.1261 - val_accuracy: 0.9668 - val_loss: 0.1200\n",
      "Epoch 32/100\n",
      "\u001b[1m835/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9631 - loss: 0.1223\n",
      "Epoch 32: val_accuracy improved from 0.96683 to 0.96800, saving model to MLP_Models\\Deep_MLP_Model-lr_0.001-bs_64-20250917-090423\\best_model_epoch-32_val_acc-0.9680.keras\n",
      "\n",
      "Epoch 32: val_accuracy improved from 0.96683 to 0.96800, saving model to wandb_models/Deep_MLP_Model-lr_0.001-bs_64-20250917-090423/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9631 - loss: 0.1229 - val_accuracy: 0.9680 - val_loss: 0.1183\n",
      "Epoch 33/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9640 - loss: 0.1193\n",
      "Epoch 33: val_accuracy improved from 0.96800 to 0.96817, saving model to MLP_Models\\Deep_MLP_Model-lr_0.001-bs_64-20250917-090423\\best_model_epoch-33_val_acc-0.9682.keras\n",
      "\n",
      "Epoch 33: val_accuracy improved from 0.96800 to 0.96817, saving model to wandb_models/Deep_MLP_Model-lr_0.001-bs_64-20250917-090423/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9639 - loss: 0.1199 - val_accuracy: 0.9682 - val_loss: 0.1167\n",
      "Epoch 34/100\n",
      "\u001b[1m840/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9649 - loss: 0.1164\n",
      "Epoch 34: val_accuracy improved from 0.96817 to 0.96867, saving model to MLP_Models\\Deep_MLP_Model-lr_0.001-bs_64-20250917-090423\\best_model_epoch-34_val_acc-0.9687.keras\n",
      "\n",
      "Epoch 34: val_accuracy improved from 0.96817 to 0.96867, saving model to wandb_models/Deep_MLP_Model-lr_0.001-bs_64-20250917-090423/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9650 - loss: 0.1169 - val_accuracy: 0.9687 - val_loss: 0.1151\n",
      "Epoch 35/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9655 - loss: 0.1136\n",
      "Epoch 35: val_accuracy improved from 0.96867 to 0.96900, saving model to MLP_Models\\Deep_MLP_Model-lr_0.001-bs_64-20250917-090423\\best_model_epoch-35_val_acc-0.9690.keras\n",
      "\n",
      "Epoch 35: val_accuracy improved from 0.96867 to 0.96900, saving model to wandb_models/Deep_MLP_Model-lr_0.001-bs_64-20250917-090423/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9657 - loss: 0.1141 - val_accuracy: 0.9690 - val_loss: 0.1137\n",
      "Epoch 36/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9661 - loss: 0.1109\n",
      "Epoch 36: val_accuracy improved from 0.96900 to 0.96950, saving model to MLP_Models\\Deep_MLP_Model-lr_0.001-bs_64-20250917-090423\\best_model_epoch-36_val_acc-0.9695.keras\n",
      "\n",
      "Epoch 36: val_accuracy improved from 0.96900 to 0.96950, saving model to wandb_models/Deep_MLP_Model-lr_0.001-bs_64-20250917-090423/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9662 - loss: 0.1114 - val_accuracy: 0.9695 - val_loss: 0.1124\n",
      "Epoch 37/100\n",
      "\u001b[1m835/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9672 - loss: 0.1083\n",
      "Epoch 37: val_accuracy improved from 0.96950 to 0.97000, saving model to MLP_Models\\Deep_MLP_Model-lr_0.001-bs_64-20250917-090423\\best_model_epoch-37_val_acc-0.9700.keras\n",
      "\n",
      "Epoch 37: val_accuracy improved from 0.96950 to 0.97000, saving model to wandb_models/Deep_MLP_Model-lr_0.001-bs_64-20250917-090423/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9671 - loss: 0.1088 - val_accuracy: 0.9700 - val_loss: 0.1112\n",
      "Epoch 38/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9681 - loss: 0.1058\n",
      "Epoch 38: val_accuracy improved from 0.97000 to 0.97033, saving model to MLP_Models\\Deep_MLP_Model-lr_0.001-bs_64-20250917-090423\\best_model_epoch-38_val_acc-0.9703.keras\n",
      "\n",
      "Epoch 38: val_accuracy improved from 0.97000 to 0.97033, saving model to wandb_models/Deep_MLP_Model-lr_0.001-bs_64-20250917-090423/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9679 - loss: 0.1063 - val_accuracy: 0.9703 - val_loss: 0.1100\n",
      "Epoch 39/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9685 - loss: 0.1034\n",
      "Epoch 39: val_accuracy improved from 0.97033 to 0.97050, saving model to MLP_Models\\Deep_MLP_Model-lr_0.001-bs_64-20250917-090423\\best_model_epoch-39_val_acc-0.9705.keras\n",
      "\n",
      "Epoch 39: val_accuracy improved from 0.97033 to 0.97050, saving model to wandb_models/Deep_MLP_Model-lr_0.001-bs_64-20250917-090423/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9683 - loss: 0.1038 - val_accuracy: 0.9705 - val_loss: 0.1090\n",
      "Epoch 40/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9692 - loss: 0.1010\n",
      "Epoch 40: val_accuracy improved from 0.97050 to 0.97067, saving model to MLP_Models\\Deep_MLP_Model-lr_0.001-bs_64-20250917-090423\\best_model_epoch-40_val_acc-0.9707.keras\n",
      "\n",
      "Epoch 40: val_accuracy improved from 0.97050 to 0.97067, saving model to wandb_models/Deep_MLP_Model-lr_0.001-bs_64-20250917-090423/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9690 - loss: 0.1014 - val_accuracy: 0.9707 - val_loss: 0.1080\n",
      "Epoch 41/100\n",
      "\u001b[1m839/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9702 - loss: 0.0987\n",
      "Epoch 41: val_accuracy did not improve from 0.97067\n",
      "\n",
      "Epoch 41: val_accuracy did not improve from 0.97067\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.9700 - loss: 0.0991 - val_accuracy: 0.9703 - val_loss: 0.1070\n",
      "Epoch 42/100\n",
      "\u001b[1m836/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9709 - loss: 0.0965\n",
      "Epoch 42: val_accuracy did not improve from 0.97067\n",
      "\n",
      "Epoch 42: val_accuracy did not improve from 0.97067\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.9707 - loss: 0.0969 - val_accuracy: 0.9703 - val_loss: 0.1061\n",
      "Epoch 43/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9719 - loss: 0.0943\n",
      "Epoch 43: val_accuracy did not improve from 0.97067\n",
      "\n",
      "Epoch 43: val_accuracy did not improve from 0.97067\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.9714 - loss: 0.0948 - val_accuracy: 0.9705 - val_loss: 0.1053\n",
      "Epoch 44/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9724 - loss: 0.0922\n",
      "Epoch 44: val_accuracy improved from 0.97067 to 0.97117, saving model to MLP_Models\\Deep_MLP_Model-lr_0.001-bs_64-20250917-090423\\best_model_epoch-44_val_acc-0.9712.keras\n",
      "\n",
      "Epoch 44: val_accuracy improved from 0.97067 to 0.97117, saving model to wandb_models/Deep_MLP_Model-lr_0.001-bs_64-20250917-090423/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.9719 - loss: 0.0926 - val_accuracy: 0.9712 - val_loss: 0.1044\n",
      "Epoch 45/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9731 - loss: 0.0902\n",
      "Epoch 45: val_accuracy improved from 0.97117 to 0.97133, saving model to MLP_Models\\Deep_MLP_Model-lr_0.001-bs_64-20250917-090423\\best_model_epoch-45_val_acc-0.9713.keras\n",
      "\n",
      "Epoch 45: val_accuracy improved from 0.97117 to 0.97133, saving model to wandb_models/Deep_MLP_Model-lr_0.001-bs_64-20250917-090423/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.9726 - loss: 0.0906 - val_accuracy: 0.9713 - val_loss: 0.1037\n",
      "Epoch 46/100\n",
      "\u001b[1m838/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9736 - loss: 0.0882\n",
      "Epoch 46: val_accuracy improved from 0.97133 to 0.97167, saving model to MLP_Models\\Deep_MLP_Model-lr_0.001-bs_64-20250917-090423\\best_model_epoch-46_val_acc-0.9717.keras\n",
      "\n",
      "Epoch 46: val_accuracy improved from 0.97133 to 0.97167, saving model to wandb_models/Deep_MLP_Model-lr_0.001-bs_64-20250917-090423/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9733 - loss: 0.0886 - val_accuracy: 0.9717 - val_loss: 0.1030\n",
      "Epoch 47/100\n",
      "\u001b[1m831/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9742 - loss: 0.0863\n",
      "Epoch 47: val_accuracy improved from 0.97167 to 0.97200, saving model to MLP_Models\\Deep_MLP_Model-lr_0.001-bs_64-20250917-090423\\best_model_epoch-47_val_acc-0.9720.keras\n",
      "\n",
      "Epoch 47: val_accuracy improved from 0.97167 to 0.97200, saving model to wandb_models/Deep_MLP_Model-lr_0.001-bs_64-20250917-090423/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9739 - loss: 0.0867 - val_accuracy: 0.9720 - val_loss: 0.1024\n",
      "Epoch 48/100\n",
      "\u001b[1m833/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9749 - loss: 0.0844\n",
      "Epoch 48: val_accuracy improved from 0.97200 to 0.97217, saving model to MLP_Models\\Deep_MLP_Model-lr_0.001-bs_64-20250917-090423\\best_model_epoch-48_val_acc-0.9722.keras\n",
      "\n",
      "Epoch 48: val_accuracy improved from 0.97200 to 0.97217, saving model to wandb_models/Deep_MLP_Model-lr_0.001-bs_64-20250917-090423/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9745 - loss: 0.0848 - val_accuracy: 0.9722 - val_loss: 0.1018\n",
      "Epoch 49/100\n",
      "\u001b[1m831/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9756 - loss: 0.0826\n",
      "Epoch 49: val_accuracy improved from 0.97217 to 0.97250, saving model to MLP_Models\\Deep_MLP_Model-lr_0.001-bs_64-20250917-090423\\best_model_epoch-49_val_acc-0.9725.keras\n",
      "\n",
      "Epoch 49: val_accuracy improved from 0.97217 to 0.97250, saving model to wandb_models/Deep_MLP_Model-lr_0.001-bs_64-20250917-090423/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9752 - loss: 0.0830 - val_accuracy: 0.9725 - val_loss: 0.1012\n",
      "Epoch 50/100\n",
      "\u001b[1m831/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9763 - loss: 0.0808\n",
      "Epoch 50: val_accuracy did not improve from 0.97250\n",
      "\n",
      "Epoch 50: val_accuracy did not improve from 0.97250\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9758 - loss: 0.0812 - val_accuracy: 0.9723 - val_loss: 0.1008\n",
      "Epoch 51/100\n",
      "\u001b[1m835/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9767 - loss: 0.0791\n",
      "Epoch 51: val_accuracy improved from 0.97250 to 0.97267, saving model to MLP_Models\\Deep_MLP_Model-lr_0.001-bs_64-20250917-090423\\best_model_epoch-51_val_acc-0.9727.keras\n",
      "\n",
      "Epoch 51: val_accuracy improved from 0.97250 to 0.97267, saving model to wandb_models/Deep_MLP_Model-lr_0.001-bs_64-20250917-090423/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9763 - loss: 0.0795 - val_accuracy: 0.9727 - val_loss: 0.1003\n",
      "Epoch 52/100\n",
      "\u001b[1m832/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9773 - loss: 0.0774\n",
      "Epoch 52: val_accuracy did not improve from 0.97267\n",
      "\n",
      "Epoch 52: val_accuracy did not improve from 0.97267\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9770 - loss: 0.0778 - val_accuracy: 0.9723 - val_loss: 0.0999\n",
      "Epoch 53/100\n",
      "\u001b[1m836/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9777 - loss: 0.0758\n",
      "Epoch 53: val_accuracy did not improve from 0.97267\n",
      "\n",
      "Epoch 53: val_accuracy did not improve from 0.97267\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9776 - loss: 0.0762 - val_accuracy: 0.9725 - val_loss: 0.0993\n",
      "Epoch 54/100\n",
      "\u001b[1m834/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9785 - loss: 0.0742\n",
      "Epoch 54: val_accuracy did not improve from 0.97267\n",
      "\n",
      "Epoch 54: val_accuracy did not improve from 0.97267\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.9782 - loss: 0.0746 - val_accuracy: 0.9723 - val_loss: 0.0989\n",
      "Epoch 55/100\n",
      "\u001b[1m838/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9790 - loss: 0.0726\n",
      "Epoch 55: val_accuracy did not improve from 0.97267\n",
      "\n",
      "Epoch 55: val_accuracy did not improve from 0.97267\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.9787 - loss: 0.0730 - val_accuracy: 0.9727 - val_loss: 0.0987\n",
      "Epoch 56/100\n",
      "\u001b[1m832/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9795 - loss: 0.0711\n",
      "Epoch 56: val_accuracy did not improve from 0.97267\n",
      "\n",
      "Epoch 56: val_accuracy did not improve from 0.97267\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9791 - loss: 0.0715 - val_accuracy: 0.9727 - val_loss: 0.0985\n",
      "Epoch 57/100\n",
      "\u001b[1m835/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9799 - loss: 0.0696\n",
      "Epoch 57: val_accuracy improved from 0.97267 to 0.97317, saving model to MLP_Models\\Deep_MLP_Model-lr_0.001-bs_64-20250917-090423\\best_model_epoch-57_val_acc-0.9732.keras\n",
      "\n",
      "Epoch 57: val_accuracy improved from 0.97267 to 0.97317, saving model to wandb_models/Deep_MLP_Model-lr_0.001-bs_64-20250917-090423/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9796 - loss: 0.0700 - val_accuracy: 0.9732 - val_loss: 0.0982\n",
      "Epoch 58/100\n",
      "\u001b[1m839/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9805 - loss: 0.0681\n",
      "Epoch 58: val_accuracy improved from 0.97317 to 0.97333, saving model to MLP_Models\\Deep_MLP_Model-lr_0.001-bs_64-20250917-090423\\best_model_epoch-58_val_acc-0.9733.keras\n",
      "\n",
      "Epoch 58: val_accuracy improved from 0.97317 to 0.97333, saving model to wandb_models/Deep_MLP_Model-lr_0.001-bs_64-20250917-090423/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9801 - loss: 0.0685 - val_accuracy: 0.9733 - val_loss: 0.0980\n",
      "Epoch 59/100\n",
      "\u001b[1m840/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9808 - loss: 0.0667\n",
      "Epoch 59: val_accuracy did not improve from 0.97333\n",
      "\n",
      "Epoch 59: val_accuracy did not improve from 0.97333\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.9805 - loss: 0.0671 - val_accuracy: 0.9732 - val_loss: 0.0977\n",
      "Epoch 60/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9813 - loss: 0.0653\n",
      "Epoch 60: val_accuracy did not improve from 0.97333\n",
      "\n",
      "Epoch 60: val_accuracy did not improve from 0.97333\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.9810 - loss: 0.0657 - val_accuracy: 0.9732 - val_loss: 0.0975\n",
      "Epoch 61/100\n",
      "\u001b[1m836/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9816 - loss: 0.0639\n",
      "Epoch 61: val_accuracy did not improve from 0.97333\n",
      "\n",
      "Epoch 61: val_accuracy did not improve from 0.97333\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9814 - loss: 0.0643 - val_accuracy: 0.9728 - val_loss: 0.0973\n",
      "Epoch 62/100\n",
      "\u001b[1m839/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9821 - loss: 0.0626\n",
      "Epoch 62: val_accuracy did not improve from 0.97333\n",
      "\n",
      "Epoch 62: val_accuracy did not improve from 0.97333\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9818 - loss: 0.0630 - val_accuracy: 0.9732 - val_loss: 0.0971\n",
      "Epoch 63/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9825 - loss: 0.0613\n",
      "Epoch 63: val_accuracy did not improve from 0.97333\n",
      "\n",
      "Epoch 63: val_accuracy did not improve from 0.97333\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9824 - loss: 0.0617 - val_accuracy: 0.9733 - val_loss: 0.0970\n",
      "Epoch 64/100\n",
      "\u001b[1m838/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9828 - loss: 0.0600\n",
      "Epoch 64: val_accuracy improved from 0.97333 to 0.97350, saving model to MLP_Models\\Deep_MLP_Model-lr_0.001-bs_64-20250917-090423\\best_model_epoch-64_val_acc-0.9735.keras\n",
      "\n",
      "Epoch 64: val_accuracy improved from 0.97333 to 0.97350, saving model to wandb_models/Deep_MLP_Model-lr_0.001-bs_64-20250917-090423/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.9828 - loss: 0.0604 - val_accuracy: 0.9735 - val_loss: 0.0969\n",
      "Epoch 65/100\n",
      "\u001b[1m838/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9830 - loss: 0.0587\n",
      "Epoch 65: val_accuracy did not improve from 0.97350\n",
      "\n",
      "Epoch 65: val_accuracy did not improve from 0.97350\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.9832 - loss: 0.0591 - val_accuracy: 0.9735 - val_loss: 0.0967\n",
      "Epoch 66/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9836 - loss: 0.0575\n",
      "Epoch 66: val_accuracy improved from 0.97350 to 0.97400, saving model to MLP_Models\\Deep_MLP_Model-lr_0.001-bs_64-20250917-090423\\best_model_epoch-66_val_acc-0.9740.keras\n",
      "\n",
      "Epoch 66: val_accuracy improved from 0.97350 to 0.97400, saving model to wandb_models/Deep_MLP_Model-lr_0.001-bs_64-20250917-090423/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.9836 - loss: 0.0578 - val_accuracy: 0.9740 - val_loss: 0.0965\n",
      "Epoch 67/100\n",
      "\u001b[1m840/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9839 - loss: 0.0562\n",
      "Epoch 67: val_accuracy did not improve from 0.97400\n",
      "\n",
      "Epoch 67: val_accuracy did not improve from 0.97400\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.9840 - loss: 0.0566 - val_accuracy: 0.9740 - val_loss: 0.0964\n",
      "Epoch 68/100\n",
      "\u001b[1m835/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9842 - loss: 0.0551\n",
      "Epoch 68: val_accuracy improved from 0.97400 to 0.97417, saving model to MLP_Models\\Deep_MLP_Model-lr_0.001-bs_64-20250917-090423\\best_model_epoch-68_val_acc-0.9742.keras\n",
      "\n",
      "Epoch 68: val_accuracy improved from 0.97400 to 0.97417, saving model to wandb_models/Deep_MLP_Model-lr_0.001-bs_64-20250917-090423/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.9843 - loss: 0.0554 - val_accuracy: 0.9742 - val_loss: 0.0964\n",
      "Epoch 69/100\n",
      "\u001b[1m834/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9846 - loss: 0.0539\n",
      "Epoch 69: val_accuracy did not improve from 0.97417\n",
      "\n",
      "Epoch 69: val_accuracy did not improve from 0.97417\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9848 - loss: 0.0543 - val_accuracy: 0.9742 - val_loss: 0.0960\n",
      "Epoch 70/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9849 - loss: 0.0527\n",
      "Epoch 70: val_accuracy improved from 0.97417 to 0.97433, saving model to MLP_Models\\Deep_MLP_Model-lr_0.001-bs_64-20250917-090423\\best_model_epoch-70_val_acc-0.9743.keras\n",
      "\n",
      "Epoch 70: val_accuracy improved from 0.97417 to 0.97433, saving model to wandb_models/Deep_MLP_Model-lr_0.001-bs_64-20250917-090423/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9850 - loss: 0.0531 - val_accuracy: 0.9743 - val_loss: 0.0960\n",
      "Epoch 71/100\n",
      "\u001b[1m832/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9854 - loss: 0.0516\n",
      "Epoch 71: val_accuracy did not improve from 0.97433\n",
      "\n",
      "Epoch 71: val_accuracy did not improve from 0.97433\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9854 - loss: 0.0520 - val_accuracy: 0.9740 - val_loss: 0.0959\n",
      "Epoch 72/100\n",
      "\u001b[1m836/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9858 - loss: 0.0505\n",
      "Epoch 72: val_accuracy did not improve from 0.97433\n",
      "\n",
      "Epoch 72: val_accuracy did not improve from 0.97433\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9858 - loss: 0.0509 - val_accuracy: 0.9740 - val_loss: 0.0958\n",
      "Epoch 73/100\n",
      "\u001b[1m836/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9863 - loss: 0.0494\n",
      "Epoch 73: val_accuracy did not improve from 0.97433\n",
      "\n",
      "Epoch 73: val_accuracy did not improve from 0.97433\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9862 - loss: 0.0498 - val_accuracy: 0.9740 - val_loss: 0.0958\n",
      "Epoch 74/100\n",
      "\u001b[1m835/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9868 - loss: 0.0484\n",
      "Epoch 74: val_accuracy did not improve from 0.97433\n",
      "\n",
      "Epoch 74: val_accuracy did not improve from 0.97433\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9866 - loss: 0.0488 - val_accuracy: 0.9742 - val_loss: 0.0957\n",
      "Epoch 75/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9872 - loss: 0.0474\n",
      "Epoch 75: val_accuracy did not improve from 0.97433\n",
      "\n",
      "Epoch 75: val_accuracy did not improve from 0.97433\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.9870 - loss: 0.0477 - val_accuracy: 0.9742 - val_loss: 0.0957\n",
      "Epoch 76/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9875 - loss: 0.0464\n",
      "Epoch 76: val_accuracy did not improve from 0.97433\n",
      "\n",
      "Epoch 76: val_accuracy did not improve from 0.97433\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.9873 - loss: 0.0467 - val_accuracy: 0.9743 - val_loss: 0.0957\n",
      "Epoch 77/100\n",
      "\u001b[1m832/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9878 - loss: 0.0454\n",
      "Epoch 77: val_accuracy improved from 0.97433 to 0.97450, saving model to MLP_Models\\Deep_MLP_Model-lr_0.001-bs_64-20250917-090423\\best_model_epoch-77_val_acc-0.9745.keras\n",
      "\n",
      "Epoch 77: val_accuracy improved from 0.97433 to 0.97450, saving model to wandb_models/Deep_MLP_Model-lr_0.001-bs_64-20250917-090423/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.9876 - loss: 0.0457 - val_accuracy: 0.9745 - val_loss: 0.0957\n",
      "Epoch 78/100\n",
      "\u001b[1m839/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9881 - loss: 0.0444\n",
      "Epoch 78: val_accuracy improved from 0.97450 to 0.97467, saving model to MLP_Models\\Deep_MLP_Model-lr_0.001-bs_64-20250917-090423\\best_model_epoch-78_val_acc-0.9747.keras\n",
      "\n",
      "Epoch 78: val_accuracy improved from 0.97450 to 0.97467, saving model to wandb_models/Deep_MLP_Model-lr_0.001-bs_64-20250917-090423/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.9880 - loss: 0.0447 - val_accuracy: 0.9747 - val_loss: 0.0958\n",
      "Epoch 79/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9884 - loss: 0.0434\n",
      "Epoch 79: val_accuracy improved from 0.97467 to 0.97483, saving model to MLP_Models\\Deep_MLP_Model-lr_0.001-bs_64-20250917-090423\\best_model_epoch-79_val_acc-0.9748.keras\n",
      "\n",
      "Epoch 79: val_accuracy improved from 0.97467 to 0.97483, saving model to wandb_models/Deep_MLP_Model-lr_0.001-bs_64-20250917-090423/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.9883 - loss: 0.0438 - val_accuracy: 0.9748 - val_loss: 0.0958\n",
      "Epoch 80/100\n",
      "\u001b[1m832/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9887 - loss: 0.0425\n",
      "Epoch 80: val_accuracy improved from 0.97483 to 0.97517, saving model to MLP_Models\\Deep_MLP_Model-lr_0.001-bs_64-20250917-090423\\best_model_epoch-80_val_acc-0.9752.keras\n",
      "\n",
      "Epoch 80: val_accuracy improved from 0.97483 to 0.97517, saving model to wandb_models/Deep_MLP_Model-lr_0.001-bs_64-20250917-090423/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9886 - loss: 0.0428 - val_accuracy: 0.9752 - val_loss: 0.0958\n",
      "Epoch 81/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9890 - loss: 0.0416\n",
      "Epoch 81: val_accuracy did not improve from 0.97517\n",
      "\n",
      "Epoch 81: val_accuracy did not improve from 0.97517\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.9889 - loss: 0.0419 - val_accuracy: 0.9752 - val_loss: 0.0959\n",
      "Epoch 82/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9894 - loss: 0.0407\n",
      "Epoch 82: val_accuracy improved from 0.97517 to 0.97583, saving model to MLP_Models\\Deep_MLP_Model-lr_0.001-bs_64-20250917-090423\\best_model_epoch-82_val_acc-0.9758.keras\n",
      "\n",
      "Epoch 82: val_accuracy improved from 0.97517 to 0.97583, saving model to wandb_models/Deep_MLP_Model-lr_0.001-bs_64-20250917-090423/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9893 - loss: 0.0410 - val_accuracy: 0.9758 - val_loss: 0.0959\n",
      "Epoch 83/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9897 - loss: 0.0398\n",
      "Epoch 83: val_accuracy did not improve from 0.97583\n",
      "\n",
      "Epoch 83: val_accuracy did not improve from 0.97583\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9896 - loss: 0.0401 - val_accuracy: 0.9758 - val_loss: 0.0960\n",
      "Epoch 84/100\n",
      "\u001b[1m834/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9900 - loss: 0.0389\n",
      "Epoch 84: val_accuracy did not improve from 0.97583\n",
      "\n",
      "Epoch 84: val_accuracy did not improve from 0.97583\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9899 - loss: 0.0392 - val_accuracy: 0.9758 - val_loss: 0.0961\n",
      "Epoch 85/100\n",
      "\u001b[1m835/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9902 - loss: 0.0381\n",
      "Epoch 85: val_accuracy did not improve from 0.97583\n",
      "\n",
      "Epoch 85: val_accuracy did not improve from 0.97583\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.9901 - loss: 0.0384 - val_accuracy: 0.9758 - val_loss: 0.0962\n",
      "Epoch 86/100\n",
      "\u001b[1m840/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9904 - loss: 0.0373\n",
      "Epoch 86: val_accuracy did not improve from 0.97583\n",
      "\n",
      "Epoch 86: val_accuracy did not improve from 0.97583\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.9903 - loss: 0.0375 - val_accuracy: 0.9757 - val_loss: 0.0964\n",
      "Epoch 87/100\n",
      "\u001b[1m839/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9907 - loss: 0.0364\n",
      "Epoch 87: val_accuracy did not improve from 0.97583\n",
      "\n",
      "Epoch 87: val_accuracy did not improve from 0.97583\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - accuracy: 0.9905 - loss: 0.0367 - val_accuracy: 0.9757 - val_loss: 0.0964\n",
      "Epoch 88/100\n",
      "\u001b[1m835/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9909 - loss: 0.0357\n",
      "Epoch 88: val_accuracy did not improve from 0.97583\n",
      "\n",
      "Epoch 88: val_accuracy did not improve from 0.97583\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9907 - loss: 0.0359 - val_accuracy: 0.9750 - val_loss: 0.0966\n",
      "Epoch 89/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9911 - loss: 0.0349\n",
      "Epoch 89: val_accuracy did not improve from 0.97583\n",
      "\n",
      "Epoch 89: val_accuracy did not improve from 0.97583\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 8ms/step - accuracy: 0.9910 - loss: 0.0351 - val_accuracy: 0.9750 - val_loss: 0.0968\n",
      "Epoch 90/100\n",
      "\u001b[1m837/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9916 - loss: 0.0341\n",
      "Epoch 90: val_accuracy did not improve from 0.97583\n",
      "\n",
      "Epoch 90: val_accuracy did not improve from 0.97583\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9914 - loss: 0.0343 - val_accuracy: 0.9752 - val_loss: 0.0969\n",
      "Epoch 91/100\n",
      "\u001b[1m838/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9920 - loss: 0.0334\n",
      "Epoch 91: val_accuracy did not improve from 0.97583\n",
      "\n",
      "Epoch 91: val_accuracy did not improve from 0.97583\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9917 - loss: 0.0336 - val_accuracy: 0.9753 - val_loss: 0.0971\n",
      "Epoch 92/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9922 - loss: 0.0326\n",
      "Epoch 92: val_accuracy did not improve from 0.97583\n",
      "\n",
      "Epoch 92: val_accuracy did not improve from 0.97583\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9921 - loss: 0.0328 - val_accuracy: 0.9752 - val_loss: 0.0973\n",
      "\n",
      "Training history saved to: MLP_Models\\Deep_MLP_Model-lr_0.001-bs_64-20250917-090423\\training_history.pkl\n",
      "\n",
      "--- Peak Performance Summary ---\n",
      "Best validation accuracy:           0.9758\n",
      "Associated training accuracy:       0.9893\n",
      "Occurred at epoch:                  82\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch/accuracy</td><td>▁▃▄▅▅▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇██████████████████</td></tr><tr><td>epoch/epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇██</td></tr><tr><td>epoch/learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/loss</td><td>█▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/val_accuracy</td><td>▁▅▇▇████████████████████████████████████</td></tr><tr><td>epoch/val_loss</td><td>█▅▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch/accuracy</td><td>0.99206</td></tr><tr><td>epoch/epoch</td><td>91</td></tr><tr><td>epoch/learning_rate</td><td>0.001</td></tr><tr><td>epoch/loss</td><td>0.03281</td></tr><tr><td>epoch/val_accuracy</td><td>0.97517</td></tr><tr><td>epoch/val_loss</td><td>0.09729</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Deep_MLP_Model-lr_0.001-bs_64-20250917-090423</strong> at: <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/odwg102b' target=\"_blank\">https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/odwg102b</a><br> View project at: <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2' target=\"_blank\">https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2</a><br>Synced 5 W&B file(s), 0 media file(s), 114 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250917_090423-odwg102b\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Adapting the normalisation layer...\n",
      "Adaptation complete.\n",
      "\n",
      "\n",
      "--- Starting Experiment: Deep_MLP_Model ---\n",
      "\n",
      "--- Model Architecture ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"Deep_MLP_Model\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"Deep_MLP_Model\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ normalization_19                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Normalization</span>)                 │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_88 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">100,480</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_89 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">33,024</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_90 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">65,792</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_91 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">65,792</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_92 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">65,792</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_93 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,448</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_94 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_95 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">330</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ normalization_19                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m1\u001b[0m)      │             \u001b[38;5;34m3\u001b[0m │\n",
       "│ (\u001b[38;5;33mNormalization\u001b[0m)                 │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_19 (\u001b[38;5;33mFlatten\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m784\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_88 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m100,480\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_89 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │        \u001b[38;5;34m33,024\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_90 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │        \u001b[38;5;34m65,792\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_91 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │        \u001b[38;5;34m65,792\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_92 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │        \u001b[38;5;34m65,792\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_93 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m16,448\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_94 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m2,080\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_95 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │           \u001b[38;5;34m330\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">349,741</span> (1.33 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m349,741\u001b[0m (1.33 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">349,738</span> (1.33 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m349,738\u001b[0m (1.33 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3</span> (16.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m3\u001b[0m (16.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Hyperparameters ---\n",
      "optimiser           : SGD\n",
      "learning_rate       : 0.01\n",
      "epochs              : 100\n",
      "batch_size          : 64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "creating run (0.0s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\TimVos\\VSC Projects\\CSE5ML\\Assessment 2\\wandb\\run-20250917_091243-vzn75ovw</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/vzn75ovw' target=\"_blank\">Deep_MLP_Model-lr_0.01-bs_64-20250917-091243</a></strong> to <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2' target=\"_blank\">https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/vzn75ovw' target=\"_blank\">https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/vzn75ovw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m834/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4991 - loss: 1.5038\n",
      "Epoch 1: val_accuracy improved from None to 0.92133, saving model to MLP_Models\\Deep_MLP_Model-lr_0.01-bs_64-20250917-091243\\best_model_epoch-01_val_acc-0.9213.keras\n",
      "\n",
      "Epoch 1: val_accuracy improved from None to 0.92133, saving model to wandb_models/Deep_MLP_Model-lr_0.01-bs_64-20250917-091243/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 9ms/step - accuracy: 0.7223 - loss: 0.8885 - val_accuracy: 0.9213 - val_loss: 0.2646\n",
      "Epoch 2/100\n",
      "\u001b[1m833/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9149 - loss: 0.2838\n",
      "Epoch 2: val_accuracy improved from 0.92133 to 0.94683, saving model to MLP_Models\\Deep_MLP_Model-lr_0.01-bs_64-20250917-091243\\best_model_epoch-02_val_acc-0.9468.keras\n",
      "\n",
      "Epoch 2: val_accuracy improved from 0.92133 to 0.94683, saving model to wandb_models/Deep_MLP_Model-lr_0.01-bs_64-20250917-091243/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.9236 - loss: 0.2556 - val_accuracy: 0.9468 - val_loss: 0.1811\n",
      "Epoch 3/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9422 - loss: 0.1922\n",
      "Epoch 3: val_accuracy improved from 0.94683 to 0.95667, saving model to MLP_Models\\Deep_MLP_Model-lr_0.01-bs_64-20250917-091243\\best_model_epoch-03_val_acc-0.9567.keras\n",
      "\n",
      "Epoch 3: val_accuracy improved from 0.94683 to 0.95667, saving model to wandb_models/Deep_MLP_Model-lr_0.01-bs_64-20250917-091243/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.9460 - loss: 0.1812 - val_accuracy: 0.9567 - val_loss: 0.1494\n",
      "Epoch 4/100\n",
      "\u001b[1m838/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9548 - loss: 0.1483\n",
      "Epoch 4: val_accuracy improved from 0.95667 to 0.96283, saving model to MLP_Models\\Deep_MLP_Model-lr_0.01-bs_64-20250917-091243\\best_model_epoch-04_val_acc-0.9628.keras\n",
      "\n",
      "Epoch 4: val_accuracy improved from 0.95667 to 0.96283, saving model to wandb_models/Deep_MLP_Model-lr_0.01-bs_64-20250917-091243/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.9570 - loss: 0.1424 - val_accuracy: 0.9628 - val_loss: 0.1294\n",
      "Epoch 5/100\n",
      "\u001b[1m837/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9640 - loss: 0.1204\n",
      "Epoch 5: val_accuracy improved from 0.96283 to 0.96767, saving model to MLP_Models\\Deep_MLP_Model-lr_0.01-bs_64-20250917-091243\\best_model_epoch-05_val_acc-0.9677.keras\n",
      "\n",
      "Epoch 5: val_accuracy improved from 0.96283 to 0.96767, saving model to wandb_models/Deep_MLP_Model-lr_0.01-bs_64-20250917-091243/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.9650 - loss: 0.1168 - val_accuracy: 0.9677 - val_loss: 0.1154\n",
      "Epoch 6/100\n",
      "\u001b[1m834/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9698 - loss: 0.1009\n",
      "Epoch 6: val_accuracy improved from 0.96767 to 0.97100, saving model to MLP_Models\\Deep_MLP_Model-lr_0.01-bs_64-20250917-091243\\best_model_epoch-06_val_acc-0.9710.keras\n",
      "\n",
      "Epoch 6: val_accuracy improved from 0.96767 to 0.97100, saving model to wandb_models/Deep_MLP_Model-lr_0.01-bs_64-20250917-091243/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - accuracy: 0.9707 - loss: 0.0985 - val_accuracy: 0.9710 - val_loss: 0.1050\n",
      "Epoch 7/100\n",
      "\u001b[1m830/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9746 - loss: 0.0859\n",
      "Epoch 7: val_accuracy improved from 0.97100 to 0.97367, saving model to MLP_Models\\Deep_MLP_Model-lr_0.01-bs_64-20250917-091243\\best_model_epoch-07_val_acc-0.9737.keras\n",
      "\n",
      "Epoch 7: val_accuracy improved from 0.97100 to 0.97367, saving model to wandb_models/Deep_MLP_Model-lr_0.01-bs_64-20250917-091243/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9753 - loss: 0.0843 - val_accuracy: 0.9737 - val_loss: 0.0985\n",
      "Epoch 8/100\n",
      "\u001b[1m836/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9785 - loss: 0.0740\n",
      "Epoch 8: val_accuracy improved from 0.97367 to 0.97450, saving model to MLP_Models\\Deep_MLP_Model-lr_0.01-bs_64-20250917-091243\\best_model_epoch-08_val_acc-0.9745.keras\n",
      "\n",
      "Epoch 8: val_accuracy improved from 0.97367 to 0.97450, saving model to wandb_models/Deep_MLP_Model-lr_0.01-bs_64-20250917-091243/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.9792 - loss: 0.0727 - val_accuracy: 0.9745 - val_loss: 0.0929\n",
      "Epoch 9/100\n",
      "\u001b[1m832/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9818 - loss: 0.0643\n",
      "Epoch 9: val_accuracy improved from 0.97450 to 0.97567, saving model to MLP_Models\\Deep_MLP_Model-lr_0.01-bs_64-20250917-091243\\best_model_epoch-09_val_acc-0.9757.keras\n",
      "\n",
      "Epoch 9: val_accuracy improved from 0.97450 to 0.97567, saving model to wandb_models/Deep_MLP_Model-lr_0.01-bs_64-20250917-091243/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.9821 - loss: 0.0631 - val_accuracy: 0.9757 - val_loss: 0.0902\n",
      "Epoch 10/100\n",
      "\u001b[1m837/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9843 - loss: 0.0559\n",
      "Epoch 10: val_accuracy improved from 0.97567 to 0.97650, saving model to MLP_Models\\Deep_MLP_Model-lr_0.01-bs_64-20250917-091243\\best_model_epoch-10_val_acc-0.9765.keras\n",
      "\n",
      "Epoch 10: val_accuracy improved from 0.97567 to 0.97650, saving model to wandb_models/Deep_MLP_Model-lr_0.01-bs_64-20250917-091243/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9848 - loss: 0.0548 - val_accuracy: 0.9765 - val_loss: 0.0883\n",
      "Epoch 11/100\n",
      "\u001b[1m834/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9863 - loss: 0.0486\n",
      "Epoch 11: val_accuracy did not improve from 0.97650\n",
      "\n",
      "Epoch 11: val_accuracy did not improve from 0.97650\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9871 - loss: 0.0475 - val_accuracy: 0.9765 - val_loss: 0.0872\n",
      "Epoch 12/100\n",
      "\u001b[1m840/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9888 - loss: 0.0421\n",
      "Epoch 12: val_accuracy improved from 0.97650 to 0.97667, saving model to MLP_Models\\Deep_MLP_Model-lr_0.01-bs_64-20250917-091243\\best_model_epoch-12_val_acc-0.9767.keras\n",
      "\n",
      "Epoch 12: val_accuracy improved from 0.97650 to 0.97667, saving model to wandb_models/Deep_MLP_Model-lr_0.01-bs_64-20250917-091243/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9892 - loss: 0.0411 - val_accuracy: 0.9767 - val_loss: 0.0875\n",
      "Epoch 13/100\n",
      "\u001b[1m833/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9907 - loss: 0.0361\n",
      "Epoch 13: val_accuracy did not improve from 0.97667\n",
      "\n",
      "Epoch 13: val_accuracy did not improve from 0.97667\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9911 - loss: 0.0354 - val_accuracy: 0.9763 - val_loss: 0.0893\n",
      "Epoch 14/100\n",
      "\u001b[1m838/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9925 - loss: 0.0308\n",
      "Epoch 14: val_accuracy improved from 0.97667 to 0.97683, saving model to MLP_Models\\Deep_MLP_Model-lr_0.01-bs_64-20250917-091243\\best_model_epoch-14_val_acc-0.9768.keras\n",
      "\n",
      "Epoch 14: val_accuracy improved from 0.97667 to 0.97683, saving model to wandb_models/Deep_MLP_Model-lr_0.01-bs_64-20250917-091243/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9929 - loss: 0.0302 - val_accuracy: 0.9768 - val_loss: 0.0911\n",
      "Epoch 15/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9940 - loss: 0.0261\n",
      "Epoch 15: val_accuracy improved from 0.97683 to 0.97717, saving model to MLP_Models\\Deep_MLP_Model-lr_0.01-bs_64-20250917-091243\\best_model_epoch-15_val_acc-0.9772.keras\n",
      "\n",
      "Epoch 15: val_accuracy improved from 0.97683 to 0.97717, saving model to wandb_models/Deep_MLP_Model-lr_0.01-bs_64-20250917-091243/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.9942 - loss: 0.0258 - val_accuracy: 0.9772 - val_loss: 0.0937\n",
      "Epoch 16/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9953 - loss: 0.0220\n",
      "Epoch 16: val_accuracy did not improve from 0.97717\n",
      "\n",
      "Epoch 16: val_accuracy did not improve from 0.97717\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.9956 - loss: 0.0219 - val_accuracy: 0.9770 - val_loss: 0.0973\n",
      "Epoch 17/100\n",
      "\u001b[1m835/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9965 - loss: 0.0184\n",
      "Epoch 17: val_accuracy did not improve from 0.97717\n",
      "\n",
      "Epoch 17: val_accuracy did not improve from 0.97717\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9966 - loss: 0.0184 - val_accuracy: 0.9758 - val_loss: 0.1022\n",
      "Epoch 18/100\n",
      "\u001b[1m839/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9972 - loss: 0.0155\n",
      "Epoch 18: val_accuracy did not improve from 0.97717\n",
      "\n",
      "Epoch 18: val_accuracy did not improve from 0.97717\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9974 - loss: 0.0155 - val_accuracy: 0.9768 - val_loss: 0.1046\n",
      "Epoch 19/100\n",
      "\u001b[1m837/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9977 - loss: 0.0130\n",
      "Epoch 19: val_accuracy did not improve from 0.97717\n",
      "\n",
      "Epoch 19: val_accuracy did not improve from 0.97717\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9978 - loss: 0.0132 - val_accuracy: 0.9762 - val_loss: 0.1039\n",
      "Epoch 20/100\n",
      "\u001b[1m834/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9982 - loss: 0.0108\n",
      "Epoch 20: val_accuracy did not improve from 0.97717\n",
      "\n",
      "Epoch 20: val_accuracy did not improve from 0.97717\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.9983 - loss: 0.0111 - val_accuracy: 0.9768 - val_loss: 0.1038\n",
      "Epoch 21/100\n",
      "\u001b[1m838/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9988 - loss: 0.0089\n",
      "Epoch 21: val_accuracy improved from 0.97717 to 0.97783, saving model to MLP_Models\\Deep_MLP_Model-lr_0.01-bs_64-20250917-091243\\best_model_epoch-21_val_acc-0.9778.keras\n",
      "\n",
      "Epoch 21: val_accuracy improved from 0.97717 to 0.97783, saving model to wandb_models/Deep_MLP_Model-lr_0.01-bs_64-20250917-091243/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9987 - loss: 0.0093 - val_accuracy: 0.9778 - val_loss: 0.1051\n",
      "Epoch 22/100\n",
      "\u001b[1m840/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9990 - loss: 0.0074\n",
      "Epoch 22: val_accuracy improved from 0.97783 to 0.97850, saving model to MLP_Models\\Deep_MLP_Model-lr_0.01-bs_64-20250917-091243\\best_model_epoch-22_val_acc-0.9785.keras\n",
      "\n",
      "Epoch 22: val_accuracy improved from 0.97783 to 0.97850, saving model to wandb_models/Deep_MLP_Model-lr_0.01-bs_64-20250917-091243/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9990 - loss: 0.0078 - val_accuracy: 0.9785 - val_loss: 0.1060\n",
      "Epoch 23/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9995 - loss: 0.0061\n",
      "Epoch 23: val_accuracy improved from 0.97850 to 0.97917, saving model to MLP_Models\\Deep_MLP_Model-lr_0.01-bs_64-20250917-091243\\best_model_epoch-23_val_acc-0.9792.keras\n",
      "\n",
      "Epoch 23: val_accuracy improved from 0.97850 to 0.97917, saving model to wandb_models/Deep_MLP_Model-lr_0.01-bs_64-20250917-091243/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.9993 - loss: 0.0066 - val_accuracy: 0.9792 - val_loss: 0.1074\n",
      "Epoch 24/100\n",
      "\u001b[1m840/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9996 - loss: 0.0051\n",
      "Epoch 24: val_accuracy improved from 0.97917 to 0.97950, saving model to MLP_Models\\Deep_MLP_Model-lr_0.01-bs_64-20250917-091243\\best_model_epoch-24_val_acc-0.9795.keras\n",
      "\n",
      "Epoch 24: val_accuracy improved from 0.97917 to 0.97950, saving model to wandb_models/Deep_MLP_Model-lr_0.01-bs_64-20250917-091243/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.9995 - loss: 0.0056 - val_accuracy: 0.9795 - val_loss: 0.1081\n",
      "Epoch 25/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9996 - loss: 0.0043\n",
      "Epoch 25: val_accuracy did not improve from 0.97950\n",
      "\n",
      "Epoch 25: val_accuracy did not improve from 0.97950\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.9995 - loss: 0.0047 - val_accuracy: 0.9795 - val_loss: 0.1094\n",
      "Epoch 26/100\n",
      "\u001b[1m838/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9997 - loss: 0.0036\n",
      "Epoch 26: val_accuracy did not improve from 0.97950\n",
      "\n",
      "Epoch 26: val_accuracy did not improve from 0.97950\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9996 - loss: 0.0040 - val_accuracy: 0.9792 - val_loss: 0.1106\n",
      "Epoch 27/100\n",
      "\u001b[1m837/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9998 - loss: 0.0031\n",
      "Epoch 27: val_accuracy improved from 0.97950 to 0.98033, saving model to MLP_Models\\Deep_MLP_Model-lr_0.01-bs_64-20250917-091243\\best_model_epoch-27_val_acc-0.9803.keras\n",
      "\n",
      "Epoch 27: val_accuracy improved from 0.97950 to 0.98033, saving model to wandb_models/Deep_MLP_Model-lr_0.01-bs_64-20250917-091243/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9997 - loss: 0.0034 - val_accuracy: 0.9803 - val_loss: 0.1112\n",
      "Epoch 28/100\n",
      "\u001b[1m833/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9998 - loss: 0.0028\n",
      "Epoch 28: val_accuracy improved from 0.98033 to 0.98050, saving model to MLP_Models\\Deep_MLP_Model-lr_0.01-bs_64-20250917-091243\\best_model_epoch-28_val_acc-0.9805.keras\n",
      "\n",
      "Epoch 28: val_accuracy improved from 0.98033 to 0.98050, saving model to wandb_models/Deep_MLP_Model-lr_0.01-bs_64-20250917-091243/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.9998 - loss: 0.0031 - val_accuracy: 0.9805 - val_loss: 0.1123\n",
      "Epoch 29/100\n",
      "\u001b[1m833/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9999 - loss: 0.0024\n",
      "Epoch 29: val_accuracy did not improve from 0.98050\n",
      "\n",
      "Epoch 29: val_accuracy did not improve from 0.98050\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.9998 - loss: 0.0026 - val_accuracy: 0.9803 - val_loss: 0.1129\n",
      "Epoch 30/100\n",
      "\u001b[1m837/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9999 - loss: 0.0020\n",
      "Epoch 30: val_accuracy did not improve from 0.98050\n",
      "\n",
      "Epoch 30: val_accuracy did not improve from 0.98050\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9998 - loss: 0.0022 - val_accuracy: 0.9803 - val_loss: 0.1140\n",
      "Epoch 31/100\n",
      "\u001b[1m838/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9999 - loss: 0.0018\n",
      "Epoch 31: val_accuracy did not improve from 0.98050\n",
      "\n",
      "Epoch 31: val_accuracy did not improve from 0.98050\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9999 - loss: 0.0020 - val_accuracy: 0.9803 - val_loss: 0.1152\n",
      "Epoch 32/100\n",
      "\u001b[1m839/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9999 - loss: 0.0016\n",
      "Epoch 32: val_accuracy did not improve from 0.98050\n",
      "\n",
      "Epoch 32: val_accuracy did not improve from 0.98050\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9999 - loss: 0.0017 - val_accuracy: 0.9802 - val_loss: 0.1167\n",
      "Epoch 33/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.0014\n",
      "Epoch 33: val_accuracy did not improve from 0.98050\n",
      "\n",
      "Epoch 33: val_accuracy did not improve from 0.98050\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9999 - loss: 0.0015 - val_accuracy: 0.9802 - val_loss: 0.1180\n",
      "Epoch 34/100\n",
      "\u001b[1m840/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.0013\n",
      "Epoch 34: val_accuracy did not improve from 0.98050\n",
      "\n",
      "Epoch 34: val_accuracy did not improve from 0.98050\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0014 - val_accuracy: 0.9802 - val_loss: 0.1193\n",
      "Epoch 35/100\n",
      "\u001b[1m835/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.0011\n",
      "Epoch 35: val_accuracy did not improve from 0.98050\n",
      "\n",
      "Epoch 35: val_accuracy did not improve from 0.98050\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0012 - val_accuracy: 0.9800 - val_loss: 0.1206\n",
      "Epoch 36/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.0010\n",
      "Epoch 36: val_accuracy did not improve from 0.98050\n",
      "\n",
      "Epoch 36: val_accuracy did not improve from 0.98050\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0011 - val_accuracy: 0.9800 - val_loss: 0.1220\n",
      "Epoch 37/100\n",
      "\u001b[1m840/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 9.2965e-04\n",
      "Epoch 37: val_accuracy did not improve from 0.98050\n",
      "\n",
      "Epoch 37: val_accuracy did not improve from 0.98050\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 9.9069e-04 - val_accuracy: 0.9803 - val_loss: 0.1231\n",
      "Epoch 38/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 8.4531e-04\n",
      "Epoch 38: val_accuracy did not improve from 0.98050\n",
      "\n",
      "Epoch 38: val_accuracy did not improve from 0.98050\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 8.9430e-04 - val_accuracy: 0.9803 - val_loss: 0.1242\n",
      "\n",
      "Training history saved to: MLP_Models\\Deep_MLP_Model-lr_0.01-bs_64-20250917-091243\\training_history.pkl\n",
      "\n",
      "--- Peak Performance Summary ---\n",
      "Best validation accuracy:           0.9805\n",
      "Associated training accuracy:       0.9998\n",
      "Occurred at epoch:                  28\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch/accuracy</td><td>▁▆▇▇▇▇▇▇██████████████████████████████</td></tr><tr><td>epoch/epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>epoch/learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/loss</td><td>█▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/val_accuracy</td><td>▁▄▅▆▆▇▇▇▇███████▇█▇███████████████████</td></tr><tr><td>epoch/val_loss</td><td>█▅▃▃▂▂▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch/accuracy</td><td>0.99998</td></tr><tr><td>epoch/epoch</td><td>37</td></tr><tr><td>epoch/learning_rate</td><td>0.01</td></tr><tr><td>epoch/loss</td><td>0.00089</td></tr><tr><td>epoch/val_accuracy</td><td>0.98033</td></tr><tr><td>epoch/val_loss</td><td>0.12418</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Deep_MLP_Model-lr_0.01-bs_64-20250917-091243</strong> at: <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/vzn75ovw' target=\"_blank\">https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/vzn75ovw</a><br> View project at: <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2' target=\"_blank\">https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2</a><br>Synced 5 W&B file(s), 0 media file(s), 38 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250917_091243-vzn75ovw\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_mlp_experiments: bool = True # A simple flag to control whether we run the MLP experiments or not. \n",
    "\n",
    "all_exp_params: bool = True # A flag to control whether we use the entire set of hyperparameters to train with, or just a single set. \n",
    "\n",
    "# --- Run the full list of experiments ---\n",
    "if run_mlp_experiments:\n",
    "    mlp_histories: List[History] = []\n",
    "    for mlp_model_function in mlp_model_functions:\n",
    "        if not all_exp_params:\n",
    "            mlp_history: History = run_experiment(\n",
    "                model_creation_func=mlp_model_function, \n",
    "                hyperparameters=mlp_exp_1_config,  # If we do not set the all experiment parameters to True, then we only need the 1 config file. \n",
    "                parent_folder='MLP_Models',\n",
    "                X_train=X_train,\n",
    "                Y_train=Y_train,\n",
    "            )\n",
    "        else:\n",
    "            for config in mlp_config:\n",
    "                mlp_history: History = run_experiment(\n",
    "                    model_creation_func=mlp_model_function, \n",
    "                    hyperparameters=config, \n",
    "                    parent_folder='MLP_Models',\n",
    "                    X_train=X_train,\n",
    "                    Y_train=Y_train,\n",
    "                )\n",
    "\n",
    "        mlp_histories.append(mlp_history)\n",
    "\n",
    "# # --- To run the second experiment, we just call it again with a different config (hyper parameter set) :-)\n",
    "# --- We first  test different model architectures before running more experiments with different hyperparameters (Part 1 Task 1 & 2). ---\n",
    "# --- THen we test the best performing model of Part 1 Task 1& 2 with different hyperparameters. ---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba7b25e",
   "metadata": {},
   "source": [
    "## Part 1, Task 2: Creating a simple Convolutional Neural Network (CNN)\n",
    "\n",
    "The code below defines our base model.\n",
    "\n",
    "To experiment with different architectures or tune its hyperparameters, we simply copy this entire cell and make our changes.\n",
    "\n",
    "We need to make sure to give each new model a unique name. This ensures that when the ModelCheckpoint callback saves the best-performing version during training, the filename will be clear and identifiable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d24564b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Task 2**\n",
    "\n",
    "# Build a neural network with the use of convolutional layers (you can decide other layer types you want to include in your network). \n",
    "# Then you can change the number of convolutional layers and the number of filters or activation functions in the convolutional layers to be able to improve network performance.\n",
    "\n",
    "def create_cnn_model_base() -> Sequential:\n",
    "    \"\"\"\n",
    "    Defines the base CNN model architecture with Dropout for regularization.\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        # Preprocessing layers\n",
    "        Input(shape=(28, 28, 1)),\n",
    "        Normalization(),\n",
    "        \n",
    "        # --- Convolutional Block 1 ---\n",
    "        Conv2D(filters=32, kernel_size=(3, 3), activation='relu'),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "        # --- Convolutional Block 2 ---\n",
    "        Conv2D(filters=64, kernel_size=(3, 3), activation='relu'),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        \n",
    "        # --- Classification Head ---\n",
    "        Flatten(),\n",
    "        # Dropout(0.5),\n",
    "        Dense(units=128, activation='relu'),\n",
    "        Dense(units=10, activation='softmax')\n",
    "    ], name=\"Base_CNN\")\n",
    "    return model    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe9d128",
   "metadata": {},
   "source": [
    "## Creating variations to the CNN base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f5954ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a second CNN base model with the same architecture, but a dropout in the classification head.\n",
    "def create_cnn_model_base_dropout() -> Sequential:\n",
    "    \"\"\"\n",
    "    Defines and returns the base CNN model architecture with dropout in the classification head.\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        # Preprocessing layers\n",
    "        Input(shape=(28, 28, 1)),\n",
    "        Normalization(),\n",
    "            \n",
    "        # --- Convolutional Block 1 ---\n",
    "        Conv2D(filters=32, kernel_size=(3, 3), activation='relu'),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "        # --- Convolutional Block 2 ---\n",
    "        Conv2D(filters=64, kernel_size=(3, 3), activation='relu'),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        \n",
    "        # --- Classification Head ---\n",
    "        Flatten(),\n",
    "        Dropout(0.5),\n",
    "        Dense(units=128, activation='relu'),\n",
    "        Dense(units=10, activation='softmax')\n",
    "    ], name=\"CNN_with_Dropout\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1879ccb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a wide CNN \n",
    "# The width is defined by the nunber of units in its layers.\n",
    "# to widen it, we can increase the filter argument in the Conv2D layers\n",
    "# widen the Dense layer units, e.g. from 128 to 256.\n",
    "def create_cnn_model_wide() -> Sequential:\n",
    "    \"\"\"\n",
    "    Defines and returns a wide CNN model architecture.\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        # Preprocessing layers\n",
    "        Input(shape=(28, 28, 1)),\n",
    "        Normalization(),\n",
    "        \n",
    "        # --- Convolutional Block 1 ---\n",
    "        Conv2D(filters=64, kernel_size=(3, 3), activation='relu'),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "        # --- Convolutional Block 2 ---\n",
    "        Conv2D(filters=128, kernel_size=(3, 3), activation='relu'),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        \n",
    "        # --- Classification Head ---\n",
    "        Flatten(),\n",
    "        Dropout(0.5),\n",
    "        Dense(units=256, activation='relu'),\n",
    "        Dense(units=10, activation='softmax')\n",
    "    ], name=\"Wide_CNN_Model\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ec63191a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn_model_deep() -> Sequential:\n",
    "    \"\"\"\n",
    "    Defines and returns a deep CNN model architecture with padding to preserve dimensions.\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        Input(shape=(28, 28, 1)),\n",
    "        Normalization(),\n",
    "        \n",
    "        # --- Convolutional Block 1 ---\n",
    "        # Add padding='same' to all Conv2D layers, so we preserve the spatial dimensions. If we don't do this, the image shrinks too quickly.\n",
    "        # and no meaningful features can be extracted.\n",
    "        Conv2D(filters=32, kernel_size=(3, 3), activation='relu', padding='same'),\n",
    "        Conv2D(filters=32, kernel_size=(3, 3), activation='relu', padding='same'),\n",
    "        Conv2D(filters=32, kernel_size=(3, 3), activation='relu', padding='same'),\n",
    "        MaxPooling2D(pool_size=(2, 2)), # This is where the shrinking now happens (28x28 -> 14x14)\n",
    "\n",
    "        # --- Convolutional Block 2 ---\n",
    "        Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same'),\n",
    "        Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same'),\n",
    "        Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same'),\n",
    "        MaxPooling2D(pool_size=(2, 2)), # Second shrink (14x14 -> 7x7)\n",
    "        \n",
    "        # --- Classification Head ---\n",
    "        # The input to Flatten is now a healthy 7x7x64=3,136 full of rich features.\n",
    "        Flatten(),\n",
    "        Dropout(0.5),\n",
    "        Dense(units=128, activation='relu'),\n",
    "        Dense(units=10, activation='softmax')\n",
    "    ], name=\"Deep_CNN_Model_Padded\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e745d2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_high_performance_cnn() -> Sequential:\n",
    "    \"\"\"\n",
    "    Defines a deep, VGG-style CNN architecture with Batch Normalization and Dropout,\n",
    "    designed for high performance on the MNIST dataset.\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        Input(shape=(28, 28, 1)),\n",
    "        Normalization(),\n",
    "\n",
    "        # --- Block 1 ---\n",
    "        Conv2D(filters=32, kernel_size=(3, 3), activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        Conv2D(filters=32, kernel_size=(3, 3), activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Dropout(0.25),\n",
    "\n",
    "        # --- Block 2 ---\n",
    "        Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Dropout(0.25),\n",
    "        \n",
    "        # --- Block 3 ---\n",
    "        Conv2D(filters=128, kernel_size=(3, 3), activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Dropout(0.25),\n",
    "\n",
    "        # --- Classifier Head ---\n",
    "        Flatten(),\n",
    "        Dense(units=256, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        Dense(units=10, activation='softmax')\n",
    "        \n",
    "    ], name=\"High_Performance_CNN\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "12d18c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model_functions: List[Callable] = [\n",
    "    create_cnn_model_base,\n",
    "    create_cnn_model_base_dropout,\n",
    "    create_cnn_model_wide,\n",
    "    create_cnn_model_deep, # We teain the deep network separately with a much lower LR. We have observed that this model performs no better than a random guess (i.e. 10% val. accuracy)\n",
    "    # Almost certainly this has to do with the LR, the model can't get to the valleys of the loss landscape, as it overshoots. \n",
    "    # Future to do: Find a way to reduce the loss landscape to 3D, or 2D dimension, and show the parth the Optimiser takes, to be able to demonstrate the effect of the LR. \n",
    "]\n",
    "\n",
    "\n",
    "# cnn_model_functions = [\n",
    "#     create_cnn_model_deep,\n",
    "# ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42570c15",
   "metadata": {},
   "source": [
    "## Defining hyperparameters for the CNN models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "94729803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define Hyperparameter Set for the CNN models ---\n",
    "cnn_epochs: int = 100\n",
    "cnn_batch_size: int = 64\n",
    "\n",
    "cnn_exp_1_config: Dict[str, Union[str, float, int]] = {\n",
    "    \"optimiser\": \"adamw\",\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"epochs\": cnn_epochs,\n",
    "    \"batch_size\": cnn_batch_size\n",
    "}\n",
    "cnn_exp_2_config: Dict[str, Union[str, float, int]] = {\n",
    "    \"optimiser\": \"adamw\",\n",
    "    \"learning_rate\": 0.0001,\n",
    "    \"epochs\": cnn_epochs,\n",
    "    \"batch_size\": cnn_batch_size\n",
    "}\n",
    "\n",
    "cnn_exp_3_config: Dict[str, Union[str, float, int]] = {\n",
    "    \"optimiser\": \"SGD\",\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"epochs\": cnn_epochs,\n",
    "    \"batch_size\": cnn_batch_size\n",
    "}\n",
    "\n",
    "cnn_exp_4_config: Dict[str, Union[str, float, int]] = {\n",
    "    \"optimiser\": \"SGD\",\n",
    "    \"learning_rate\": 0.0001,\n",
    "    \"epochs\": cnn_epochs,\n",
    "    \"batch_size\": cnn_batch_size\n",
    "}\n",
    "\n",
    "cnn_config: List[Dict[str, Union[str, float, int]]] = [cnn_exp_1_config, cnn_exp_2_config, cnn_exp_3_config, cnn_exp_4_config] # ,cnn_exp_3_config, cnn_exp_4_config\n",
    "# cnn_exp_1_config,  We do not need to include the cnn_exp_1_config, because we have already run this on all models. \n",
    "# In the cell below, setting the all_exp_params is a flag that ensure that when set to True, all models are trained on all parameters (computationally intensive)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5606f5",
   "metadata": {},
   "source": [
    "## Training the CNN models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1d5b96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Adapting the normalisation layer...\n",
      "Adaptation complete.\n",
      "\n",
      "\n",
      "--- Starting Experiment: Base_CNN ---\n",
      "\n",
      "--- Model Architecture ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"Base_CNN\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"Base_CNN\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ normalization_20                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Normalization</span>)                 │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1600</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_96 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">204,928</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_97 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,290</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ normalization_20                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m1\u001b[0m)      │             \u001b[38;5;34m3\u001b[0m │\n",
       "│ (\u001b[38;5;33mNormalization\u001b[0m)                 │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m320\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_20 (\u001b[38;5;33mFlatten\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1600\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_96 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m204,928\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_97 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │         \u001b[38;5;34m1,290\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">225,037</span> (879.05 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m225,037\u001b[0m (879.05 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">225,034</span> (879.04 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m225,034\u001b[0m (879.04 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3</span> (16.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m3\u001b[0m (16.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Hyperparameters ---\n",
      "optimiser           : adamw\n",
      "learning_rate       : 0.001\n",
      "epochs              : 100\n",
      "batch_size          : 64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "creating run (0.0s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\TimVos\\VSC Projects\\CSE5ML\\Assessment 2\\wandb\\run-20250917_091616-7uzr6og0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/7uzr6og0' target=\"_blank\">Base_CNN-lr_0.001-bs_64-20250917-091616</a></strong> to <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2' target=\"_blank\">https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/7uzr6og0' target=\"_blank\">https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/7uzr6og0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m840/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8943 - loss: 0.3327\n",
      "Epoch 1: val_accuracy improved from None to 0.98767, saving model to CNN_Models\\Base_CNN-lr_0.001-bs_64-20250917-091616\\best_model_epoch-01_val_acc-0.9877.keras\n",
      "\n",
      "Epoch 1: val_accuracy improved from None to 0.98767, saving model to wandb_models/Base_CNN-lr_0.001-bs_64-20250917-091616/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 15ms/step - accuracy: 0.9559 - loss: 0.1424 - val_accuracy: 0.9877 - val_loss: 0.0474\n",
      "Epoch 2/100\n",
      "\u001b[1m840/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9849 - loss: 0.0473\n",
      "Epoch 2: val_accuracy improved from 0.98767 to 0.98883, saving model to CNN_Models\\Base_CNN-lr_0.001-bs_64-20250917-091616\\best_model_epoch-02_val_acc-0.9888.keras\n",
      "\n",
      "Epoch 2: val_accuracy improved from 0.98767 to 0.98883, saving model to wandb_models/Base_CNN-lr_0.001-bs_64-20250917-091616/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 14ms/step - accuracy: 0.9861 - loss: 0.0440 - val_accuracy: 0.9888 - val_loss: 0.0421\n",
      "Epoch 3/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9909 - loss: 0.0300\n",
      "Epoch 3: val_accuracy improved from 0.98883 to 0.99000, saving model to CNN_Models\\Base_CNN-lr_0.001-bs_64-20250917-091616\\best_model_epoch-03_val_acc-0.9900.keras\n",
      "\n",
      "Epoch 3: val_accuracy improved from 0.98883 to 0.99000, saving model to wandb_models/Base_CNN-lr_0.001-bs_64-20250917-091616/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - accuracy: 0.9912 - loss: 0.0284 - val_accuracy: 0.9900 - val_loss: 0.0401\n",
      "Epoch 4/100\n",
      "\u001b[1m839/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9933 - loss: 0.0213\n",
      "Epoch 4: val_accuracy did not improve from 0.99000\n",
      "\n",
      "Epoch 4: val_accuracy did not improve from 0.99000\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - accuracy: 0.9936 - loss: 0.0202 - val_accuracy: 0.9895 - val_loss: 0.0395\n",
      "Epoch 5/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9949 - loss: 0.0151\n",
      "Epoch 5: val_accuracy did not improve from 0.99000\n",
      "\n",
      "Epoch 5: val_accuracy did not improve from 0.99000\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - accuracy: 0.9953 - loss: 0.0146 - val_accuracy: 0.9892 - val_loss: 0.0454\n",
      "Epoch 6/100\n",
      "\u001b[1m840/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9949 - loss: 0.0152\n",
      "Epoch 6: val_accuracy improved from 0.99000 to 0.99117, saving model to CNN_Models\\Base_CNN-lr_0.001-bs_64-20250917-091616\\best_model_epoch-06_val_acc-0.9912.keras\n",
      "\n",
      "Epoch 6: val_accuracy improved from 0.99000 to 0.99117, saving model to wandb_models/Base_CNN-lr_0.001-bs_64-20250917-091616/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 14ms/step - accuracy: 0.9950 - loss: 0.0141 - val_accuracy: 0.9912 - val_loss: 0.0381\n",
      "Epoch 7/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9965 - loss: 0.0101\n",
      "Epoch 7: val_accuracy did not improve from 0.99117\n",
      "\n",
      "Epoch 7: val_accuracy did not improve from 0.99117\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 14ms/step - accuracy: 0.9966 - loss: 0.0102 - val_accuracy: 0.9908 - val_loss: 0.0446\n",
      "Epoch 8/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9972 - loss: 0.0091\n",
      "Epoch 8: val_accuracy did not improve from 0.99117\n",
      "\n",
      "Epoch 8: val_accuracy did not improve from 0.99117\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 14ms/step - accuracy: 0.9969 - loss: 0.0094 - val_accuracy: 0.9903 - val_loss: 0.0514\n",
      "Epoch 9/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9970 - loss: 0.0077\n",
      "Epoch 9: val_accuracy did not improve from 0.99117\n",
      "\n",
      "Epoch 9: val_accuracy did not improve from 0.99117\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 14ms/step - accuracy: 0.9972 - loss: 0.0074 - val_accuracy: 0.9908 - val_loss: 0.0516\n",
      "Epoch 10/100\n",
      "\u001b[1m840/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9978 - loss: 0.0062\n",
      "Epoch 10: val_accuracy improved from 0.99117 to 0.99150, saving model to CNN_Models\\Base_CNN-lr_0.001-bs_64-20250917-091616\\best_model_epoch-10_val_acc-0.9915.keras\n",
      "\n",
      "Epoch 10: val_accuracy improved from 0.99117 to 0.99150, saving model to wandb_models/Base_CNN-lr_0.001-bs_64-20250917-091616/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.9977 - loss: 0.0063 - val_accuracy: 0.9915 - val_loss: 0.0429\n",
      "Epoch 11/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9979 - loss: 0.0057\n",
      "Epoch 11: val_accuracy improved from 0.99150 to 0.99200, saving model to CNN_Models\\Base_CNN-lr_0.001-bs_64-20250917-091616\\best_model_epoch-11_val_acc-0.9920.keras\n",
      "\n",
      "Epoch 11: val_accuracy improved from 0.99150 to 0.99200, saving model to wandb_models/Base_CNN-lr_0.001-bs_64-20250917-091616/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 14ms/step - accuracy: 0.9974 - loss: 0.0072 - val_accuracy: 0.9920 - val_loss: 0.0448\n",
      "Epoch 12/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9979 - loss: 0.0066\n",
      "Epoch 12: val_accuracy improved from 0.99200 to 0.99250, saving model to CNN_Models\\Base_CNN-lr_0.001-bs_64-20250917-091616\\best_model_epoch-12_val_acc-0.9925.keras\n",
      "\n",
      "Epoch 12: val_accuracy improved from 0.99200 to 0.99250, saving model to wandb_models/Base_CNN-lr_0.001-bs_64-20250917-091616/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 15ms/step - accuracy: 0.9984 - loss: 0.0050 - val_accuracy: 0.9925 - val_loss: 0.0488\n",
      "Epoch 13/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9984 - loss: 0.0052\n",
      "Epoch 13: val_accuracy did not improve from 0.99250\n",
      "\n",
      "Epoch 13: val_accuracy did not improve from 0.99250\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - accuracy: 0.9983 - loss: 0.0057 - val_accuracy: 0.9918 - val_loss: 0.0583\n",
      "Epoch 14/100\n",
      "\u001b[1m840/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9979 - loss: 0.0052\n",
      "Epoch 14: val_accuracy did not improve from 0.99250\n",
      "\n",
      "Epoch 14: val_accuracy did not improve from 0.99250\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 14ms/step - accuracy: 0.9981 - loss: 0.0051 - val_accuracy: 0.9915 - val_loss: 0.0522\n",
      "Epoch 15/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9982 - loss: 0.0058\n",
      "Epoch 15: val_accuracy did not improve from 0.99250\n",
      "\n",
      "Epoch 15: val_accuracy did not improve from 0.99250\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - accuracy: 0.9982 - loss: 0.0057 - val_accuracy: 0.9917 - val_loss: 0.0477\n",
      "Epoch 16/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9981 - loss: 0.0046\n",
      "Epoch 16: val_accuracy did not improve from 0.99250\n",
      "\n",
      "Epoch 16: val_accuracy did not improve from 0.99250\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 14ms/step - accuracy: 0.9985 - loss: 0.0042 - val_accuracy: 0.9905 - val_loss: 0.0629\n",
      "Epoch 17/100\n",
      "\u001b[1m840/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9983 - loss: 0.0064\n",
      "Epoch 17: val_accuracy did not improve from 0.99250\n",
      "\n",
      "Epoch 17: val_accuracy did not improve from 0.99250\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 14ms/step - accuracy: 0.9989 - loss: 0.0039 - val_accuracy: 0.9908 - val_loss: 0.0500\n",
      "Epoch 18/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9993 - loss: 0.0024\n",
      "Epoch 18: val_accuracy improved from 0.99250 to 0.99267, saving model to CNN_Models\\Base_CNN-lr_0.001-bs_64-20250917-091616\\best_model_epoch-18_val_acc-0.9927.keras\n",
      "\n",
      "Epoch 18: val_accuracy improved from 0.99250 to 0.99267, saving model to wandb_models/Base_CNN-lr_0.001-bs_64-20250917-091616/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 16ms/step - accuracy: 0.9990 - loss: 0.0032 - val_accuracy: 0.9927 - val_loss: 0.0519\n",
      "Epoch 19/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9986 - loss: 0.0043\n",
      "Epoch 19: val_accuracy did not improve from 0.99267\n",
      "\n",
      "Epoch 19: val_accuracy did not improve from 0.99267\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 14ms/step - accuracy: 0.9985 - loss: 0.0045 - val_accuracy: 0.9927 - val_loss: 0.0533\n",
      "Epoch 20/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9993 - loss: 0.0023\n",
      "Epoch 20: val_accuracy did not improve from 0.99267\n",
      "\n",
      "Epoch 20: val_accuracy did not improve from 0.99267\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 14ms/step - accuracy: 0.9986 - loss: 0.0043 - val_accuracy: 0.9915 - val_loss: 0.0652\n",
      "Epoch 21/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9988 - loss: 0.0039\n",
      "Epoch 21: val_accuracy did not improve from 0.99267\n",
      "\n",
      "Epoch 21: val_accuracy did not improve from 0.99267\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 14ms/step - accuracy: 0.9987 - loss: 0.0039 - val_accuracy: 0.9907 - val_loss: 0.0626\n",
      "Epoch 22/100\n",
      "\u001b[1m840/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9995 - loss: 0.0015\n",
      "Epoch 22: val_accuracy did not improve from 0.99267\n",
      "\n",
      "Epoch 22: val_accuracy did not improve from 0.99267\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.9992 - loss: 0.0024 - val_accuracy: 0.9927 - val_loss: 0.0530\n",
      "Epoch 23/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9992 - loss: 0.0025\n",
      "Epoch 23: val_accuracy did not improve from 0.99267\n",
      "\n",
      "Epoch 23: val_accuracy did not improve from 0.99267\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.9992 - loss: 0.0025 - val_accuracy: 0.9918 - val_loss: 0.0646\n",
      "Epoch 24/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9989 - loss: 0.0040\n",
      "Epoch 24: val_accuracy did not improve from 0.99267\n",
      "\n",
      "Epoch 24: val_accuracy did not improve from 0.99267\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 15ms/step - accuracy: 0.9988 - loss: 0.0039 - val_accuracy: 0.9922 - val_loss: 0.0539\n",
      "Epoch 25/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9989 - loss: 0.0028\n",
      "Epoch 25: val_accuracy did not improve from 0.99267\n",
      "\n",
      "Epoch 25: val_accuracy did not improve from 0.99267\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 14ms/step - accuracy: 0.9990 - loss: 0.0027 - val_accuracy: 0.9927 - val_loss: 0.0646\n",
      "Epoch 26/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9993 - loss: 0.0023\n",
      "Epoch 26: val_accuracy did not improve from 0.99267\n",
      "\n",
      "Epoch 26: val_accuracy did not improve from 0.99267\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 15ms/step - accuracy: 0.9989 - loss: 0.0041 - val_accuracy: 0.9913 - val_loss: 0.0608\n",
      "Epoch 27/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9992 - loss: 0.0028\n",
      "Epoch 27: val_accuracy did not improve from 0.99267\n",
      "\n",
      "Epoch 27: val_accuracy did not improve from 0.99267\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 15ms/step - accuracy: 0.9990 - loss: 0.0035 - val_accuracy: 0.9918 - val_loss: 0.0511\n",
      "Epoch 28/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9995 - loss: 0.0012\n",
      "Epoch 28: val_accuracy did not improve from 0.99267\n",
      "\n",
      "Epoch 28: val_accuracy did not improve from 0.99267\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 14ms/step - accuracy: 0.9996 - loss: 0.0010 - val_accuracy: 0.9915 - val_loss: 0.0651\n",
      "\n",
      "Training history saved to: CNN_Models\\Base_CNN-lr_0.001-bs_64-20250917-091616\\training_history.pkl\n",
      "\n",
      "--- Peak Performance Summary ---\n",
      "Best validation accuracy:           0.9927\n",
      "Associated training accuracy:       0.9990\n",
      "Occurred at epoch:                  18\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch/accuracy</td><td>▁▆▇▇▇▇██████████████████████</td></tr><tr><td>epoch/epoch</td><td>▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇██</td></tr><tr><td>epoch/learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/loss</td><td>█▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/val_accuracy</td><td>▁▃▄▄▃▆▅▅▅▆▇█▇▆▇▅▅██▆▅█▇▇█▆▇▆</td></tr><tr><td>epoch/val_loss</td><td>▃▂▂▁▃▁▃▄▄▂▃▄▆▅▃▇▄▅▅█▇▅█▅█▇▄█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch/accuracy</td><td>0.99963</td></tr><tr><td>epoch/epoch</td><td>27</td></tr><tr><td>epoch/learning_rate</td><td>0.001</td></tr><tr><td>epoch/loss</td><td>0.00105</td></tr><tr><td>epoch/val_accuracy</td><td>0.9915</td></tr><tr><td>epoch/val_loss</td><td>0.06513</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Base_CNN-lr_0.001-bs_64-20250917-091616</strong> at: <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/7uzr6og0' target=\"_blank\">https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/7uzr6og0</a><br> View project at: <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2' target=\"_blank\">https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2</a><br>Synced 5 W&B file(s), 0 media file(s), 16 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250917_091616-7uzr6og0\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Adapting the normalisation layer...\n",
      "Adaptation complete.\n",
      "\n",
      "\n",
      "--- Starting Experiment: Base_CNN ---\n",
      "\n",
      "--- Model Architecture ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"Base_CNN\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"Base_CNN\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ normalization_21                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Normalization</span>)                 │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1600</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_98 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">204,928</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_99 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,290</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ normalization_21                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m1\u001b[0m)      │             \u001b[38;5;34m3\u001b[0m │\n",
       "│ (\u001b[38;5;33mNormalization\u001b[0m)                 │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m320\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_3 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_21 (\u001b[38;5;33mFlatten\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1600\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_98 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m204,928\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_99 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │         \u001b[38;5;34m1,290\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">225,037</span> (879.05 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m225,037\u001b[0m (879.05 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">225,034</span> (879.04 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m225,034\u001b[0m (879.04 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3</span> (16.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m3\u001b[0m (16.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Hyperparameters ---\n",
      "optimiser           : adamw\n",
      "learning_rate       : 0.0001\n",
      "epochs              : 100\n",
      "batch_size          : 64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "creating run (0.0s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\TimVos\\VSC Projects\\CSE5ML\\Assessment 2\\wandb\\run-20250917_092156-xauj0n39</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/xauj0n39' target=\"_blank\">Base_CNN-lr_0.0001-bs_64-20250917-092156</a></strong> to <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2' target=\"_blank\">https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/xauj0n39' target=\"_blank\">https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/xauj0n39</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7776 - loss: 0.8389\n",
      "Epoch 1: val_accuracy improved from None to 0.97050, saving model to CNN_Models\\Base_CNN-lr_0.0001-bs_64-20250917-092156\\best_model_epoch-01_val_acc-0.9705.keras\n",
      "\n",
      "Epoch 1: val_accuracy improved from None to 0.97050, saving model to wandb_models/Base_CNN-lr_0.0001-bs_64-20250917-092156/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 15ms/step - accuracy: 0.8915 - loss: 0.4090 - val_accuracy: 0.9705 - val_loss: 0.1082\n",
      "Epoch 2/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9653 - loss: 0.1193\n",
      "Epoch 2: val_accuracy improved from 0.97050 to 0.98150, saving model to CNN_Models\\Base_CNN-lr_0.0001-bs_64-20250917-092156\\best_model_epoch-02_val_acc-0.9815.keras\n",
      "\n",
      "Epoch 2: val_accuracy improved from 0.97050 to 0.98150, saving model to wandb_models/Base_CNN-lr_0.0001-bs_64-20250917-092156/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.9682 - loss: 0.1081 - val_accuracy: 0.9815 - val_loss: 0.0687\n",
      "Epoch 3/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9774 - loss: 0.0775\n",
      "Epoch 3: val_accuracy improved from 0.98150 to 0.98433, saving model to CNN_Models\\Base_CNN-lr_0.0001-bs_64-20250917-092156\\best_model_epoch-03_val_acc-0.9843.keras\n",
      "\n",
      "Epoch 3: val_accuracy improved from 0.98150 to 0.98433, saving model to wandb_models/Base_CNN-lr_0.0001-bs_64-20250917-092156/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 14ms/step - accuracy: 0.9780 - loss: 0.0747 - val_accuracy: 0.9843 - val_loss: 0.0559\n",
      "Epoch 4/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9817 - loss: 0.0607\n",
      "Epoch 4: val_accuracy improved from 0.98433 to 0.98667, saving model to CNN_Models\\Base_CNN-lr_0.0001-bs_64-20250917-092156\\best_model_epoch-04_val_acc-0.9867.keras\n",
      "\n",
      "Epoch 4: val_accuracy improved from 0.98433 to 0.98667, saving model to wandb_models/Base_CNN-lr_0.0001-bs_64-20250917-092156/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 14ms/step - accuracy: 0.9820 - loss: 0.0597 - val_accuracy: 0.9867 - val_loss: 0.0503\n",
      "Epoch 5/100\n",
      "\u001b[1m840/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9850 - loss: 0.0509\n",
      "Epoch 5: val_accuracy improved from 0.98667 to 0.98700, saving model to CNN_Models\\Base_CNN-lr_0.0001-bs_64-20250917-092156\\best_model_epoch-05_val_acc-0.9870.keras\n",
      "\n",
      "Epoch 5: val_accuracy improved from 0.98667 to 0.98700, saving model to wandb_models/Base_CNN-lr_0.0001-bs_64-20250917-092156/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 14ms/step - accuracy: 0.9850 - loss: 0.0505 - val_accuracy: 0.9870 - val_loss: 0.0476\n",
      "Epoch 6/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9869 - loss: 0.0439\n",
      "Epoch 6: val_accuracy did not improve from 0.98700\n",
      "\n",
      "Epoch 6: val_accuracy did not improve from 0.98700\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - accuracy: 0.9870 - loss: 0.0440 - val_accuracy: 0.9868 - val_loss: 0.0458\n",
      "Epoch 7/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9885 - loss: 0.0386\n",
      "Epoch 7: val_accuracy did not improve from 0.98700\n",
      "\n",
      "Epoch 7: val_accuracy did not improve from 0.98700\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 14ms/step - accuracy: 0.9884 - loss: 0.0389 - val_accuracy: 0.9870 - val_loss: 0.0451\n",
      "Epoch 8/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9898 - loss: 0.0343\n",
      "Epoch 8: val_accuracy improved from 0.98700 to 0.98750, saving model to CNN_Models\\Base_CNN-lr_0.0001-bs_64-20250917-092156\\best_model_epoch-08_val_acc-0.9875.keras\n",
      "\n",
      "Epoch 8: val_accuracy improved from 0.98700 to 0.98750, saving model to wandb_models/Base_CNN-lr_0.0001-bs_64-20250917-092156/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 14ms/step - accuracy: 0.9897 - loss: 0.0345 - val_accuracy: 0.9875 - val_loss: 0.0443\n",
      "Epoch 9/100\n",
      "\u001b[1m840/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9913 - loss: 0.0304\n",
      "Epoch 9: val_accuracy did not improve from 0.98750\n",
      "\n",
      "Epoch 9: val_accuracy did not improve from 0.98750\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 14ms/step - accuracy: 0.9910 - loss: 0.0307 - val_accuracy: 0.9873 - val_loss: 0.0441\n",
      "Epoch 10/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9922 - loss: 0.0273\n",
      "Epoch 10: val_accuracy improved from 0.98750 to 0.98800, saving model to CNN_Models\\Base_CNN-lr_0.0001-bs_64-20250917-092156\\best_model_epoch-10_val_acc-0.9880.keras\n",
      "\n",
      "Epoch 10: val_accuracy improved from 0.98750 to 0.98800, saving model to wandb_models/Base_CNN-lr_0.0001-bs_64-20250917-092156/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - accuracy: 0.9919 - loss: 0.0274 - val_accuracy: 0.9880 - val_loss: 0.0439\n",
      "Epoch 11/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9933 - loss: 0.0245\n",
      "Epoch 11: val_accuracy did not improve from 0.98800\n",
      "\n",
      "Epoch 11: val_accuracy did not improve from 0.98800\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - accuracy: 0.9931 - loss: 0.0245 - val_accuracy: 0.9878 - val_loss: 0.0440\n",
      "Epoch 12/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9940 - loss: 0.0219\n",
      "Epoch 12: val_accuracy did not improve from 0.98800\n",
      "\n",
      "Epoch 12: val_accuracy did not improve from 0.98800\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - accuracy: 0.9939 - loss: 0.0219 - val_accuracy: 0.9877 - val_loss: 0.0437\n",
      "Epoch 13/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9947 - loss: 0.0197\n",
      "Epoch 13: val_accuracy improved from 0.98800 to 0.98850, saving model to CNN_Models\\Base_CNN-lr_0.0001-bs_64-20250917-092156\\best_model_epoch-13_val_acc-0.9885.keras\n",
      "\n",
      "Epoch 13: val_accuracy improved from 0.98800 to 0.98850, saving model to wandb_models/Base_CNN-lr_0.0001-bs_64-20250917-092156/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - accuracy: 0.9946 - loss: 0.0197 - val_accuracy: 0.9885 - val_loss: 0.0438\n",
      "Epoch 14/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9953 - loss: 0.0176\n",
      "Epoch 14: val_accuracy did not improve from 0.98850\n",
      "\n",
      "Epoch 14: val_accuracy did not improve from 0.98850\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - accuracy: 0.9953 - loss: 0.0175 - val_accuracy: 0.9882 - val_loss: 0.0441\n",
      "Epoch 15/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9960 - loss: 0.0156\n",
      "Epoch 15: val_accuracy improved from 0.98850 to 0.98867, saving model to CNN_Models\\Base_CNN-lr_0.0001-bs_64-20250917-092156\\best_model_epoch-15_val_acc-0.9887.keras\n",
      "\n",
      "Epoch 15: val_accuracy improved from 0.98850 to 0.98867, saving model to wandb_models/Base_CNN-lr_0.0001-bs_64-20250917-092156/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 14ms/step - accuracy: 0.9960 - loss: 0.0155 - val_accuracy: 0.9887 - val_loss: 0.0441\n",
      "Epoch 16/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9965 - loss: 0.0138\n",
      "Epoch 16: val_accuracy did not improve from 0.98867\n",
      "\n",
      "Epoch 16: val_accuracy did not improve from 0.98867\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - accuracy: 0.9966 - loss: 0.0138 - val_accuracy: 0.9880 - val_loss: 0.0449\n",
      "Epoch 17/100\n",
      "\u001b[1m840/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9971 - loss: 0.0121\n",
      "Epoch 17: val_accuracy did not improve from 0.98867\n",
      "\n",
      "Epoch 17: val_accuracy did not improve from 0.98867\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - accuracy: 0.9971 - loss: 0.0121 - val_accuracy: 0.9887 - val_loss: 0.0452\n",
      "Epoch 18/100\n",
      "\u001b[1m840/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9977 - loss: 0.0106\n",
      "Epoch 18: val_accuracy improved from 0.98867 to 0.98950, saving model to CNN_Models\\Base_CNN-lr_0.0001-bs_64-20250917-092156\\best_model_epoch-18_val_acc-0.9895.keras\n",
      "\n",
      "Epoch 18: val_accuracy improved from 0.98867 to 0.98950, saving model to wandb_models/Base_CNN-lr_0.0001-bs_64-20250917-092156/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 14ms/step - accuracy: 0.9977 - loss: 0.0107 - val_accuracy: 0.9895 - val_loss: 0.0441\n",
      "Epoch 19/100\n",
      "\u001b[1m840/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9980 - loss: 0.0093\n",
      "Epoch 19: val_accuracy did not improve from 0.98950\n",
      "\n",
      "Epoch 19: val_accuracy did not improve from 0.98950\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - accuracy: 0.9981 - loss: 0.0094 - val_accuracy: 0.9895 - val_loss: 0.0450\n",
      "Epoch 20/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9985 - loss: 0.0081\n",
      "Epoch 20: val_accuracy improved from 0.98950 to 0.99000, saving model to CNN_Models\\Base_CNN-lr_0.0001-bs_64-20250917-092156\\best_model_epoch-20_val_acc-0.9900.keras\n",
      "\n",
      "Epoch 20: val_accuracy improved from 0.98950 to 0.99000, saving model to wandb_models/Base_CNN-lr_0.0001-bs_64-20250917-092156/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - accuracy: 0.9983 - loss: 0.0083 - val_accuracy: 0.9900 - val_loss: 0.0448\n",
      "Epoch 21/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9987 - loss: 0.0070\n",
      "Epoch 21: val_accuracy did not improve from 0.99000\n",
      "\n",
      "Epoch 21: val_accuracy did not improve from 0.99000\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 14ms/step - accuracy: 0.9986 - loss: 0.0072 - val_accuracy: 0.9898 - val_loss: 0.0450\n",
      "Epoch 22/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9990 - loss: 0.0061\n",
      "Epoch 22: val_accuracy did not improve from 0.99000\n",
      "\n",
      "Epoch 22: val_accuracy did not improve from 0.99000\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 14ms/step - accuracy: 0.9989 - loss: 0.0063 - val_accuracy: 0.9892 - val_loss: 0.0463\n",
      "Epoch 23/100\n",
      "\u001b[1m839/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9992 - loss: 0.0053\n",
      "Epoch 23: val_accuracy did not improve from 0.99000\n",
      "\n",
      "Epoch 23: val_accuracy did not improve from 0.99000\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 14ms/step - accuracy: 0.9991 - loss: 0.0055 - val_accuracy: 0.9900 - val_loss: 0.0472\n",
      "Epoch 24/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9992 - loss: 0.0046\n",
      "Epoch 24: val_accuracy did not improve from 0.99000\n",
      "\n",
      "Epoch 24: val_accuracy did not improve from 0.99000\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 14ms/step - accuracy: 0.9992 - loss: 0.0048 - val_accuracy: 0.9893 - val_loss: 0.0481\n",
      "Epoch 25/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9993 - loss: 0.0041\n",
      "Epoch 25: val_accuracy did not improve from 0.99000\n",
      "\n",
      "Epoch 25: val_accuracy did not improve from 0.99000\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 14ms/step - accuracy: 0.9990 - loss: 0.0046 - val_accuracy: 0.9897 - val_loss: 0.0510\n",
      "Epoch 26/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9992 - loss: 0.0041\n",
      "Epoch 26: val_accuracy did not improve from 0.99000\n",
      "\n",
      "Epoch 26: val_accuracy did not improve from 0.99000\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 14ms/step - accuracy: 0.9989 - loss: 0.0046 - val_accuracy: 0.9897 - val_loss: 0.0530\n",
      "Epoch 27/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9986 - loss: 0.0050\n",
      "Epoch 27: val_accuracy improved from 0.99000 to 0.99017, saving model to CNN_Models\\Base_CNN-lr_0.0001-bs_64-20250917-092156\\best_model_epoch-27_val_acc-0.9902.keras\n",
      "\n",
      "Epoch 27: val_accuracy improved from 0.99000 to 0.99017, saving model to wandb_models/Base_CNN-lr_0.0001-bs_64-20250917-092156/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 14ms/step - accuracy: 0.9988 - loss: 0.0047 - val_accuracy: 0.9902 - val_loss: 0.0513\n",
      "Epoch 28/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9994 - loss: 0.0032\n",
      "Epoch 28: val_accuracy did not improve from 0.99017\n",
      "\n",
      "Epoch 28: val_accuracy did not improve from 0.99017\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 14ms/step - accuracy: 0.9993 - loss: 0.0033 - val_accuracy: 0.9902 - val_loss: 0.0499\n",
      "Epoch 29/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9994 - loss: 0.0029\n",
      "Epoch 29: val_accuracy improved from 0.99017 to 0.99067, saving model to CNN_Models\\Base_CNN-lr_0.0001-bs_64-20250917-092156\\best_model_epoch-29_val_acc-0.9907.keras\n",
      "\n",
      "Epoch 29: val_accuracy improved from 0.99017 to 0.99067, saving model to wandb_models/Base_CNN-lr_0.0001-bs_64-20250917-092156/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 14ms/step - accuracy: 0.9992 - loss: 0.0034 - val_accuracy: 0.9907 - val_loss: 0.0521\n",
      "Epoch 30/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9995 - loss: 0.0028\n",
      "Epoch 30: val_accuracy did not improve from 0.99067\n",
      "\n",
      "Epoch 30: val_accuracy did not improve from 0.99067\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 14ms/step - accuracy: 0.9992 - loss: 0.0030 - val_accuracy: 0.9902 - val_loss: 0.0526\n",
      "Epoch 31/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9996 - loss: 0.0024\n",
      "Epoch 31: val_accuracy did not improve from 0.99067\n",
      "\n",
      "Epoch 31: val_accuracy did not improve from 0.99067\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.9994 - loss: 0.0028 - val_accuracy: 0.9898 - val_loss: 0.0525\n",
      "Epoch 32/100\n",
      "\u001b[1m839/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9997 - loss: 0.0021\n",
      "Epoch 32: val_accuracy did not improve from 0.99067\n",
      "\n",
      "Epoch 32: val_accuracy did not improve from 0.99067\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 14ms/step - accuracy: 0.9995 - loss: 0.0025 - val_accuracy: 0.9903 - val_loss: 0.0533\n",
      "Epoch 33/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9996 - loss: 0.0020\n",
      "Epoch 33: val_accuracy did not improve from 0.99067\n",
      "\n",
      "Epoch 33: val_accuracy did not improve from 0.99067\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 14ms/step - accuracy: 0.9994 - loss: 0.0025 - val_accuracy: 0.9898 - val_loss: 0.0552\n",
      "Epoch 34/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9987 - loss: 0.0040\n",
      "Epoch 34: val_accuracy did not improve from 0.99067\n",
      "\n",
      "Epoch 34: val_accuracy did not improve from 0.99067\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 15ms/step - accuracy: 0.9991 - loss: 0.0030 - val_accuracy: 0.9905 - val_loss: 0.0562\n",
      "Epoch 35/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9997 - loss: 0.0014\n",
      "Epoch 35: val_accuracy did not improve from 0.99067\n",
      "\n",
      "Epoch 35: val_accuracy did not improve from 0.99067\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 16ms/step - accuracy: 0.9996 - loss: 0.0016 - val_accuracy: 0.9897 - val_loss: 0.0554\n",
      "Epoch 36/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9995 - loss: 0.0020\n",
      "Epoch 36: val_accuracy did not improve from 0.99067\n",
      "\n",
      "Epoch 36: val_accuracy did not improve from 0.99067\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.9992 - loss: 0.0027 - val_accuracy: 0.9907 - val_loss: 0.0568\n",
      "Epoch 37/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9998 - loss: 0.0013\n",
      "Epoch 37: val_accuracy did not improve from 0.99067\n",
      "\n",
      "Epoch 37: val_accuracy did not improve from 0.99067\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.9997 - loss: 0.0015 - val_accuracy: 0.9902 - val_loss: 0.0573\n",
      "Epoch 38/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9997 - loss: 0.0017\n",
      "Epoch 38: val_accuracy did not improve from 0.99067\n",
      "\n",
      "Epoch 38: val_accuracy did not improve from 0.99067\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 14ms/step - accuracy: 0.9996 - loss: 0.0017 - val_accuracy: 0.9897 - val_loss: 0.0611\n",
      "Epoch 39/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9999 - loss: 9.4923e-04\n",
      "Epoch 39: val_accuracy did not improve from 0.99067\n",
      "\n",
      "Epoch 39: val_accuracy did not improve from 0.99067\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 14ms/step - accuracy: 0.9998 - loss: 0.0012 - val_accuracy: 0.9903 - val_loss: 0.0574\n",
      "\n",
      "Training history saved to: CNN_Models\\Base_CNN-lr_0.0001-bs_64-20250917-092156\\training_history.pkl\n",
      "\n",
      "--- Peak Performance Summary ---\n",
      "Best validation accuracy:           0.9907\n",
      "Associated training accuracy:       0.9992\n",
      "Occurred at epoch:                  29\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch/accuracy</td><td>▁▆▇▇▇▇▇▇▇▇█████████████████████████████</td></tr><tr><td>epoch/epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>epoch/learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/loss</td><td>█▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/val_accuracy</td><td>▁▅▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇████▇█████████████████</td></tr><tr><td>epoch/val_loss</td><td>█▄▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂▂▂▂▂▃▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch/accuracy</td><td>0.99978</td></tr><tr><td>epoch/epoch</td><td>38</td></tr><tr><td>epoch/learning_rate</td><td>0.0001</td></tr><tr><td>epoch/loss</td><td>0.00123</td></tr><tr><td>epoch/val_accuracy</td><td>0.99033</td></tr><tr><td>epoch/val_loss</td><td>0.05742</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Base_CNN-lr_0.0001-bs_64-20250917-092156</strong> at: <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/xauj0n39' target=\"_blank\">https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/xauj0n39</a><br> View project at: <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2' target=\"_blank\">https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2</a><br>Synced 5 W&B file(s), 0 media file(s), 26 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250917_092156-xauj0n39\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Adapting the normalisation layer...\n",
      "Adaptation complete.\n",
      "\n",
      "\n",
      "--- Starting Experiment: Base_CNN ---\n",
      "\n",
      "--- Model Architecture ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"Base_CNN\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"Base_CNN\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ normalization_22                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Normalization</span>)                 │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1600</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_100 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">204,928</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_101 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,290</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ normalization_22                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m1\u001b[0m)      │             \u001b[38;5;34m3\u001b[0m │\n",
       "│ (\u001b[38;5;33mNormalization\u001b[0m)                 │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_4 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m320\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_4 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_5 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_5 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_22 (\u001b[38;5;33mFlatten\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1600\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_100 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m204,928\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_101 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │         \u001b[38;5;34m1,290\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">225,037</span> (879.05 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m225,037\u001b[0m (879.05 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">225,034</span> (879.04 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m225,034\u001b[0m (879.04 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3</span> (16.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m3\u001b[0m (16.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Hyperparameters ---\n",
      "optimiser           : SGD\n",
      "learning_rate       : 0.001\n",
      "epochs              : 100\n",
      "batch_size          : 64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "creating run (0.0s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\TimVos\\VSC Projects\\CSE5ML\\Assessment 2\\wandb\\run-20250917_092953-aat0yd33</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/aat0yd33' target=\"_blank\">Base_CNN-lr_0.001-bs_64-20250917-092953</a></strong> to <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2' target=\"_blank\">https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/aat0yd33' target=\"_blank\">https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/aat0yd33</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m839/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.3606 - loss: 2.0603\n",
      "Epoch 1: val_accuracy improved from None to 0.83617, saving model to CNN_Models\\Base_CNN-lr_0.001-bs_64-20250917-092953\\best_model_epoch-01_val_acc-0.8362.keras\n",
      "\n",
      "Epoch 1: val_accuracy improved from None to 0.83617, saving model to wandb_models/Base_CNN-lr_0.001-bs_64-20250917-092953/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 15ms/step - accuracy: 0.5573 - loss: 1.6917 - val_accuracy: 0.8362 - val_loss: 0.7865\n",
      "Epoch 2/100\n",
      "\u001b[1m840/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8338 - loss: 0.6637\n",
      "Epoch 2: val_accuracy improved from 0.83617 to 0.91050, saving model to CNN_Models\\Base_CNN-lr_0.001-bs_64-20250917-092953\\best_model_epoch-02_val_acc-0.9105.keras\n",
      "\n",
      "Epoch 2: val_accuracy improved from 0.83617 to 0.91050, saving model to wandb_models/Base_CNN-lr_0.001-bs_64-20250917-092953/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.8561 - loss: 0.5538 - val_accuracy: 0.9105 - val_loss: 0.3383\n",
      "Epoch 3/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8913 - loss: 0.3837\n",
      "Epoch 3: val_accuracy improved from 0.91050 to 0.92883, saving model to CNN_Models\\Base_CNN-lr_0.001-bs_64-20250917-092953\\best_model_epoch-03_val_acc-0.9288.keras\n",
      "\n",
      "Epoch 3: val_accuracy improved from 0.91050 to 0.92883, saving model to wandb_models/Base_CNN-lr_0.001-bs_64-20250917-092953/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 14ms/step - accuracy: 0.8964 - loss: 0.3622 - val_accuracy: 0.9288 - val_loss: 0.2574\n",
      "Epoch 4/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9114 - loss: 0.3074\n",
      "Epoch 4: val_accuracy improved from 0.92883 to 0.93950, saving model to CNN_Models\\Base_CNN-lr_0.001-bs_64-20250917-092953\\best_model_epoch-04_val_acc-0.9395.keras\n",
      "\n",
      "Epoch 4: val_accuracy improved from 0.92883 to 0.93950, saving model to wandb_models/Base_CNN-lr_0.001-bs_64-20250917-092953/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.9135 - loss: 0.2979 - val_accuracy: 0.9395 - val_loss: 0.2181\n",
      "Epoch 5/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9220 - loss: 0.2641\n",
      "Epoch 5: val_accuracy improved from 0.93950 to 0.94733, saving model to CNN_Models\\Base_CNN-lr_0.001-bs_64-20250917-092953\\best_model_epoch-05_val_acc-0.9473.keras\n",
      "\n",
      "Epoch 5: val_accuracy improved from 0.93950 to 0.94733, saving model to wandb_models/Base_CNN-lr_0.001-bs_64-20250917-092953/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 18ms/step - accuracy: 0.9242 - loss: 0.2588 - val_accuracy: 0.9473 - val_loss: 0.1922\n",
      "Epoch 6/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9315 - loss: 0.2333\n",
      "Epoch 6: val_accuracy improved from 0.94733 to 0.95317, saving model to CNN_Models\\Base_CNN-lr_0.001-bs_64-20250917-092953\\best_model_epoch-06_val_acc-0.9532.keras\n",
      "\n",
      "Epoch 6: val_accuracy improved from 0.94733 to 0.95317, saving model to wandb_models/Base_CNN-lr_0.001-bs_64-20250917-092953/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 16ms/step - accuracy: 0.9325 - loss: 0.2303 - val_accuracy: 0.9532 - val_loss: 0.1732\n",
      "Epoch 7/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9385 - loss: 0.2095\n",
      "Epoch 7: val_accuracy improved from 0.95317 to 0.95850, saving model to CNN_Models\\Base_CNN-lr_0.001-bs_64-20250917-092953\\best_model_epoch-07_val_acc-0.9585.keras\n",
      "\n",
      "Epoch 7: val_accuracy improved from 0.95317 to 0.95850, saving model to wandb_models/Base_CNN-lr_0.001-bs_64-20250917-092953/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 16ms/step - accuracy: 0.9390 - loss: 0.2079 - val_accuracy: 0.9585 - val_loss: 0.1582\n",
      "Epoch 8/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9438 - loss: 0.1902\n",
      "Epoch 8: val_accuracy improved from 0.95850 to 0.96267, saving model to CNN_Models\\Base_CNN-lr_0.001-bs_64-20250917-092953\\best_model_epoch-08_val_acc-0.9627.keras\n",
      "\n",
      "Epoch 8: val_accuracy improved from 0.95850 to 0.96267, saving model to wandb_models/Base_CNN-lr_0.001-bs_64-20250917-092953/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 15ms/step - accuracy: 0.9442 - loss: 0.1896 - val_accuracy: 0.9627 - val_loss: 0.1459\n",
      "Epoch 9/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9487 - loss: 0.1742\n",
      "Epoch 9: val_accuracy improved from 0.96267 to 0.96550, saving model to CNN_Models\\Base_CNN-lr_0.001-bs_64-20250917-092953\\best_model_epoch-09_val_acc-0.9655.keras\n",
      "\n",
      "Epoch 9: val_accuracy improved from 0.96267 to 0.96550, saving model to wandb_models/Base_CNN-lr_0.001-bs_64-20250917-092953/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 14ms/step - accuracy: 0.9491 - loss: 0.1742 - val_accuracy: 0.9655 - val_loss: 0.1356\n",
      "Epoch 10/100\n",
      "\u001b[1m840/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9537 - loss: 0.1607\n",
      "Epoch 10: val_accuracy improved from 0.96550 to 0.96767, saving model to CNN_Models\\Base_CNN-lr_0.001-bs_64-20250917-092953\\best_model_epoch-10_val_acc-0.9677.keras\n",
      "\n",
      "Epoch 10: val_accuracy improved from 0.96550 to 0.96767, saving model to wandb_models/Base_CNN-lr_0.001-bs_64-20250917-092953/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 15ms/step - accuracy: 0.9533 - loss: 0.1612 - val_accuracy: 0.9677 - val_loss: 0.1266\n",
      "Epoch 11/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9577 - loss: 0.1492\n",
      "Epoch 11: val_accuracy improved from 0.96767 to 0.97017, saving model to CNN_Models\\Base_CNN-lr_0.001-bs_64-20250917-092953\\best_model_epoch-11_val_acc-0.9702.keras\n",
      "\n",
      "Epoch 11: val_accuracy improved from 0.96767 to 0.97017, saving model to wandb_models/Base_CNN-lr_0.001-bs_64-20250917-092953/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 14ms/step - accuracy: 0.9569 - loss: 0.1500 - val_accuracy: 0.9702 - val_loss: 0.1190\n",
      "Epoch 12/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9608 - loss: 0.1394\n",
      "Epoch 12: val_accuracy improved from 0.97017 to 0.97183, saving model to CNN_Models\\Base_CNN-lr_0.001-bs_64-20250917-092953\\best_model_epoch-12_val_acc-0.9718.keras\n",
      "\n",
      "Epoch 12: val_accuracy improved from 0.97017 to 0.97183, saving model to wandb_models/Base_CNN-lr_0.001-bs_64-20250917-092953/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 14ms/step - accuracy: 0.9600 - loss: 0.1404 - val_accuracy: 0.9718 - val_loss: 0.1126\n",
      "Epoch 13/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9629 - loss: 0.1310\n",
      "Epoch 13: val_accuracy improved from 0.97183 to 0.97233, saving model to CNN_Models\\Base_CNN-lr_0.001-bs_64-20250917-092953\\best_model_epoch-13_val_acc-0.9723.keras\n",
      "\n",
      "Epoch 13: val_accuracy improved from 0.97183 to 0.97233, saving model to wandb_models/Base_CNN-lr_0.001-bs_64-20250917-092953/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 14ms/step - accuracy: 0.9623 - loss: 0.1321 - val_accuracy: 0.9723 - val_loss: 0.1071\n",
      "Epoch 14/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9645 - loss: 0.1237\n",
      "Epoch 14: val_accuracy improved from 0.97233 to 0.97350, saving model to CNN_Models\\Base_CNN-lr_0.001-bs_64-20250917-092953\\best_model_epoch-14_val_acc-0.9735.keras\n",
      "\n",
      "Epoch 14: val_accuracy improved from 0.97233 to 0.97350, saving model to wandb_models/Base_CNN-lr_0.001-bs_64-20250917-092953/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 15ms/step - accuracy: 0.9640 - loss: 0.1249 - val_accuracy: 0.9735 - val_loss: 0.1023\n",
      "Epoch 15/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9666 - loss: 0.1173\n",
      "Epoch 15: val_accuracy improved from 0.97350 to 0.97517, saving model to CNN_Models\\Base_CNN-lr_0.001-bs_64-20250917-092953\\best_model_epoch-15_val_acc-0.9752.keras\n",
      "\n",
      "Epoch 15: val_accuracy improved from 0.97350 to 0.97517, saving model to wandb_models/Base_CNN-lr_0.001-bs_64-20250917-092953/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 14ms/step - accuracy: 0.9658 - loss: 0.1186 - val_accuracy: 0.9752 - val_loss: 0.0981\n",
      "Epoch 16/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9679 - loss: 0.1117\n",
      "Epoch 16: val_accuracy improved from 0.97517 to 0.97567, saving model to CNN_Models\\Base_CNN-lr_0.001-bs_64-20250917-092953\\best_model_epoch-16_val_acc-0.9757.keras\n",
      "\n",
      "Epoch 16: val_accuracy improved from 0.97517 to 0.97567, saving model to wandb_models/Base_CNN-lr_0.001-bs_64-20250917-092953/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.9671 - loss: 0.1130 - val_accuracy: 0.9757 - val_loss: 0.0943\n",
      "Epoch 17/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9691 - loss: 0.1066\n",
      "Epoch 17: val_accuracy improved from 0.97567 to 0.97650, saving model to CNN_Models\\Base_CNN-lr_0.001-bs_64-20250917-092953\\best_model_epoch-17_val_acc-0.9765.keras\n",
      "\n",
      "Epoch 17: val_accuracy improved from 0.97567 to 0.97650, saving model to wandb_models/Base_CNN-lr_0.001-bs_64-20250917-092953/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 14ms/step - accuracy: 0.9685 - loss: 0.1080 - val_accuracy: 0.9765 - val_loss: 0.0910\n",
      "Epoch 18/100\n",
      "\u001b[1m840/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9706 - loss: 0.1021\n",
      "Epoch 18: val_accuracy improved from 0.97650 to 0.97683, saving model to CNN_Models\\Base_CNN-lr_0.001-bs_64-20250917-092953\\best_model_epoch-18_val_acc-0.9768.keras\n",
      "\n",
      "Epoch 18: val_accuracy improved from 0.97650 to 0.97683, saving model to wandb_models/Base_CNN-lr_0.001-bs_64-20250917-092953/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - accuracy: 0.9698 - loss: 0.1035 - val_accuracy: 0.9768 - val_loss: 0.0881\n",
      "Epoch 19/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9713 - loss: 0.0980\n",
      "Epoch 19: val_accuracy improved from 0.97683 to 0.97733, saving model to CNN_Models\\Base_CNN-lr_0.001-bs_64-20250917-092953\\best_model_epoch-19_val_acc-0.9773.keras\n",
      "\n",
      "Epoch 19: val_accuracy improved from 0.97683 to 0.97733, saving model to wandb_models/Base_CNN-lr_0.001-bs_64-20250917-092953/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 12ms/step - accuracy: 0.9707 - loss: 0.0994 - val_accuracy: 0.9773 - val_loss: 0.0855\n",
      "Epoch 20/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9719 - loss: 0.0944\n",
      "Epoch 20: val_accuracy improved from 0.97733 to 0.97750, saving model to CNN_Models\\Base_CNN-lr_0.001-bs_64-20250917-092953\\best_model_epoch-20_val_acc-0.9775.keras\n",
      "\n",
      "Epoch 20: val_accuracy improved from 0.97733 to 0.97750, saving model to wandb_models/Base_CNN-lr_0.001-bs_64-20250917-092953/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - accuracy: 0.9716 - loss: 0.0957 - val_accuracy: 0.9775 - val_loss: 0.0831\n",
      "Epoch 21/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9731 - loss: 0.0910\n",
      "Epoch 21: val_accuracy improved from 0.97750 to 0.97800, saving model to CNN_Models\\Base_CNN-lr_0.001-bs_64-20250917-092953\\best_model_epoch-21_val_acc-0.9780.keras\n",
      "\n",
      "Epoch 21: val_accuracy improved from 0.97750 to 0.97800, saving model to wandb_models/Base_CNN-lr_0.001-bs_64-20250917-092953/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.9725 - loss: 0.0923 - val_accuracy: 0.9780 - val_loss: 0.0809\n",
      "Epoch 22/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9742 - loss: 0.0879\n",
      "Epoch 22: val_accuracy improved from 0.97800 to 0.97883, saving model to CNN_Models\\Base_CNN-lr_0.001-bs_64-20250917-092953\\best_model_epoch-22_val_acc-0.9788.keras\n",
      "\n",
      "Epoch 22: val_accuracy improved from 0.97800 to 0.97883, saving model to wandb_models/Base_CNN-lr_0.001-bs_64-20250917-092953/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 14ms/step - accuracy: 0.9738 - loss: 0.0893 - val_accuracy: 0.9788 - val_loss: 0.0790\n",
      "Epoch 23/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9747 - loss: 0.0851\n",
      "Epoch 23: val_accuracy improved from 0.97883 to 0.97967, saving model to CNN_Models\\Base_CNN-lr_0.001-bs_64-20250917-092953\\best_model_epoch-23_val_acc-0.9797.keras\n",
      "\n",
      "Epoch 23: val_accuracy improved from 0.97883 to 0.97967, saving model to wandb_models/Base_CNN-lr_0.001-bs_64-20250917-092953/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 14ms/step - accuracy: 0.9745 - loss: 0.0864 - val_accuracy: 0.9797 - val_loss: 0.0771\n",
      "Epoch 24/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9755 - loss: 0.0824\n",
      "Epoch 24: val_accuracy did not improve from 0.97967\n",
      "\n",
      "Epoch 24: val_accuracy did not improve from 0.97967\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 14ms/step - accuracy: 0.9752 - loss: 0.0838 - val_accuracy: 0.9797 - val_loss: 0.0755\n",
      "Epoch 25/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9763 - loss: 0.0800\n",
      "Epoch 25: val_accuracy improved from 0.97967 to 0.98000, saving model to CNN_Models\\Base_CNN-lr_0.001-bs_64-20250917-092953\\best_model_epoch-25_val_acc-0.9800.keras\n",
      "\n",
      "Epoch 25: val_accuracy improved from 0.97967 to 0.98000, saving model to wandb_models/Base_CNN-lr_0.001-bs_64-20250917-092953/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 14ms/step - accuracy: 0.9761 - loss: 0.0814 - val_accuracy: 0.9800 - val_loss: 0.0741\n",
      "Epoch 26/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9771 - loss: 0.0777\n",
      "Epoch 26: val_accuracy improved from 0.98000 to 0.98033, saving model to CNN_Models\\Base_CNN-lr_0.001-bs_64-20250917-092953\\best_model_epoch-26_val_acc-0.9803.keras\n",
      "\n",
      "Epoch 26: val_accuracy improved from 0.98000 to 0.98033, saving model to wandb_models/Base_CNN-lr_0.001-bs_64-20250917-092953/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 14ms/step - accuracy: 0.9768 - loss: 0.0791 - val_accuracy: 0.9803 - val_loss: 0.0727\n",
      "Epoch 27/100\n",
      "\u001b[1m840/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9777 - loss: 0.0756\n",
      "Epoch 27: val_accuracy did not improve from 0.98033\n",
      "\n",
      "Epoch 27: val_accuracy did not improve from 0.98033\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - accuracy: 0.9773 - loss: 0.0770 - val_accuracy: 0.9802 - val_loss: 0.0714\n",
      "Epoch 28/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9784 - loss: 0.0736\n",
      "Epoch 28: val_accuracy did not improve from 0.98033\n",
      "\n",
      "Epoch 28: val_accuracy did not improve from 0.98033\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - accuracy: 0.9779 - loss: 0.0750 - val_accuracy: 0.9800 - val_loss: 0.0702\n",
      "Epoch 29/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9789 - loss: 0.0718\n",
      "Epoch 29: val_accuracy did not improve from 0.98033\n",
      "\n",
      "Epoch 29: val_accuracy did not improve from 0.98033\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - accuracy: 0.9785 - loss: 0.0732 - val_accuracy: 0.9798 - val_loss: 0.0691\n",
      "Epoch 30/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9792 - loss: 0.0701\n",
      "Epoch 30: val_accuracy did not improve from 0.98033\n",
      "\n",
      "Epoch 30: val_accuracy did not improve from 0.98033\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - accuracy: 0.9789 - loss: 0.0715 - val_accuracy: 0.9803 - val_loss: 0.0680\n",
      "Epoch 31/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9796 - loss: 0.0684\n",
      "Epoch 31: val_accuracy did not improve from 0.98033\n",
      "\n",
      "Epoch 31: val_accuracy did not improve from 0.98033\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - accuracy: 0.9794 - loss: 0.0698 - val_accuracy: 0.9803 - val_loss: 0.0670\n",
      "Epoch 32/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9800 - loss: 0.0668\n",
      "Epoch 32: val_accuracy improved from 0.98033 to 0.98133, saving model to CNN_Models\\Base_CNN-lr_0.001-bs_64-20250917-092953\\best_model_epoch-32_val_acc-0.9813.keras\n",
      "\n",
      "Epoch 32: val_accuracy improved from 0.98033 to 0.98133, saving model to wandb_models/Base_CNN-lr_0.001-bs_64-20250917-092953/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 12ms/step - accuracy: 0.9797 - loss: 0.0683 - val_accuracy: 0.9813 - val_loss: 0.0661\n",
      "Epoch 33/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9807 - loss: 0.0653\n",
      "Epoch 33: val_accuracy improved from 0.98133 to 0.98200, saving model to CNN_Models\\Base_CNN-lr_0.001-bs_64-20250917-092953\\best_model_epoch-33_val_acc-0.9820.keras\n",
      "\n",
      "Epoch 33: val_accuracy improved from 0.98133 to 0.98200, saving model to wandb_models/Base_CNN-lr_0.001-bs_64-20250917-092953/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - accuracy: 0.9803 - loss: 0.0668 - val_accuracy: 0.9820 - val_loss: 0.0653\n",
      "Epoch 34/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9811 - loss: 0.0639\n",
      "Epoch 34: val_accuracy improved from 0.98200 to 0.98217, saving model to CNN_Models\\Base_CNN-lr_0.001-bs_64-20250917-092953\\best_model_epoch-34_val_acc-0.9822.keras\n",
      "\n",
      "Epoch 34: val_accuracy improved from 0.98200 to 0.98217, saving model to wandb_models/Base_CNN-lr_0.001-bs_64-20250917-092953/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - accuracy: 0.9807 - loss: 0.0654 - val_accuracy: 0.9822 - val_loss: 0.0644\n",
      "Epoch 35/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9817 - loss: 0.0626\n",
      "Epoch 35: val_accuracy improved from 0.98217 to 0.98233, saving model to CNN_Models\\Base_CNN-lr_0.001-bs_64-20250917-092953\\best_model_epoch-35_val_acc-0.9823.keras\n",
      "\n",
      "Epoch 35: val_accuracy improved from 0.98217 to 0.98233, saving model to wandb_models/Base_CNN-lr_0.001-bs_64-20250917-092953/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - accuracy: 0.9811 - loss: 0.0641 - val_accuracy: 0.9823 - val_loss: 0.0637\n",
      "Epoch 36/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9821 - loss: 0.0613\n",
      "Epoch 36: val_accuracy improved from 0.98233 to 0.98250, saving model to CNN_Models\\Base_CNN-lr_0.001-bs_64-20250917-092953\\best_model_epoch-36_val_acc-0.9825.keras\n",
      "\n",
      "Epoch 36: val_accuracy improved from 0.98233 to 0.98250, saving model to wandb_models/Base_CNN-lr_0.001-bs_64-20250917-092953/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - accuracy: 0.9816 - loss: 0.0628 - val_accuracy: 0.9825 - val_loss: 0.0629\n",
      "Epoch 37/100\n",
      "\u001b[1m840/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9824 - loss: 0.0601\n",
      "Epoch 37: val_accuracy improved from 0.98250 to 0.98283, saving model to CNN_Models\\Base_CNN-lr_0.001-bs_64-20250917-092953\\best_model_epoch-37_val_acc-0.9828.keras\n",
      "\n",
      "Epoch 37: val_accuracy improved from 0.98250 to 0.98283, saving model to wandb_models/Base_CNN-lr_0.001-bs_64-20250917-092953/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - accuracy: 0.9819 - loss: 0.0616 - val_accuracy: 0.9828 - val_loss: 0.0622\n",
      "Epoch 38/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9826 - loss: 0.0590\n",
      "Epoch 38: val_accuracy improved from 0.98283 to 0.98300, saving model to CNN_Models\\Base_CNN-lr_0.001-bs_64-20250917-092953\\best_model_epoch-38_val_acc-0.9830.keras\n",
      "\n",
      "Epoch 38: val_accuracy improved from 0.98283 to 0.98300, saving model to wandb_models/Base_CNN-lr_0.001-bs_64-20250917-092953/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 14ms/step - accuracy: 0.9822 - loss: 0.0605 - val_accuracy: 0.9830 - val_loss: 0.0616\n",
      "Epoch 39/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9831 - loss: 0.0579\n",
      "Epoch 39: val_accuracy did not improve from 0.98300\n",
      "\n",
      "Epoch 39: val_accuracy did not improve from 0.98300\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - accuracy: 0.9826 - loss: 0.0594 - val_accuracy: 0.9830 - val_loss: 0.0610\n",
      "Epoch 40/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9835 - loss: 0.0569\n",
      "Epoch 40: val_accuracy improved from 0.98300 to 0.98317, saving model to CNN_Models\\Base_CNN-lr_0.001-bs_64-20250917-092953\\best_model_epoch-40_val_acc-0.9832.keras\n",
      "\n",
      "Epoch 40: val_accuracy improved from 0.98300 to 0.98317, saving model to wandb_models/Base_CNN-lr_0.001-bs_64-20250917-092953/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 14ms/step - accuracy: 0.9830 - loss: 0.0583 - val_accuracy: 0.9832 - val_loss: 0.0604\n",
      "Epoch 41/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9837 - loss: 0.0558\n",
      "Epoch 41: val_accuracy improved from 0.98317 to 0.98367, saving model to CNN_Models\\Base_CNN-lr_0.001-bs_64-20250917-092953\\best_model_epoch-41_val_acc-0.9837.keras\n",
      "\n",
      "Epoch 41: val_accuracy improved from 0.98317 to 0.98367, saving model to wandb_models/Base_CNN-lr_0.001-bs_64-20250917-092953/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - accuracy: 0.9832 - loss: 0.0573 - val_accuracy: 0.9837 - val_loss: 0.0598\n",
      "Epoch 42/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9839 - loss: 0.0549\n",
      "Epoch 42: val_accuracy did not improve from 0.98367\n",
      "\n",
      "Epoch 42: val_accuracy did not improve from 0.98367\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - accuracy: 0.9835 - loss: 0.0564 - val_accuracy: 0.9837 - val_loss: 0.0593\n",
      "Epoch 43/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9839 - loss: 0.0540\n",
      "Epoch 43: val_accuracy did not improve from 0.98367\n",
      "\n",
      "Epoch 43: val_accuracy did not improve from 0.98367\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 14ms/step - accuracy: 0.9835 - loss: 0.0554 - val_accuracy: 0.9832 - val_loss: 0.0587\n",
      "Epoch 44/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9841 - loss: 0.0531\n",
      "Epoch 44: val_accuracy did not improve from 0.98367\n",
      "\n",
      "Epoch 44: val_accuracy did not improve from 0.98367\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 14ms/step - accuracy: 0.9838 - loss: 0.0546 - val_accuracy: 0.9835 - val_loss: 0.0583\n",
      "Epoch 45/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9843 - loss: 0.0522\n",
      "Epoch 45: val_accuracy did not improve from 0.98367\n",
      "\n",
      "Epoch 45: val_accuracy did not improve from 0.98367\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 15ms/step - accuracy: 0.9841 - loss: 0.0537 - val_accuracy: 0.9837 - val_loss: 0.0578\n",
      "Epoch 46/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9846 - loss: 0.0514\n",
      "Epoch 46: val_accuracy improved from 0.98367 to 0.98383, saving model to CNN_Models\\Base_CNN-lr_0.001-bs_64-20250917-092953\\best_model_epoch-46_val_acc-0.9838.keras\n",
      "\n",
      "Epoch 46: val_accuracy improved from 0.98367 to 0.98383, saving model to wandb_models/Base_CNN-lr_0.001-bs_64-20250917-092953/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.9844 - loss: 0.0529 - val_accuracy: 0.9838 - val_loss: 0.0573\n",
      "Epoch 47/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9849 - loss: 0.0505\n",
      "Epoch 47: val_accuracy improved from 0.98383 to 0.98400, saving model to CNN_Models\\Base_CNN-lr_0.001-bs_64-20250917-092953\\best_model_epoch-47_val_acc-0.9840.keras\n",
      "\n",
      "Epoch 47: val_accuracy improved from 0.98383 to 0.98400, saving model to wandb_models/Base_CNN-lr_0.001-bs_64-20250917-092953/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 14ms/step - accuracy: 0.9846 - loss: 0.0520 - val_accuracy: 0.9840 - val_loss: 0.0569\n",
      "Epoch 48/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9851 - loss: 0.0498\n",
      "Epoch 48: val_accuracy did not improve from 0.98400\n",
      "\n",
      "Epoch 48: val_accuracy did not improve from 0.98400\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 14ms/step - accuracy: 0.9849 - loss: 0.0513 - val_accuracy: 0.9840 - val_loss: 0.0564\n",
      "Epoch 49/100\n",
      "\u001b[1m839/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9852 - loss: 0.0490\n",
      "Epoch 49: val_accuracy improved from 0.98400 to 0.98417, saving model to CNN_Models\\Base_CNN-lr_0.001-bs_64-20250917-092953\\best_model_epoch-49_val_acc-0.9842.keras\n",
      "\n",
      "Epoch 49: val_accuracy improved from 0.98400 to 0.98417, saving model to wandb_models/Base_CNN-lr_0.001-bs_64-20250917-092953/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - accuracy: 0.9850 - loss: 0.0505 - val_accuracy: 0.9842 - val_loss: 0.0560\n",
      "Epoch 50/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9855 - loss: 0.0483\n",
      "Epoch 50: val_accuracy improved from 0.98417 to 0.98433, saving model to CNN_Models\\Base_CNN-lr_0.001-bs_64-20250917-092953\\best_model_epoch-50_val_acc-0.9843.keras\n",
      "\n",
      "Epoch 50: val_accuracy improved from 0.98417 to 0.98433, saving model to wandb_models/Base_CNN-lr_0.001-bs_64-20250917-092953/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.9852 - loss: 0.0498 - val_accuracy: 0.9843 - val_loss: 0.0557\n",
      "Epoch 51/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9856 - loss: 0.0476\n",
      "Epoch 51: val_accuracy did not improve from 0.98433\n",
      "\n",
      "Epoch 51: val_accuracy did not improve from 0.98433\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 14ms/step - accuracy: 0.9854 - loss: 0.0491 - val_accuracy: 0.9843 - val_loss: 0.0553\n",
      "Epoch 52/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9857 - loss: 0.0469\n",
      "Epoch 52: val_accuracy improved from 0.98433 to 0.98467, saving model to CNN_Models\\Base_CNN-lr_0.001-bs_64-20250917-092953\\best_model_epoch-52_val_acc-0.9847.keras\n",
      "\n",
      "Epoch 52: val_accuracy improved from 0.98433 to 0.98467, saving model to wandb_models/Base_CNN-lr_0.001-bs_64-20250917-092953/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 18ms/step - accuracy: 0.9856 - loss: 0.0484 - val_accuracy: 0.9847 - val_loss: 0.0549\n",
      "Epoch 53/100\n",
      "\u001b[1m840/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9861 - loss: 0.0463\n",
      "Epoch 53: val_accuracy improved from 0.98467 to 0.98517, saving model to CNN_Models\\Base_CNN-lr_0.001-bs_64-20250917-092953\\best_model_epoch-53_val_acc-0.9852.keras\n",
      "\n",
      "Epoch 53: val_accuracy improved from 0.98467 to 0.98517, saving model to wandb_models/Base_CNN-lr_0.001-bs_64-20250917-092953/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - accuracy: 0.9858 - loss: 0.0477 - val_accuracy: 0.9852 - val_loss: 0.0546\n",
      "Epoch 54/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9862 - loss: 0.0456\n",
      "Epoch 54: val_accuracy improved from 0.98517 to 0.98533, saving model to CNN_Models\\Base_CNN-lr_0.001-bs_64-20250917-092953\\best_model_epoch-54_val_acc-0.9853.keras\n",
      "\n",
      "Epoch 54: val_accuracy improved from 0.98517 to 0.98533, saving model to wandb_models/Base_CNN-lr_0.001-bs_64-20250917-092953/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 15ms/step - accuracy: 0.9859 - loss: 0.0471 - val_accuracy: 0.9853 - val_loss: 0.0543\n",
      "Epoch 55/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9865 - loss: 0.0450\n",
      "Epoch 55: val_accuracy did not improve from 0.98533\n",
      "\n",
      "Epoch 55: val_accuracy did not improve from 0.98533\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 14ms/step - accuracy: 0.9861 - loss: 0.0465 - val_accuracy: 0.9853 - val_loss: 0.0540\n",
      "Epoch 56/100\n",
      "\u001b[1m840/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9867 - loss: 0.0443\n",
      "Epoch 56: val_accuracy did not improve from 0.98533\n",
      "\n",
      "Epoch 56: val_accuracy did not improve from 0.98533\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 14ms/step - accuracy: 0.9864 - loss: 0.0459 - val_accuracy: 0.9853 - val_loss: 0.0536\n",
      "Epoch 57/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9868 - loss: 0.0438\n",
      "Epoch 57: val_accuracy improved from 0.98533 to 0.98583, saving model to CNN_Models\\Base_CNN-lr_0.001-bs_64-20250917-092953\\best_model_epoch-57_val_acc-0.9858.keras\n",
      "\n",
      "Epoch 57: val_accuracy improved from 0.98533 to 0.98583, saving model to wandb_models/Base_CNN-lr_0.001-bs_64-20250917-092953/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 15ms/step - accuracy: 0.9866 - loss: 0.0453 - val_accuracy: 0.9858 - val_loss: 0.0533\n",
      "Epoch 58/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9870 - loss: 0.0432\n",
      "Epoch 58: val_accuracy improved from 0.98583 to 0.98617, saving model to CNN_Models\\Base_CNN-lr_0.001-bs_64-20250917-092953\\best_model_epoch-58_val_acc-0.9862.keras\n",
      "\n",
      "Epoch 58: val_accuracy improved from 0.98583 to 0.98617, saving model to wandb_models/Base_CNN-lr_0.001-bs_64-20250917-092953/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.9867 - loss: 0.0447 - val_accuracy: 0.9862 - val_loss: 0.0531\n",
      "Epoch 59/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9872 - loss: 0.0426\n",
      "Epoch 59: val_accuracy did not improve from 0.98617\n",
      "\n",
      "Epoch 59: val_accuracy did not improve from 0.98617\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 15ms/step - accuracy: 0.9869 - loss: 0.0441 - val_accuracy: 0.9862 - val_loss: 0.0528\n",
      "Epoch 60/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9874 - loss: 0.0421\n",
      "Epoch 60: val_accuracy improved from 0.98617 to 0.98633, saving model to CNN_Models\\Base_CNN-lr_0.001-bs_64-20250917-092953\\best_model_epoch-60_val_acc-0.9863.keras\n",
      "\n",
      "Epoch 60: val_accuracy improved from 0.98617 to 0.98633, saving model to wandb_models/Base_CNN-lr_0.001-bs_64-20250917-092953/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 14ms/step - accuracy: 0.9870 - loss: 0.0436 - val_accuracy: 0.9863 - val_loss: 0.0526\n",
      "Epoch 61/100\n",
      "\u001b[1m840/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9876 - loss: 0.0415\n",
      "Epoch 61: val_accuracy improved from 0.98633 to 0.98667, saving model to CNN_Models\\Base_CNN-lr_0.001-bs_64-20250917-092953\\best_model_epoch-61_val_acc-0.9867.keras\n",
      "\n",
      "Epoch 61: val_accuracy improved from 0.98633 to 0.98667, saving model to wandb_models/Base_CNN-lr_0.001-bs_64-20250917-092953/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 14ms/step - accuracy: 0.9872 - loss: 0.0430 - val_accuracy: 0.9867 - val_loss: 0.0524\n",
      "Epoch 62/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9877 - loss: 0.0410\n",
      "Epoch 62: val_accuracy did not improve from 0.98667\n",
      "\n",
      "Epoch 62: val_accuracy did not improve from 0.98667\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 15ms/step - accuracy: 0.9873 - loss: 0.0425 - val_accuracy: 0.9865 - val_loss: 0.0521\n",
      "Epoch 63/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9879 - loss: 0.0405\n",
      "Epoch 63: val_accuracy improved from 0.98667 to 0.98683, saving model to CNN_Models\\Base_CNN-lr_0.001-bs_64-20250917-092953\\best_model_epoch-63_val_acc-0.9868.keras\n",
      "\n",
      "Epoch 63: val_accuracy improved from 0.98667 to 0.98683, saving model to wandb_models/Base_CNN-lr_0.001-bs_64-20250917-092953/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - accuracy: 0.9875 - loss: 0.0420 - val_accuracy: 0.9868 - val_loss: 0.0519\n",
      "Epoch 64/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9879 - loss: 0.0400\n",
      "Epoch 64: val_accuracy did not improve from 0.98683\n",
      "\n",
      "Epoch 64: val_accuracy did not improve from 0.98683\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - accuracy: 0.9876 - loss: 0.0415 - val_accuracy: 0.9867 - val_loss: 0.0517\n",
      "Epoch 65/100\n",
      "\u001b[1m840/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9882 - loss: 0.0395\n",
      "Epoch 65: val_accuracy did not improve from 0.98683\n",
      "\n",
      "Epoch 65: val_accuracy did not improve from 0.98683\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 12ms/step - accuracy: 0.9878 - loss: 0.0410 - val_accuracy: 0.9867 - val_loss: 0.0515\n",
      "Epoch 66/100\n",
      "\u001b[1m840/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9885 - loss: 0.0390\n",
      "Epoch 66: val_accuracy did not improve from 0.98683\n",
      "\n",
      "Epoch 66: val_accuracy did not improve from 0.98683\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - accuracy: 0.9880 - loss: 0.0405 - val_accuracy: 0.9868 - val_loss: 0.0512\n",
      "Epoch 67/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9887 - loss: 0.0386\n",
      "Epoch 67: val_accuracy did not improve from 0.98683\n",
      "\n",
      "Epoch 67: val_accuracy did not improve from 0.98683\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.9882 - loss: 0.0401 - val_accuracy: 0.9868 - val_loss: 0.0511\n",
      "Epoch 68/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9889 - loss: 0.0381\n",
      "Epoch 68: val_accuracy did not improve from 0.98683\n",
      "\n",
      "Epoch 68: val_accuracy did not improve from 0.98683\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 14ms/step - accuracy: 0.9884 - loss: 0.0396 - val_accuracy: 0.9868 - val_loss: 0.0509\n",
      "Epoch 69/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9891 - loss: 0.0377\n",
      "Epoch 69: val_accuracy did not improve from 0.98683\n",
      "\n",
      "Epoch 69: val_accuracy did not improve from 0.98683\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 14ms/step - accuracy: 0.9886 - loss: 0.0391 - val_accuracy: 0.9867 - val_loss: 0.0507\n",
      "Epoch 70/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9892 - loss: 0.0372\n",
      "Epoch 70: val_accuracy did not improve from 0.98683\n",
      "\n",
      "Epoch 70: val_accuracy did not improve from 0.98683\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 14ms/step - accuracy: 0.9887 - loss: 0.0387 - val_accuracy: 0.9867 - val_loss: 0.0505\n",
      "Epoch 71/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9893 - loss: 0.0368\n",
      "Epoch 71: val_accuracy did not improve from 0.98683\n",
      "\n",
      "Epoch 71: val_accuracy did not improve from 0.98683\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 14ms/step - accuracy: 0.9888 - loss: 0.0383 - val_accuracy: 0.9868 - val_loss: 0.0504\n",
      "Epoch 72/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9894 - loss: 0.0364\n",
      "Epoch 72: val_accuracy did not improve from 0.98683\n",
      "\n",
      "Epoch 72: val_accuracy did not improve from 0.98683\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 14ms/step - accuracy: 0.9890 - loss: 0.0378 - val_accuracy: 0.9868 - val_loss: 0.0503\n",
      "Epoch 73/100\n",
      "\u001b[1m840/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9894 - loss: 0.0360\n",
      "Epoch 73: val_accuracy did not improve from 0.98683\n",
      "\n",
      "Epoch 73: val_accuracy did not improve from 0.98683\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 17ms/step - accuracy: 0.9891 - loss: 0.0374 - val_accuracy: 0.9868 - val_loss: 0.0501\n",
      "\n",
      "Training history saved to: CNN_Models\\Base_CNN-lr_0.001-bs_64-20250917-092953\\training_history.pkl\n",
      "\n",
      "--- Peak Performance Summary ---\n",
      "Best validation accuracy:           0.9868\n",
      "Associated training accuracy:       0.9875\n",
      "Occurred at epoch:                  63\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch/accuracy</td><td>▁▃▅▅▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇█████████████████████</td></tr><tr><td>epoch/epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>epoch/learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/loss</td><td>█▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/val_accuracy</td><td>▁▆▇▇▇▇▇▇▇███████████████████████████████</td></tr><tr><td>epoch/val_loss</td><td>█▄▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch/accuracy</td><td>0.98909</td></tr><tr><td>epoch/epoch</td><td>72</td></tr><tr><td>epoch/learning_rate</td><td>0.001</td></tr><tr><td>epoch/loss</td><td>0.03741</td></tr><tr><td>epoch/val_accuracy</td><td>0.98683</td></tr><tr><td>epoch/val_loss</td><td>0.05005</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Base_CNN-lr_0.001-bs_64-20250917-092953</strong> at: <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/aat0yd33' target=\"_blank\">https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/aat0yd33</a><br> View project at: <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2' target=\"_blank\">https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2</a><br>Synced 5 W&B file(s), 0 media file(s), 92 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250917_092953-aat0yd33\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Adapting the normalisation layer...\n",
      "Adaptation complete.\n",
      "\n",
      "\n",
      "--- Starting Experiment: Base_CNN ---\n",
      "\n",
      "--- Model Architecture ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"Base_CNN\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"Base_CNN\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ normalization_23                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Normalization</span>)                 │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1600</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_102 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">204,928</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_103 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,290</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ normalization_23                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m1\u001b[0m)      │             \u001b[38;5;34m3\u001b[0m │\n",
       "│ (\u001b[38;5;33mNormalization\u001b[0m)                 │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_6 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m320\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_6 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_7 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_7 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_23 (\u001b[38;5;33mFlatten\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1600\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_102 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m204,928\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_103 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │         \u001b[38;5;34m1,290\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">225,037</span> (879.05 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m225,037\u001b[0m (879.05 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">225,034</span> (879.04 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m225,034\u001b[0m (879.04 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3</span> (16.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m3\u001b[0m (16.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Hyperparameters ---\n",
      "optimiser           : SGD\n",
      "learning_rate       : 0.0001\n",
      "epochs              : 100\n",
      "batch_size          : 64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "creating run (0.0s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\TimVos\\VSC Projects\\CSE5ML\\Assessment 2\\wandb\\run-20250917_094432-ii0v01bm</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/ii0v01bm' target=\"_blank\">Base_CNN-lr_0.0001-bs_64-20250917-094432</a></strong> to <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2' target=\"_blank\">https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/ii0v01bm' target=\"_blank\">https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/ii0v01bm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.1361 - loss: 2.2595\n",
      "Epoch 1: val_accuracy improved from None to 0.34667, saving model to CNN_Models\\Base_CNN-lr_0.0001-bs_64-20250917-094432\\best_model_epoch-01_val_acc-0.3467.keras\n",
      "\n",
      "Epoch 1: val_accuracy improved from None to 0.34667, saving model to wandb_models/Base_CNN-lr_0.0001-bs_64-20250917-094432/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 15ms/step - accuracy: 0.1994 - loss: 2.2192 - val_accuracy: 0.3467 - val_loss: 2.1368\n",
      "Epoch 2/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.3948 - loss: 2.1101\n",
      "Epoch 2: val_accuracy improved from 0.34667 to 0.56367, saving model to CNN_Models\\Base_CNN-lr_0.0001-bs_64-20250917-094432\\best_model_epoch-02_val_acc-0.5637.keras\n",
      "\n",
      "Epoch 2: val_accuracy improved from 0.34667 to 0.56367, saving model to wandb_models/Base_CNN-lr_0.0001-bs_64-20250917-094432/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 14ms/step - accuracy: 0.4475 - loss: 2.0734 - val_accuracy: 0.5637 - val_loss: 1.9837\n",
      "Epoch 3/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.5675 - loss: 1.9554\n",
      "Epoch 3: val_accuracy improved from 0.56367 to 0.67917, saving model to CNN_Models\\Base_CNN-lr_0.0001-bs_64-20250917-094432\\best_model_epoch-03_val_acc-0.6792.keras\n",
      "\n",
      "Epoch 3: val_accuracy improved from 0.56367 to 0.67917, saving model to wandb_models/Base_CNN-lr_0.0001-bs_64-20250917-094432/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 14ms/step - accuracy: 0.5950 - loss: 1.9100 - val_accuracy: 0.6792 - val_loss: 1.7942\n",
      "Epoch 4/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6621 - loss: 1.7628\n",
      "Epoch 4: val_accuracy improved from 0.67917 to 0.73617, saving model to CNN_Models\\Base_CNN-lr_0.0001-bs_64-20250917-094432\\best_model_epoch-04_val_acc-0.7362.keras\n",
      "\n",
      "Epoch 4: val_accuracy improved from 0.67917 to 0.73617, saving model to wandb_models/Base_CNN-lr_0.0001-bs_64-20250917-094432/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - accuracy: 0.6766 - loss: 1.7073 - val_accuracy: 0.7362 - val_loss: 1.5618\n",
      "Epoch 5/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7144 - loss: 1.5316\n",
      "Epoch 5: val_accuracy improved from 0.73617 to 0.77183, saving model to CNN_Models\\Base_CNN-lr_0.0001-bs_64-20250917-094432\\best_model_epoch-05_val_acc-0.7718.keras\n",
      "\n",
      "Epoch 5: val_accuracy improved from 0.73617 to 0.77183, saving model to wandb_models/Base_CNN-lr_0.0001-bs_64-20250917-094432/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - accuracy: 0.7223 - loss: 1.4690 - val_accuracy: 0.7718 - val_loss: 1.3019\n",
      "Epoch 6/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7469 - loss: 1.2836\n",
      "Epoch 6: val_accuracy improved from 0.77183 to 0.79900, saving model to CNN_Models\\Base_CNN-lr_0.0001-bs_64-20250917-094432\\best_model_epoch-06_val_acc-0.7990.keras\n",
      "\n",
      "Epoch 6: val_accuracy improved from 0.77183 to 0.79900, saving model to wandb_models/Base_CNN-lr_0.0001-bs_64-20250917-094432/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 14ms/step - accuracy: 0.7532 - loss: 1.2241 - val_accuracy: 0.7990 - val_loss: 1.0566\n",
      "Epoch 7/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7727 - loss: 1.0594\n",
      "Epoch 7: val_accuracy improved from 0.79900 to 0.82583, saving model to CNN_Models\\Base_CNN-lr_0.0001-bs_64-20250917-094432\\best_model_epoch-07_val_acc-0.8258.keras\n",
      "\n",
      "Epoch 7: val_accuracy improved from 0.79900 to 0.82583, saving model to wandb_models/Base_CNN-lr_0.0001-bs_64-20250917-094432/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - accuracy: 0.7790 - loss: 1.0116 - val_accuracy: 0.8258 - val_loss: 0.8603\n",
      "Epoch 8/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7964 - loss: 0.8852\n",
      "Epoch 8: val_accuracy improved from 0.82583 to 0.85117, saving model to CNN_Models\\Base_CNN-lr_0.0001-bs_64-20250917-094432\\best_model_epoch-08_val_acc-0.8512.keras\n",
      "\n",
      "Epoch 8: val_accuracy improved from 0.82583 to 0.85117, saving model to wandb_models/Base_CNN-lr_0.0001-bs_64-20250917-094432/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - accuracy: 0.8018 - loss: 0.8501 - val_accuracy: 0.8512 - val_loss: 0.7176\n",
      "Epoch 9/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8161 - loss: 0.7585\n",
      "Epoch 9: val_accuracy improved from 0.85117 to 0.86650, saving model to CNN_Models\\Base_CNN-lr_0.0001-bs_64-20250917-094432\\best_model_epoch-09_val_acc-0.8665.keras\n",
      "\n",
      "Epoch 9: val_accuracy improved from 0.85117 to 0.86650, saving model to wandb_models/Base_CNN-lr_0.0001-bs_64-20250917-094432/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 14ms/step - accuracy: 0.8219 - loss: 0.7331 - val_accuracy: 0.8665 - val_loss: 0.6151\n",
      "Epoch 10/100\n",
      "\u001b[1m840/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8336 - loss: 0.6667\n",
      "Epoch 10: val_accuracy improved from 0.86650 to 0.87833, saving model to CNN_Models\\Base_CNN-lr_0.0001-bs_64-20250917-094432\\best_model_epoch-10_val_acc-0.8783.keras\n",
      "\n",
      "Epoch 10: val_accuracy improved from 0.86650 to 0.87833, saving model to wandb_models/Base_CNN-lr_0.0001-bs_64-20250917-094432/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 14ms/step - accuracy: 0.8385 - loss: 0.6478 - val_accuracy: 0.8783 - val_loss: 0.5405\n",
      "Epoch 11/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8473 - loss: 0.5985\n",
      "Epoch 11: val_accuracy improved from 0.87833 to 0.88883, saving model to CNN_Models\\Base_CNN-lr_0.0001-bs_64-20250917-094432\\best_model_epoch-11_val_acc-0.8888.keras\n",
      "\n",
      "Epoch 11: val_accuracy improved from 0.87833 to 0.88883, saving model to wandb_models/Base_CNN-lr_0.0001-bs_64-20250917-094432/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 16ms/step - accuracy: 0.8516 - loss: 0.5843 - val_accuracy: 0.8888 - val_loss: 0.4850\n",
      "Epoch 12/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8578 - loss: 0.5467\n",
      "Epoch 12: val_accuracy improved from 0.88883 to 0.89650, saving model to CNN_Models\\Base_CNN-lr_0.0001-bs_64-20250917-094432\\best_model_epoch-12_val_acc-0.8965.keras\n",
      "\n",
      "Epoch 12: val_accuracy improved from 0.88883 to 0.89650, saving model to wandb_models/Base_CNN-lr_0.0001-bs_64-20250917-094432/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 16ms/step - accuracy: 0.8618 - loss: 0.5358 - val_accuracy: 0.8965 - val_loss: 0.4426\n",
      "Epoch 13/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8668 - loss: 0.5065\n",
      "Epoch 13: val_accuracy improved from 0.89650 to 0.90017, saving model to CNN_Models\\Base_CNN-lr_0.0001-bs_64-20250917-094432\\best_model_epoch-13_val_acc-0.9002.keras\n",
      "\n",
      "Epoch 13: val_accuracy improved from 0.89650 to 0.90017, saving model to wandb_models/Base_CNN-lr_0.0001-bs_64-20250917-094432/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 18ms/step - accuracy: 0.8702 - loss: 0.4980 - val_accuracy: 0.9002 - val_loss: 0.4095\n",
      "Epoch 14/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.8740 - loss: 0.4745\n",
      "Epoch 14: val_accuracy improved from 0.90017 to 0.90417, saving model to CNN_Models\\Base_CNN-lr_0.0001-bs_64-20250917-094432\\best_model_epoch-14_val_acc-0.9042.keras\n",
      "\n",
      "Epoch 14: val_accuracy improved from 0.90017 to 0.90417, saving model to wandb_models/Base_CNN-lr_0.0001-bs_64-20250917-094432/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 26ms/step - accuracy: 0.8766 - loss: 0.4677 - val_accuracy: 0.9042 - val_loss: 0.3829\n",
      "Epoch 15/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8804 - loss: 0.4484\n",
      "Epoch 15: val_accuracy improved from 0.90417 to 0.90750, saving model to CNN_Models\\Base_CNN-lr_0.0001-bs_64-20250917-094432\\best_model_epoch-15_val_acc-0.9075.keras\n",
      "\n",
      "Epoch 15: val_accuracy improved from 0.90417 to 0.90750, saving model to wandb_models/Base_CNN-lr_0.0001-bs_64-20250917-094432/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 21ms/step - accuracy: 0.8827 - loss: 0.4430 - val_accuracy: 0.9075 - val_loss: 0.3613\n",
      "Epoch 16/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8853 - loss: 0.4268\n",
      "Epoch 16: val_accuracy improved from 0.90750 to 0.91067, saving model to CNN_Models\\Base_CNN-lr_0.0001-bs_64-20250917-094432\\best_model_epoch-16_val_acc-0.9107.keras\n",
      "\n",
      "Epoch 16: val_accuracy improved from 0.90750 to 0.91067, saving model to wandb_models/Base_CNN-lr_0.0001-bs_64-20250917-094432/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 19ms/step - accuracy: 0.8873 - loss: 0.4223 - val_accuracy: 0.9107 - val_loss: 0.3432\n",
      "Epoch 17/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8897 - loss: 0.4085\n",
      "Epoch 17: val_accuracy improved from 0.91067 to 0.91333, saving model to CNN_Models\\Base_CNN-lr_0.0001-bs_64-20250917-094432\\best_model_epoch-17_val_acc-0.9133.keras\n",
      "\n",
      "Epoch 17: val_accuracy improved from 0.91067 to 0.91333, saving model to wandb_models/Base_CNN-lr_0.0001-bs_64-20250917-094432/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 20ms/step - accuracy: 0.8909 - loss: 0.4048 - val_accuracy: 0.9133 - val_loss: 0.3279\n",
      "Epoch 18/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8928 - loss: 0.3927\n",
      "Epoch 18: val_accuracy improved from 0.91333 to 0.91550, saving model to CNN_Models\\Base_CNN-lr_0.0001-bs_64-20250917-094432\\best_model_epoch-18_val_acc-0.9155.keras\n",
      "\n",
      "Epoch 18: val_accuracy improved from 0.91333 to 0.91550, saving model to wandb_models/Base_CNN-lr_0.0001-bs_64-20250917-094432/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 16ms/step - accuracy: 0.8938 - loss: 0.3898 - val_accuracy: 0.9155 - val_loss: 0.3148\n",
      "Epoch 19/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8957 - loss: 0.3790\n",
      "Epoch 19: val_accuracy improved from 0.91550 to 0.91883, saving model to CNN_Models\\Base_CNN-lr_0.0001-bs_64-20250917-094432\\best_model_epoch-19_val_acc-0.9188.keras\n",
      "\n",
      "Epoch 19: val_accuracy improved from 0.91550 to 0.91883, saving model to wandb_models/Base_CNN-lr_0.0001-bs_64-20250917-094432/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 15ms/step - accuracy: 0.8964 - loss: 0.3766 - val_accuracy: 0.9188 - val_loss: 0.3033\n",
      "Epoch 20/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8987 - loss: 0.3669\n",
      "Epoch 20: val_accuracy improved from 0.91883 to 0.92117, saving model to CNN_Models\\Base_CNN-lr_0.0001-bs_64-20250917-094432\\best_model_epoch-20_val_acc-0.9212.keras\n",
      "\n",
      "Epoch 20: val_accuracy improved from 0.91883 to 0.92117, saving model to wandb_models/Base_CNN-lr_0.0001-bs_64-20250917-094432/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 14ms/step - accuracy: 0.8991 - loss: 0.3649 - val_accuracy: 0.9212 - val_loss: 0.2932\n",
      "Epoch 21/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9004 - loss: 0.3560\n",
      "Epoch 21: val_accuracy improved from 0.92117 to 0.92367, saving model to CNN_Models\\Base_CNN-lr_0.0001-bs_64-20250917-094432\\best_model_epoch-21_val_acc-0.9237.keras\n",
      "\n",
      "Epoch 21: val_accuracy improved from 0.92117 to 0.92367, saving model to wandb_models/Base_CNN-lr_0.0001-bs_64-20250917-094432/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 15ms/step - accuracy: 0.9014 - loss: 0.3545 - val_accuracy: 0.9237 - val_loss: 0.2842\n",
      "Epoch 22/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9032 - loss: 0.3463\n",
      "Epoch 22: val_accuracy improved from 0.92367 to 0.92650, saving model to CNN_Models\\Base_CNN-lr_0.0001-bs_64-20250917-094432\\best_model_epoch-22_val_acc-0.9265.keras\n",
      "\n",
      "Epoch 22: val_accuracy improved from 0.92367 to 0.92650, saving model to wandb_models/Base_CNN-lr_0.0001-bs_64-20250917-094432/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 15ms/step - accuracy: 0.9035 - loss: 0.3450 - val_accuracy: 0.9265 - val_loss: 0.2762\n",
      "Epoch 23/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9051 - loss: 0.3374\n",
      "Epoch 23: val_accuracy improved from 0.92650 to 0.92783, saving model to CNN_Models\\Base_CNN-lr_0.0001-bs_64-20250917-094432\\best_model_epoch-23_val_acc-0.9278.keras\n",
      "\n",
      "Epoch 23: val_accuracy improved from 0.92650 to 0.92783, saving model to wandb_models/Base_CNN-lr_0.0001-bs_64-20250917-094432/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 15ms/step - accuracy: 0.9054 - loss: 0.3364 - val_accuracy: 0.9278 - val_loss: 0.2689\n",
      "Epoch 24/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9072 - loss: 0.3292\n",
      "Epoch 24: val_accuracy improved from 0.92783 to 0.92983, saving model to CNN_Models\\Base_CNN-lr_0.0001-bs_64-20250917-094432\\best_model_epoch-24_val_acc-0.9298.keras\n",
      "\n",
      "Epoch 24: val_accuracy improved from 0.92783 to 0.92983, saving model to wandb_models/Base_CNN-lr_0.0001-bs_64-20250917-094432/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.9074 - loss: 0.3285 - val_accuracy: 0.9298 - val_loss: 0.2622\n",
      "Epoch 25/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9090 - loss: 0.3217\n",
      "Epoch 25: val_accuracy improved from 0.92983 to 0.93167, saving model to CNN_Models\\Base_CNN-lr_0.0001-bs_64-20250917-094432\\best_model_epoch-25_val_acc-0.9317.keras\n",
      "\n",
      "Epoch 25: val_accuracy improved from 0.92983 to 0.93167, saving model to wandb_models/Base_CNN-lr_0.0001-bs_64-20250917-094432/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 17ms/step - accuracy: 0.9093 - loss: 0.3212 - val_accuracy: 0.9317 - val_loss: 0.2560\n",
      "Epoch 26/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9112 - loss: 0.3147\n",
      "Epoch 26: val_accuracy improved from 0.93167 to 0.93300, saving model to CNN_Models\\Base_CNN-lr_0.0001-bs_64-20250917-094432\\best_model_epoch-26_val_acc-0.9330.keras\n",
      "\n",
      "Epoch 26: val_accuracy improved from 0.93167 to 0.93300, saving model to wandb_models/Base_CNN-lr_0.0001-bs_64-20250917-094432/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 19ms/step - accuracy: 0.9114 - loss: 0.3145 - val_accuracy: 0.9330 - val_loss: 0.2504\n",
      "Epoch 27/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9128 - loss: 0.3082\n",
      "Epoch 27: val_accuracy improved from 0.93300 to 0.93400, saving model to CNN_Models\\Base_CNN-lr_0.0001-bs_64-20250917-094432\\best_model_epoch-27_val_acc-0.9340.keras\n",
      "\n",
      "Epoch 27: val_accuracy improved from 0.93300 to 0.93400, saving model to wandb_models/Base_CNN-lr_0.0001-bs_64-20250917-094432/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - accuracy: 0.9129 - loss: 0.3082 - val_accuracy: 0.9340 - val_loss: 0.2451\n",
      "Epoch 28/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9142 - loss: 0.3022\n",
      "Epoch 28: val_accuracy improved from 0.93400 to 0.93550, saving model to CNN_Models\\Base_CNN-lr_0.0001-bs_64-20250917-094432\\best_model_epoch-28_val_acc-0.9355.keras\n",
      "\n",
      "Epoch 28: val_accuracy improved from 0.93400 to 0.93550, saving model to wandb_models/Base_CNN-lr_0.0001-bs_64-20250917-094432/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - accuracy: 0.9144 - loss: 0.3023 - val_accuracy: 0.9355 - val_loss: 0.2402\n",
      "Epoch 29/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9159 - loss: 0.2965\n",
      "Epoch 29: val_accuracy improved from 0.93550 to 0.93667, saving model to CNN_Models\\Base_CNN-lr_0.0001-bs_64-20250917-094432\\best_model_epoch-29_val_acc-0.9367.keras\n",
      "\n",
      "Epoch 29: val_accuracy improved from 0.93550 to 0.93667, saving model to wandb_models/Base_CNN-lr_0.0001-bs_64-20250917-094432/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.9158 - loss: 0.2967 - val_accuracy: 0.9367 - val_loss: 0.2357\n",
      "Epoch 30/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9173 - loss: 0.2911\n",
      "Epoch 30: val_accuracy improved from 0.93667 to 0.93900, saving model to CNN_Models\\Base_CNN-lr_0.0001-bs_64-20250917-094432\\best_model_epoch-30_val_acc-0.9390.keras\n",
      "\n",
      "Epoch 30: val_accuracy improved from 0.93667 to 0.93900, saving model to wandb_models/Base_CNN-lr_0.0001-bs_64-20250917-094432/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 15ms/step - accuracy: 0.9171 - loss: 0.2915 - val_accuracy: 0.9390 - val_loss: 0.2314\n",
      "Epoch 31/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9184 - loss: 0.2860\n",
      "Epoch 31: val_accuracy improved from 0.93900 to 0.93967, saving model to CNN_Models\\Base_CNN-lr_0.0001-bs_64-20250917-094432\\best_model_epoch-31_val_acc-0.9397.keras\n",
      "\n",
      "Epoch 31: val_accuracy improved from 0.93900 to 0.93967, saving model to wandb_models/Base_CNN-lr_0.0001-bs_64-20250917-094432/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 14ms/step - accuracy: 0.9184 - loss: 0.2866 - val_accuracy: 0.9397 - val_loss: 0.2274\n",
      "Epoch 32/100\n",
      "\u001b[1m839/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9197 - loss: 0.2812\n",
      "Epoch 32: val_accuracy improved from 0.93967 to 0.94033, saving model to CNN_Models\\Base_CNN-lr_0.0001-bs_64-20250917-094432\\best_model_epoch-32_val_acc-0.9403.keras\n",
      "\n",
      "Epoch 32: val_accuracy improved from 0.93967 to 0.94033, saving model to wandb_models/Base_CNN-lr_0.0001-bs_64-20250917-094432/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - accuracy: 0.9196 - loss: 0.2819 - val_accuracy: 0.9403 - val_loss: 0.2236\n",
      "Epoch 33/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9211 - loss: 0.2766\n",
      "Epoch 33: val_accuracy improved from 0.94033 to 0.94183, saving model to CNN_Models\\Base_CNN-lr_0.0001-bs_64-20250917-094432\\best_model_epoch-33_val_acc-0.9418.keras\n",
      "\n",
      "Epoch 33: val_accuracy improved from 0.94033 to 0.94183, saving model to wandb_models/Base_CNN-lr_0.0001-bs_64-20250917-094432/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - accuracy: 0.9208 - loss: 0.2774 - val_accuracy: 0.9418 - val_loss: 0.2200\n",
      "Epoch 34/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9223 - loss: 0.2722\n",
      "Epoch 34: val_accuracy improved from 0.94183 to 0.94233, saving model to CNN_Models\\Base_CNN-lr_0.0001-bs_64-20250917-094432\\best_model_epoch-34_val_acc-0.9423.keras\n",
      "\n",
      "Epoch 34: val_accuracy improved from 0.94183 to 0.94233, saving model to wandb_models/Base_CNN-lr_0.0001-bs_64-20250917-094432/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - accuracy: 0.9219 - loss: 0.2731 - val_accuracy: 0.9423 - val_loss: 0.2165\n",
      "Epoch 35/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9234 - loss: 0.2681\n",
      "Epoch 35: val_accuracy improved from 0.94233 to 0.94350, saving model to CNN_Models\\Base_CNN-lr_0.0001-bs_64-20250917-094432\\best_model_epoch-35_val_acc-0.9435.keras\n",
      "\n",
      "Epoch 35: val_accuracy improved from 0.94233 to 0.94350, saving model to wandb_models/Base_CNN-lr_0.0001-bs_64-20250917-094432/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - accuracy: 0.9228 - loss: 0.2690 - val_accuracy: 0.9435 - val_loss: 0.2133\n",
      "Epoch 36/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9246 - loss: 0.2641\n",
      "Epoch 36: val_accuracy improved from 0.94350 to 0.94383, saving model to CNN_Models\\Base_CNN-lr_0.0001-bs_64-20250917-094432\\best_model_epoch-36_val_acc-0.9438.keras\n",
      "\n",
      "Epoch 36: val_accuracy improved from 0.94350 to 0.94383, saving model to wandb_models/Base_CNN-lr_0.0001-bs_64-20250917-094432/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 14ms/step - accuracy: 0.9242 - loss: 0.2651 - val_accuracy: 0.9438 - val_loss: 0.2101\n",
      "Epoch 37/100\n",
      "\u001b[1m840/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9256 - loss: 0.2602\n",
      "Epoch 37: val_accuracy improved from 0.94383 to 0.94467, saving model to CNN_Models\\Base_CNN-lr_0.0001-bs_64-20250917-094432\\best_model_epoch-37_val_acc-0.9447.keras\n",
      "\n",
      "Epoch 37: val_accuracy improved from 0.94383 to 0.94467, saving model to wandb_models/Base_CNN-lr_0.0001-bs_64-20250917-094432/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - accuracy: 0.9250 - loss: 0.2614 - val_accuracy: 0.9447 - val_loss: 0.2072\n",
      "Epoch 38/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9266 - loss: 0.2565\n",
      "Epoch 38: val_accuracy improved from 0.94467 to 0.94500, saving model to CNN_Models\\Base_CNN-lr_0.0001-bs_64-20250917-094432\\best_model_epoch-38_val_acc-0.9450.keras\n",
      "\n",
      "Epoch 38: val_accuracy improved from 0.94467 to 0.94500, saving model to wandb_models/Base_CNN-lr_0.0001-bs_64-20250917-094432/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - accuracy: 0.9260 - loss: 0.2577 - val_accuracy: 0.9450 - val_loss: 0.2043\n",
      "Epoch 39/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9276 - loss: 0.2530\n",
      "Epoch 39: val_accuracy improved from 0.94500 to 0.94533, saving model to CNN_Models\\Base_CNN-lr_0.0001-bs_64-20250917-094432\\best_model_epoch-39_val_acc-0.9453.keras\n",
      "\n",
      "Epoch 39: val_accuracy improved from 0.94500 to 0.94533, saving model to wandb_models/Base_CNN-lr_0.0001-bs_64-20250917-094432/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - accuracy: 0.9269 - loss: 0.2543 - val_accuracy: 0.9453 - val_loss: 0.2016\n",
      "Epoch 40/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9285 - loss: 0.2495\n",
      "Epoch 40: val_accuracy improved from 0.94533 to 0.94583, saving model to CNN_Models\\Base_CNN-lr_0.0001-bs_64-20250917-094432\\best_model_epoch-40_val_acc-0.9458.keras\n",
      "\n",
      "Epoch 40: val_accuracy improved from 0.94533 to 0.94583, saving model to wandb_models/Base_CNN-lr_0.0001-bs_64-20250917-094432/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - accuracy: 0.9278 - loss: 0.2509 - val_accuracy: 0.9458 - val_loss: 0.1990\n",
      "Epoch 41/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9294 - loss: 0.2462\n",
      "Epoch 41: val_accuracy improved from 0.94583 to 0.94600, saving model to CNN_Models\\Base_CNN-lr_0.0001-bs_64-20250917-094432\\best_model_epoch-41_val_acc-0.9460.keras\n",
      "\n",
      "Epoch 41: val_accuracy improved from 0.94583 to 0.94600, saving model to wandb_models/Base_CNN-lr_0.0001-bs_64-20250917-094432/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - accuracy: 0.9287 - loss: 0.2476 - val_accuracy: 0.9460 - val_loss: 0.1965\n",
      "Epoch 42/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9304 - loss: 0.2430\n",
      "Epoch 42: val_accuracy improved from 0.94600 to 0.94667, saving model to CNN_Models\\Base_CNN-lr_0.0001-bs_64-20250917-094432\\best_model_epoch-42_val_acc-0.9467.keras\n",
      "\n",
      "Epoch 42: val_accuracy improved from 0.94600 to 0.94667, saving model to wandb_models/Base_CNN-lr_0.0001-bs_64-20250917-094432/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - accuracy: 0.9296 - loss: 0.2445 - val_accuracy: 0.9467 - val_loss: 0.1940\n",
      "Epoch 43/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9314 - loss: 0.2399\n",
      "Epoch 43: val_accuracy improved from 0.94667 to 0.94700, saving model to CNN_Models\\Base_CNN-lr_0.0001-bs_64-20250917-094432\\best_model_epoch-43_val_acc-0.9470.keras\n",
      "\n",
      "Epoch 43: val_accuracy improved from 0.94667 to 0.94700, saving model to wandb_models/Base_CNN-lr_0.0001-bs_64-20250917-094432/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - accuracy: 0.9305 - loss: 0.2414 - val_accuracy: 0.9470 - val_loss: 0.1917\n",
      "Epoch 44/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9323 - loss: 0.2369\n",
      "Epoch 44: val_accuracy improved from 0.94700 to 0.94750, saving model to CNN_Models\\Base_CNN-lr_0.0001-bs_64-20250917-094432\\best_model_epoch-44_val_acc-0.9475.keras\n",
      "\n",
      "Epoch 44: val_accuracy improved from 0.94700 to 0.94750, saving model to wandb_models/Base_CNN-lr_0.0001-bs_64-20250917-094432/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - accuracy: 0.9312 - loss: 0.2384 - val_accuracy: 0.9475 - val_loss: 0.1894\n",
      "Epoch 45/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9327 - loss: 0.2340\n",
      "Epoch 45: val_accuracy improved from 0.94750 to 0.94800, saving model to CNN_Models\\Base_CNN-lr_0.0001-bs_64-20250917-094432\\best_model_epoch-45_val_acc-0.9480.keras\n",
      "\n",
      "Epoch 45: val_accuracy improved from 0.94750 to 0.94800, saving model to wandb_models/Base_CNN-lr_0.0001-bs_64-20250917-094432/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - accuracy: 0.9319 - loss: 0.2356 - val_accuracy: 0.9480 - val_loss: 0.1873\n",
      "Epoch 46/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9334 - loss: 0.2311\n",
      "Epoch 46: val_accuracy improved from 0.94800 to 0.94833, saving model to CNN_Models\\Base_CNN-lr_0.0001-bs_64-20250917-094432\\best_model_epoch-46_val_acc-0.9483.keras\n",
      "\n",
      "Epoch 46: val_accuracy improved from 0.94800 to 0.94833, saving model to wandb_models/Base_CNN-lr_0.0001-bs_64-20250917-094432/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - accuracy: 0.9325 - loss: 0.2328 - val_accuracy: 0.9483 - val_loss: 0.1851\n",
      "Epoch 47/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9341 - loss: 0.2284\n",
      "Epoch 47: val_accuracy improved from 0.94833 to 0.94883, saving model to CNN_Models\\Base_CNN-lr_0.0001-bs_64-20250917-094432\\best_model_epoch-47_val_acc-0.9488.keras\n",
      "\n",
      "Epoch 47: val_accuracy improved from 0.94833 to 0.94883, saving model to wandb_models/Base_CNN-lr_0.0001-bs_64-20250917-094432/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - accuracy: 0.9333 - loss: 0.2300 - val_accuracy: 0.9488 - val_loss: 0.1831\n",
      "Epoch 48/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9348 - loss: 0.2257\n",
      "Epoch 48: val_accuracy improved from 0.94883 to 0.94967, saving model to CNN_Models\\Base_CNN-lr_0.0001-bs_64-20250917-094432\\best_model_epoch-48_val_acc-0.9497.keras\n",
      "\n",
      "Epoch 48: val_accuracy improved from 0.94883 to 0.94967, saving model to wandb_models/Base_CNN-lr_0.0001-bs_64-20250917-094432/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - accuracy: 0.9339 - loss: 0.2274 - val_accuracy: 0.9497 - val_loss: 0.1811\n",
      "Epoch 49/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9352 - loss: 0.2230\n",
      "Epoch 49: val_accuracy did not improve from 0.94967\n",
      "\n",
      "Epoch 49: val_accuracy did not improve from 0.94967\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - accuracy: 0.9344 - loss: 0.2248 - val_accuracy: 0.9495 - val_loss: 0.1791\n",
      "Epoch 50/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9360 - loss: 0.2205\n",
      "Epoch 50: val_accuracy improved from 0.94967 to 0.95083, saving model to CNN_Models\\Base_CNN-lr_0.0001-bs_64-20250917-094432\\best_model_epoch-50_val_acc-0.9508.keras\n",
      "\n",
      "Epoch 50: val_accuracy improved from 0.94967 to 0.95083, saving model to wandb_models/Base_CNN-lr_0.0001-bs_64-20250917-094432/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - accuracy: 0.9351 - loss: 0.2223 - val_accuracy: 0.9508 - val_loss: 0.1772\n",
      "Epoch 51/100\n",
      "\u001b[1m840/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9362 - loss: 0.2180\n",
      "Epoch 51: val_accuracy improved from 0.95083 to 0.95150, saving model to CNN_Models\\Base_CNN-lr_0.0001-bs_64-20250917-094432\\best_model_epoch-51_val_acc-0.9515.keras\n",
      "\n",
      "Epoch 51: val_accuracy improved from 0.95083 to 0.95150, saving model to wandb_models/Base_CNN-lr_0.0001-bs_64-20250917-094432/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - accuracy: 0.9355 - loss: 0.2198 - val_accuracy: 0.9515 - val_loss: 0.1754\n",
      "Epoch 52/100\n",
      "\u001b[1m840/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9371 - loss: 0.2156\n",
      "Epoch 52: val_accuracy improved from 0.95150 to 0.95167, saving model to CNN_Models\\Base_CNN-lr_0.0001-bs_64-20250917-094432\\best_model_epoch-52_val_acc-0.9517.keras\n",
      "\n",
      "Epoch 52: val_accuracy improved from 0.95150 to 0.95167, saving model to wandb_models/Base_CNN-lr_0.0001-bs_64-20250917-094432/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - accuracy: 0.9363 - loss: 0.2174 - val_accuracy: 0.9517 - val_loss: 0.1736\n",
      "Epoch 53/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9382 - loss: 0.2133\n",
      "Epoch 53: val_accuracy improved from 0.95167 to 0.95217, saving model to CNN_Models\\Base_CNN-lr_0.0001-bs_64-20250917-094432\\best_model_epoch-53_val_acc-0.9522.keras\n",
      "\n",
      "Epoch 53: val_accuracy improved from 0.95167 to 0.95217, saving model to wandb_models/Base_CNN-lr_0.0001-bs_64-20250917-094432/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - accuracy: 0.9373 - loss: 0.2151 - val_accuracy: 0.9522 - val_loss: 0.1718\n",
      "Epoch 54/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9388 - loss: 0.2110\n",
      "Epoch 54: val_accuracy improved from 0.95217 to 0.95250, saving model to CNN_Models\\Base_CNN-lr_0.0001-bs_64-20250917-094432\\best_model_epoch-54_val_acc-0.9525.keras\n",
      "\n",
      "Epoch 54: val_accuracy improved from 0.95217 to 0.95250, saving model to wandb_models/Base_CNN-lr_0.0001-bs_64-20250917-094432/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - accuracy: 0.9379 - loss: 0.2128 - val_accuracy: 0.9525 - val_loss: 0.1701\n",
      "Epoch 55/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9396 - loss: 0.2087\n",
      "Epoch 55: val_accuracy improved from 0.95250 to 0.95333, saving model to CNN_Models\\Base_CNN-lr_0.0001-bs_64-20250917-094432\\best_model_epoch-55_val_acc-0.9533.keras\n",
      "\n",
      "Epoch 55: val_accuracy improved from 0.95250 to 0.95333, saving model to wandb_models/Base_CNN-lr_0.0001-bs_64-20250917-094432/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - accuracy: 0.9387 - loss: 0.2106 - val_accuracy: 0.9533 - val_loss: 0.1685\n",
      "Epoch 56/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9400 - loss: 0.2066\n",
      "Epoch 56: val_accuracy improved from 0.95333 to 0.95383, saving model to CNN_Models\\Base_CNN-lr_0.0001-bs_64-20250917-094432\\best_model_epoch-56_val_acc-0.9538.keras\n",
      "\n",
      "Epoch 56: val_accuracy improved from 0.95333 to 0.95383, saving model to wandb_models/Base_CNN-lr_0.0001-bs_64-20250917-094432/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - accuracy: 0.9392 - loss: 0.2084 - val_accuracy: 0.9538 - val_loss: 0.1669\n",
      "Epoch 57/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9407 - loss: 0.2044\n",
      "Epoch 57: val_accuracy improved from 0.95383 to 0.95450, saving model to CNN_Models\\Base_CNN-lr_0.0001-bs_64-20250917-094432\\best_model_epoch-57_val_acc-0.9545.keras\n",
      "\n",
      "Epoch 57: val_accuracy improved from 0.95383 to 0.95450, saving model to wandb_models/Base_CNN-lr_0.0001-bs_64-20250917-094432/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - accuracy: 0.9399 - loss: 0.2063 - val_accuracy: 0.9545 - val_loss: 0.1653\n",
      "Epoch 58/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9412 - loss: 0.2023\n",
      "Epoch 58: val_accuracy improved from 0.95450 to 0.95517, saving model to CNN_Models\\Base_CNN-lr_0.0001-bs_64-20250917-094432\\best_model_epoch-58_val_acc-0.9552.keras\n",
      "\n",
      "Epoch 58: val_accuracy improved from 0.95450 to 0.95517, saving model to wandb_models/Base_CNN-lr_0.0001-bs_64-20250917-094432/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - accuracy: 0.9404 - loss: 0.2042 - val_accuracy: 0.9552 - val_loss: 0.1637\n",
      "Epoch 59/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9418 - loss: 0.2003\n",
      "Epoch 59: val_accuracy improved from 0.95517 to 0.95600, saving model to CNN_Models\\Base_CNN-lr_0.0001-bs_64-20250917-094432\\best_model_epoch-59_val_acc-0.9560.keras\n",
      "\n",
      "Epoch 59: val_accuracy improved from 0.95517 to 0.95600, saving model to wandb_models/Base_CNN-lr_0.0001-bs_64-20250917-094432/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - accuracy: 0.9409 - loss: 0.2022 - val_accuracy: 0.9560 - val_loss: 0.1622\n",
      "Epoch 60/100\n",
      "\u001b[1m840/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9422 - loss: 0.1983\n",
      "Epoch 60: val_accuracy improved from 0.95600 to 0.95617, saving model to CNN_Models\\Base_CNN-lr_0.0001-bs_64-20250917-094432\\best_model_epoch-60_val_acc-0.9562.keras\n",
      "\n",
      "Epoch 60: val_accuracy improved from 0.95600 to 0.95617, saving model to wandb_models/Base_CNN-lr_0.0001-bs_64-20250917-094432/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - accuracy: 0.9415 - loss: 0.2002 - val_accuracy: 0.9562 - val_loss: 0.1608\n",
      "Epoch 61/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9431 - loss: 0.1964\n",
      "Epoch 61: val_accuracy improved from 0.95617 to 0.95633, saving model to CNN_Models\\Base_CNN-lr_0.0001-bs_64-20250917-094432\\best_model_epoch-61_val_acc-0.9563.keras\n",
      "\n",
      "Epoch 61: val_accuracy improved from 0.95617 to 0.95633, saving model to wandb_models/Base_CNN-lr_0.0001-bs_64-20250917-094432/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - accuracy: 0.9422 - loss: 0.1983 - val_accuracy: 0.9563 - val_loss: 0.1593\n",
      "Epoch 62/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9435 - loss: 0.1945\n",
      "Epoch 62: val_accuracy did not improve from 0.95633\n",
      "\n",
      "Epoch 62: val_accuracy did not improve from 0.95633\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 14ms/step - accuracy: 0.9428 - loss: 0.1964 - val_accuracy: 0.9563 - val_loss: 0.1579\n",
      "Epoch 63/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9441 - loss: 0.1926\n",
      "Epoch 63: val_accuracy improved from 0.95633 to 0.95717, saving model to CNN_Models\\Base_CNN-lr_0.0001-bs_64-20250917-094432\\best_model_epoch-63_val_acc-0.9572.keras\n",
      "\n",
      "Epoch 63: val_accuracy improved from 0.95633 to 0.95717, saving model to wandb_models/Base_CNN-lr_0.0001-bs_64-20250917-094432/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.9434 - loss: 0.1945 - val_accuracy: 0.9572 - val_loss: 0.1565\n",
      "Epoch 64/100\n",
      "\u001b[1m839/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9446 - loss: 0.1908\n",
      "Epoch 64: val_accuracy improved from 0.95717 to 0.95750, saving model to CNN_Models\\Base_CNN-lr_0.0001-bs_64-20250917-094432\\best_model_epoch-64_val_acc-0.9575.keras\n",
      "\n",
      "Epoch 64: val_accuracy improved from 0.95717 to 0.95750, saving model to wandb_models/Base_CNN-lr_0.0001-bs_64-20250917-094432/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - accuracy: 0.9439 - loss: 0.1927 - val_accuracy: 0.9575 - val_loss: 0.1552\n",
      "Epoch 65/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9454 - loss: 0.1890\n",
      "Epoch 65: val_accuracy improved from 0.95750 to 0.95817, saving model to CNN_Models\\Base_CNN-lr_0.0001-bs_64-20250917-094432\\best_model_epoch-65_val_acc-0.9582.keras\n",
      "\n",
      "Epoch 65: val_accuracy improved from 0.95750 to 0.95817, saving model to wandb_models/Base_CNN-lr_0.0001-bs_64-20250917-094432/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 14ms/step - accuracy: 0.9445 - loss: 0.1909 - val_accuracy: 0.9582 - val_loss: 0.1538\n",
      "Epoch 66/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9460 - loss: 0.1872\n",
      "Epoch 66: val_accuracy improved from 0.95817 to 0.95833, saving model to CNN_Models\\Base_CNN-lr_0.0001-bs_64-20250917-094432\\best_model_epoch-66_val_acc-0.9583.keras\n",
      "\n",
      "Epoch 66: val_accuracy improved from 0.95817 to 0.95833, saving model to wandb_models/Base_CNN-lr_0.0001-bs_64-20250917-094432/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - accuracy: 0.9451 - loss: 0.1892 - val_accuracy: 0.9583 - val_loss: 0.1525\n",
      "Epoch 67/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9466 - loss: 0.1855\n",
      "Epoch 67: val_accuracy improved from 0.95833 to 0.95883, saving model to CNN_Models\\Base_CNN-lr_0.0001-bs_64-20250917-094432\\best_model_epoch-67_val_acc-0.9588.keras\n",
      "\n",
      "Epoch 67: val_accuracy improved from 0.95833 to 0.95883, saving model to wandb_models/Base_CNN-lr_0.0001-bs_64-20250917-094432/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 14ms/step - accuracy: 0.9456 - loss: 0.1874 - val_accuracy: 0.9588 - val_loss: 0.1512\n",
      "Epoch 68/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9468 - loss: 0.1838\n",
      "Epoch 68: val_accuracy improved from 0.95883 to 0.95933, saving model to CNN_Models\\Base_CNN-lr_0.0001-bs_64-20250917-094432\\best_model_epoch-68_val_acc-0.9593.keras\n",
      "\n",
      "Epoch 68: val_accuracy improved from 0.95883 to 0.95933, saving model to wandb_models/Base_CNN-lr_0.0001-bs_64-20250917-094432/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - accuracy: 0.9461 - loss: 0.1857 - val_accuracy: 0.9593 - val_loss: 0.1500\n",
      "Epoch 69/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9474 - loss: 0.1822\n",
      "Epoch 69: val_accuracy improved from 0.95933 to 0.96017, saving model to CNN_Models\\Base_CNN-lr_0.0001-bs_64-20250917-094432\\best_model_epoch-69_val_acc-0.9602.keras\n",
      "\n",
      "Epoch 69: val_accuracy improved from 0.95933 to 0.96017, saving model to wandb_models/Base_CNN-lr_0.0001-bs_64-20250917-094432/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 14ms/step - accuracy: 0.9466 - loss: 0.1841 - val_accuracy: 0.9602 - val_loss: 0.1487\n",
      "Epoch 70/100\n",
      "\u001b[1m840/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9480 - loss: 0.1805\n",
      "Epoch 70: val_accuracy improved from 0.96017 to 0.96067, saving model to CNN_Models\\Base_CNN-lr_0.0001-bs_64-20250917-094432\\best_model_epoch-70_val_acc-0.9607.keras\n",
      "\n",
      "Epoch 70: val_accuracy improved from 0.96017 to 0.96067, saving model to wandb_models/Base_CNN-lr_0.0001-bs_64-20250917-094432/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - accuracy: 0.9471 - loss: 0.1824 - val_accuracy: 0.9607 - val_loss: 0.1475\n",
      "Epoch 71/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9485 - loss: 0.1789\n",
      "Epoch 71: val_accuracy improved from 0.96067 to 0.96117, saving model to CNN_Models\\Base_CNN-lr_0.0001-bs_64-20250917-094432\\best_model_epoch-71_val_acc-0.9612.keras\n",
      "\n",
      "Epoch 71: val_accuracy improved from 0.96067 to 0.96117, saving model to wandb_models/Base_CNN-lr_0.0001-bs_64-20250917-094432/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - accuracy: 0.9476 - loss: 0.1808 - val_accuracy: 0.9612 - val_loss: 0.1463\n",
      "Epoch 72/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9489 - loss: 0.1774\n",
      "Epoch 72: val_accuracy did not improve from 0.96117\n",
      "\n",
      "Epoch 72: val_accuracy did not improve from 0.96117\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - accuracy: 0.9481 - loss: 0.1793 - val_accuracy: 0.9610 - val_loss: 0.1451\n",
      "Epoch 73/100\n",
      "\u001b[1m840/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9494 - loss: 0.1758\n",
      "Epoch 73: val_accuracy did not improve from 0.96117\n",
      "\n",
      "Epoch 73: val_accuracy did not improve from 0.96117\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - accuracy: 0.9485 - loss: 0.1777 - val_accuracy: 0.9610 - val_loss: 0.1440\n",
      "Epoch 74/100\n",
      "\u001b[1m840/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9497 - loss: 0.1743\n",
      "Epoch 74: val_accuracy improved from 0.96117 to 0.96167, saving model to CNN_Models\\Base_CNN-lr_0.0001-bs_64-20250917-094432\\best_model_epoch-74_val_acc-0.9617.keras\n",
      "\n",
      "Epoch 74: val_accuracy improved from 0.96117 to 0.96167, saving model to wandb_models/Base_CNN-lr_0.0001-bs_64-20250917-094432/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - accuracy: 0.9488 - loss: 0.1762 - val_accuracy: 0.9617 - val_loss: 0.1429\n",
      "Epoch 75/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9500 - loss: 0.1728\n",
      "Epoch 75: val_accuracy improved from 0.96167 to 0.96267, saving model to CNN_Models\\Base_CNN-lr_0.0001-bs_64-20250917-094432\\best_model_epoch-75_val_acc-0.9627.keras\n",
      "\n",
      "Epoch 75: val_accuracy improved from 0.96167 to 0.96267, saving model to wandb_models/Base_CNN-lr_0.0001-bs_64-20250917-094432/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - accuracy: 0.9493 - loss: 0.1747 - val_accuracy: 0.9627 - val_loss: 0.1418\n",
      "Epoch 76/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9506 - loss: 0.1714\n",
      "Epoch 76: val_accuracy did not improve from 0.96267\n",
      "\n",
      "Epoch 76: val_accuracy did not improve from 0.96267\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - accuracy: 0.9499 - loss: 0.1732 - val_accuracy: 0.9627 - val_loss: 0.1407\n",
      "Epoch 77/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9511 - loss: 0.1699\n",
      "Epoch 77: val_accuracy improved from 0.96267 to 0.96317, saving model to CNN_Models\\Base_CNN-lr_0.0001-bs_64-20250917-094432\\best_model_epoch-77_val_acc-0.9632.keras\n",
      "\n",
      "Epoch 77: val_accuracy improved from 0.96267 to 0.96317, saving model to wandb_models/Base_CNN-lr_0.0001-bs_64-20250917-094432/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - accuracy: 0.9503 - loss: 0.1718 - val_accuracy: 0.9632 - val_loss: 0.1396\n",
      "Epoch 78/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9516 - loss: 0.1685\n",
      "Epoch 78: val_accuracy improved from 0.96317 to 0.96333, saving model to CNN_Models\\Base_CNN-lr_0.0001-bs_64-20250917-094432\\best_model_epoch-78_val_acc-0.9633.keras\n",
      "\n",
      "Epoch 78: val_accuracy improved from 0.96317 to 0.96333, saving model to wandb_models/Base_CNN-lr_0.0001-bs_64-20250917-094432/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 16ms/step - accuracy: 0.9507 - loss: 0.1704 - val_accuracy: 0.9633 - val_loss: 0.1386\n",
      "Epoch 79/100\n",
      "\u001b[1m839/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9521 - loss: 0.1671\n",
      "Epoch 79: val_accuracy improved from 0.96333 to 0.96383, saving model to CNN_Models\\Base_CNN-lr_0.0001-bs_64-20250917-094432\\best_model_epoch-79_val_acc-0.9638.keras\n",
      "\n",
      "Epoch 79: val_accuracy improved from 0.96333 to 0.96383, saving model to wandb_models/Base_CNN-lr_0.0001-bs_64-20250917-094432/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - accuracy: 0.9511 - loss: 0.1690 - val_accuracy: 0.9638 - val_loss: 0.1375\n",
      "Epoch 80/100\n",
      "\u001b[1m840/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9526 - loss: 0.1658\n",
      "Epoch 80: val_accuracy improved from 0.96383 to 0.96433, saving model to CNN_Models\\Base_CNN-lr_0.0001-bs_64-20250917-094432\\best_model_epoch-80_val_acc-0.9643.keras\n",
      "\n",
      "Epoch 80: val_accuracy improved from 0.96383 to 0.96433, saving model to wandb_models/Base_CNN-lr_0.0001-bs_64-20250917-094432/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 14ms/step - accuracy: 0.9515 - loss: 0.1676 - val_accuracy: 0.9643 - val_loss: 0.1365\n",
      "Epoch 81/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9533 - loss: 0.1644\n",
      "Epoch 81: val_accuracy improved from 0.96433 to 0.96450, saving model to CNN_Models\\Base_CNN-lr_0.0001-bs_64-20250917-094432\\best_model_epoch-81_val_acc-0.9645.keras\n",
      "\n",
      "Epoch 81: val_accuracy improved from 0.96433 to 0.96450, saving model to wandb_models/Base_CNN-lr_0.0001-bs_64-20250917-094432/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 15ms/step - accuracy: 0.9520 - loss: 0.1662 - val_accuracy: 0.9645 - val_loss: 0.1355\n",
      "Epoch 82/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9538 - loss: 0.1631\n",
      "Epoch 82: val_accuracy improved from 0.96450 to 0.96533, saving model to CNN_Models\\Base_CNN-lr_0.0001-bs_64-20250917-094432\\best_model_epoch-82_val_acc-0.9653.keras\n",
      "\n",
      "Epoch 82: val_accuracy improved from 0.96450 to 0.96533, saving model to wandb_models/Base_CNN-lr_0.0001-bs_64-20250917-094432/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.9524 - loss: 0.1649 - val_accuracy: 0.9653 - val_loss: 0.1345\n",
      "Epoch 83/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.9542 - loss: 0.1618\n",
      "Epoch 83: val_accuracy improved from 0.96533 to 0.96567, saving model to CNN_Models\\Base_CNN-lr_0.0001-bs_64-20250917-094432\\best_model_epoch-83_val_acc-0.9657.keras\n",
      "\n",
      "Epoch 83: val_accuracy improved from 0.96533 to 0.96567, saving model to wandb_models/Base_CNN-lr_0.0001-bs_64-20250917-094432/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 54ms/step - accuracy: 0.9528 - loss: 0.1636 - val_accuracy: 0.9657 - val_loss: 0.1335\n",
      "Epoch 84/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9543 - loss: 0.1606\n",
      "Epoch 84: val_accuracy did not improve from 0.96567\n",
      "\n",
      "Epoch 84: val_accuracy did not improve from 0.96567\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 40ms/step - accuracy: 0.9531 - loss: 0.1623 - val_accuracy: 0.9657 - val_loss: 0.1326\n",
      "Epoch 85/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9545 - loss: 0.1593\n",
      "Epoch 85: val_accuracy improved from 0.96567 to 0.96600, saving model to CNN_Models\\Base_CNN-lr_0.0001-bs_64-20250917-094432\\best_model_epoch-85_val_acc-0.9660.keras\n",
      "\n",
      "Epoch 85: val_accuracy improved from 0.96567 to 0.96600, saving model to wandb_models/Base_CNN-lr_0.0001-bs_64-20250917-094432/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 35ms/step - accuracy: 0.9534 - loss: 0.1610 - val_accuracy: 0.9660 - val_loss: 0.1316\n",
      "Epoch 86/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.9548 - loss: 0.1581\n",
      "Epoch 86: val_accuracy improved from 0.96600 to 0.96617, saving model to CNN_Models\\Base_CNN-lr_0.0001-bs_64-20250917-094432\\best_model_epoch-86_val_acc-0.9662.keras\n",
      "\n",
      "Epoch 86: val_accuracy improved from 0.96600 to 0.96617, saving model to wandb_models/Base_CNN-lr_0.0001-bs_64-20250917-094432/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 88ms/step - accuracy: 0.9537 - loss: 0.1598 - val_accuracy: 0.9662 - val_loss: 0.1307\n",
      "Epoch 87/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9552 - loss: 0.1569\n",
      "Epoch 87: val_accuracy improved from 0.96617 to 0.96633, saving model to CNN_Models\\Base_CNN-lr_0.0001-bs_64-20250917-094432\\best_model_epoch-87_val_acc-0.9663.keras\n",
      "\n",
      "Epoch 87: val_accuracy improved from 0.96617 to 0.96633, saving model to wandb_models/Base_CNN-lr_0.0001-bs_64-20250917-094432/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 69ms/step - accuracy: 0.9540 - loss: 0.1586 - val_accuracy: 0.9663 - val_loss: 0.1298\n",
      "Epoch 88/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9558 - loss: 0.1557\n",
      "Epoch 88: val_accuracy did not improve from 0.96633\n",
      "\n",
      "Epoch 88: val_accuracy did not improve from 0.96633\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 74ms/step - accuracy: 0.9544 - loss: 0.1574 - val_accuracy: 0.9663 - val_loss: 0.1289\n",
      "Epoch 89/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.9565 - loss: 0.1545\n",
      "Epoch 89: val_accuracy improved from 0.96633 to 0.96650, saving model to CNN_Models\\Base_CNN-lr_0.0001-bs_64-20250917-094432\\best_model_epoch-89_val_acc-0.9665.keras\n",
      "\n",
      "Epoch 89: val_accuracy improved from 0.96633 to 0.96650, saving model to wandb_models/Base_CNN-lr_0.0001-bs_64-20250917-094432/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 97ms/step - accuracy: 0.9549 - loss: 0.1562 - val_accuracy: 0.9665 - val_loss: 0.1280\n",
      "Epoch 90/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.9567 - loss: 0.1534\n",
      "Epoch 90: val_accuracy improved from 0.96650 to 0.96683, saving model to CNN_Models\\Base_CNN-lr_0.0001-bs_64-20250917-094432\\best_model_epoch-90_val_acc-0.9668.keras\n",
      "\n",
      "Epoch 90: val_accuracy improved from 0.96650 to 0.96683, saving model to wandb_models/Base_CNN-lr_0.0001-bs_64-20250917-094432/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 91ms/step - accuracy: 0.9551 - loss: 0.1550 - val_accuracy: 0.9668 - val_loss: 0.1272\n",
      "Epoch 91/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.9569 - loss: 0.1522\n",
      "Epoch 91: val_accuracy improved from 0.96683 to 0.96700, saving model to CNN_Models\\Base_CNN-lr_0.0001-bs_64-20250917-094432\\best_model_epoch-91_val_acc-0.9670.keras\n",
      "\n",
      "Epoch 91: val_accuracy improved from 0.96683 to 0.96700, saving model to wandb_models/Base_CNN-lr_0.0001-bs_64-20250917-094432/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 89ms/step - accuracy: 0.9554 - loss: 0.1538 - val_accuracy: 0.9670 - val_loss: 0.1263\n",
      "Epoch 92/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 0.9573 - loss: 0.1511\n",
      "Epoch 92: val_accuracy did not improve from 0.96700\n",
      "\n",
      "Epoch 92: val_accuracy did not improve from 0.96700\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 86ms/step - accuracy: 0.9559 - loss: 0.1527 - val_accuracy: 0.9668 - val_loss: 0.1255\n",
      "Epoch 93/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 0.9577 - loss: 0.1500\n",
      "Epoch 93: val_accuracy improved from 0.96700 to 0.96717, saving model to CNN_Models\\Base_CNN-lr_0.0001-bs_64-20250917-094432\\best_model_epoch-93_val_acc-0.9672.keras\n",
      "\n",
      "Epoch 93: val_accuracy improved from 0.96700 to 0.96717, saving model to wandb_models/Base_CNN-lr_0.0001-bs_64-20250917-094432/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 82ms/step - accuracy: 0.9562 - loss: 0.1516 - val_accuracy: 0.9672 - val_loss: 0.1247\n",
      "Epoch 94/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.9580 - loss: 0.1490\n",
      "Epoch 94: val_accuracy did not improve from 0.96717\n",
      "\n",
      "Epoch 94: val_accuracy did not improve from 0.96717\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 89ms/step - accuracy: 0.9565 - loss: 0.1505 - val_accuracy: 0.9672 - val_loss: 0.1239\n",
      "Epoch 95/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.9583 - loss: 0.1479\n",
      "Epoch 95: val_accuracy improved from 0.96717 to 0.96733, saving model to CNN_Models\\Base_CNN-lr_0.0001-bs_64-20250917-094432\\best_model_epoch-95_val_acc-0.9673.keras\n",
      "\n",
      "Epoch 95: val_accuracy improved from 0.96717 to 0.96733, saving model to wandb_models/Base_CNN-lr_0.0001-bs_64-20250917-094432/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 82ms/step - accuracy: 0.9569 - loss: 0.1494 - val_accuracy: 0.9673 - val_loss: 0.1231\n",
      "Epoch 96/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.9584 - loss: 0.1469\n",
      "Epoch 96: val_accuracy improved from 0.96733 to 0.96767, saving model to CNN_Models\\Base_CNN-lr_0.0001-bs_64-20250917-094432\\best_model_epoch-96_val_acc-0.9677.keras\n",
      "\n",
      "Epoch 96: val_accuracy improved from 0.96733 to 0.96767, saving model to wandb_models/Base_CNN-lr_0.0001-bs_64-20250917-094432/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 82ms/step - accuracy: 0.9571 - loss: 0.1483 - val_accuracy: 0.9677 - val_loss: 0.1223\n",
      "Epoch 97/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - accuracy: 0.9584 - loss: 0.1458\n",
      "Epoch 97: val_accuracy improved from 0.96767 to 0.96800, saving model to CNN_Models\\Base_CNN-lr_0.0001-bs_64-20250917-094432\\best_model_epoch-97_val_acc-0.9680.keras\n",
      "\n",
      "Epoch 97: val_accuracy improved from 0.96767 to 0.96800, saving model to wandb_models/Base_CNN-lr_0.0001-bs_64-20250917-094432/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 97ms/step - accuracy: 0.9573 - loss: 0.1473 - val_accuracy: 0.9680 - val_loss: 0.1215\n",
      "Epoch 98/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.9587 - loss: 0.1448\n",
      "Epoch 98: val_accuracy improved from 0.96800 to 0.96817, saving model to CNN_Models\\Base_CNN-lr_0.0001-bs_64-20250917-094432\\best_model_epoch-98_val_acc-0.9682.keras\n",
      "\n",
      "Epoch 98: val_accuracy improved from 0.96800 to 0.96817, saving model to wandb_models/Base_CNN-lr_0.0001-bs_64-20250917-094432/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 82ms/step - accuracy: 0.9575 - loss: 0.1463 - val_accuracy: 0.9682 - val_loss: 0.1207\n",
      "Epoch 99/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.9589 - loss: 0.1438\n",
      "Epoch 99: val_accuracy improved from 0.96817 to 0.96867, saving model to CNN_Models\\Base_CNN-lr_0.0001-bs_64-20250917-094432\\best_model_epoch-99_val_acc-0.9687.keras\n",
      "\n",
      "Epoch 99: val_accuracy improved from 0.96817 to 0.96867, saving model to wandb_models/Base_CNN-lr_0.0001-bs_64-20250917-094432/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 80ms/step - accuracy: 0.9577 - loss: 0.1452 - val_accuracy: 0.9687 - val_loss: 0.1200\n",
      "Epoch 100/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 0.9593 - loss: 0.1428\n",
      "Epoch 100: val_accuracy did not improve from 0.96867\n",
      "\n",
      "Epoch 100: val_accuracy did not improve from 0.96867\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 84ms/step - accuracy: 0.9581 - loss: 0.1442 - val_accuracy: 0.9687 - val_loss: 0.1193\n",
      "\n",
      "Training history saved to: CNN_Models\\Base_CNN-lr_0.0001-bs_64-20250917-094432\\training_history.pkl\n",
      "\n",
      "--- Peak Performance Summary ---\n",
      "Best validation accuracy:           0.9687\n",
      "Associated training accuracy:       0.9577\n",
      "Occurred at epoch:                  99\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch/accuracy</td><td>▁▅▆▇▇▇▇▇████████████████████████████████</td></tr><tr><td>epoch/epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>epoch/learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/loss</td><td>█▄▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/val_accuracy</td><td>▁▃▅▆▆▇▇▇▇▇██████████████████████████████</td></tr><tr><td>epoch/val_loss</td><td>█▅▅▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch/accuracy</td><td>0.95813</td></tr><tr><td>epoch/epoch</td><td>99</td></tr><tr><td>epoch/learning_rate</td><td>0.0001</td></tr><tr><td>epoch/loss</td><td>0.14424</td></tr><tr><td>epoch/val_accuracy</td><td>0.96867</td></tr><tr><td>epoch/val_loss</td><td>0.11927</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Base_CNN-lr_0.0001-bs_64-20250917-094432</strong> at: <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/ii0v01bm' target=\"_blank\">https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/ii0v01bm</a><br> View project at: <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2' target=\"_blank\">https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2</a><br>Synced 5 W&B file(s), 0 media file(s), 180 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250917_094432-ii0v01bm\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Adapting the normalisation layer...\n",
      "Adaptation complete.\n",
      "\n",
      "\n",
      "--- Starting Experiment: CNN_with_Dropout ---\n",
      "\n",
      "--- Model Architecture ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"CNN_with_Dropout\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"CNN_with_Dropout\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ normalization_24                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Normalization</span>)                 │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_24 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1600</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1600</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_104 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">204,928</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_105 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,290</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ normalization_24                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m1\u001b[0m)      │             \u001b[38;5;34m3\u001b[0m │\n",
       "│ (\u001b[38;5;33mNormalization\u001b[0m)                 │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_8 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m320\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_8 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_9 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_9 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_24 (\u001b[38;5;33mFlatten\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1600\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1600\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_104 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m204,928\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_105 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │         \u001b[38;5;34m1,290\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">225,037</span> (879.05 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m225,037\u001b[0m (879.05 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">225,034</span> (879.04 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m225,034\u001b[0m (879.04 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3</span> (16.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m3\u001b[0m (16.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Hyperparameters ---\n",
      "optimiser           : adamw\n",
      "learning_rate       : 0.001\n",
      "epochs              : 100\n",
      "batch_size          : 64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "creating run (0.2s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\TimVos\\VSC Projects\\CSE5ML\\Assessment 2\\wandb\\run-20250917_102047-kx62m6f3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/kx62m6f3' target=\"_blank\">CNN_with_Dropout-lr_0.001-bs_64-20250917-102046</a></strong> to <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2' target=\"_blank\">https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/kx62m6f3' target=\"_blank\">https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/kx62m6f3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.8721 - loss: 0.4004\n",
      "Epoch 1: val_accuracy improved from None to 0.98717, saving model to CNN_Models\\CNN_with_Dropout-lr_0.001-bs_64-20250917-102046\\best_model_epoch-01_val_acc-0.9872.keras\n",
      "\n",
      "Epoch 1: val_accuracy improved from None to 0.98717, saving model to wandb_models/CNN_with_Dropout-lr_0.001-bs_64-20250917-102046/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 90ms/step - accuracy: 0.9426 - loss: 0.1836 - val_accuracy: 0.9872 - val_loss: 0.0440\n",
      "Epoch 2/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.9770 - loss: 0.0746\n",
      "Epoch 2: val_accuracy improved from 0.98717 to 0.99050, saving model to CNN_Models\\CNN_with_Dropout-lr_0.001-bs_64-20250917-102046\\best_model_epoch-02_val_acc-0.9905.keras\n",
      "\n",
      "Epoch 2: val_accuracy improved from 0.98717 to 0.99050, saving model to wandb_models/CNN_with_Dropout-lr_0.001-bs_64-20250917-102046/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 90ms/step - accuracy: 0.9782 - loss: 0.0697 - val_accuracy: 0.9905 - val_loss: 0.0332\n",
      "Epoch 3/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - accuracy: 0.9837 - loss: 0.0520\n",
      "Epoch 3: val_accuracy improved from 0.99050 to 0.99133, saving model to CNN_Models\\CNN_with_Dropout-lr_0.001-bs_64-20250917-102046\\best_model_epoch-03_val_acc-0.9913.keras\n",
      "\n",
      "Epoch 3: val_accuracy improved from 0.99050 to 0.99133, saving model to wandb_models/CNN_with_Dropout-lr_0.001-bs_64-20250917-102046/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 129ms/step - accuracy: 0.9839 - loss: 0.0507 - val_accuracy: 0.9913 - val_loss: 0.0301\n",
      "Epoch 4/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - accuracy: 0.9870 - loss: 0.0394\n",
      "Epoch 4: val_accuracy improved from 0.99133 to 0.99317, saving model to CNN_Models\\CNN_with_Dropout-lr_0.001-bs_64-20250917-102046\\best_model_epoch-04_val_acc-0.9932.keras\n",
      "\n",
      "Epoch 4: val_accuracy improved from 0.99133 to 0.99317, saving model to wandb_models/CNN_with_Dropout-lr_0.001-bs_64-20250917-102046/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 110ms/step - accuracy: 0.9870 - loss: 0.0407 - val_accuracy: 0.9932 - val_loss: 0.0300\n",
      "Epoch 5/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 183ms/step - accuracy: 0.9885 - loss: 0.0360\n",
      "Epoch 5: val_accuracy did not improve from 0.99317\n",
      "\n",
      "Epoch 5: val_accuracy did not improve from 0.99317\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m217s\u001b[0m 199ms/step - accuracy: 0.9881 - loss: 0.0357 - val_accuracy: 0.9930 - val_loss: 0.0272\n",
      "Epoch 6/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.9899 - loss: 0.0325\n",
      "Epoch 6: val_accuracy improved from 0.99317 to 0.99350, saving model to CNN_Models\\CNN_with_Dropout-lr_0.001-bs_64-20250917-102046\\best_model_epoch-06_val_acc-0.9935.keras\n",
      "\n",
      "Epoch 6: val_accuracy improved from 0.99317 to 0.99350, saving model to wandb_models/CNN_with_Dropout-lr_0.001-bs_64-20250917-102046/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 145ms/step - accuracy: 0.9893 - loss: 0.0329 - val_accuracy: 0.9935 - val_loss: 0.0273\n",
      "Epoch 7/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - accuracy: 0.9909 - loss: 0.0271\n",
      "Epoch 7: val_accuracy did not improve from 0.99350\n",
      "\n",
      "Epoch 7: val_accuracy did not improve from 0.99350\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 146ms/step - accuracy: 0.9907 - loss: 0.0282 - val_accuracy: 0.9933 - val_loss: 0.0260\n",
      "Epoch 8/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - accuracy: 0.9918 - loss: 0.0257\n",
      "Epoch 8: val_accuracy did not improve from 0.99350\n",
      "\n",
      "Epoch 8: val_accuracy did not improve from 0.99350\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 158ms/step - accuracy: 0.9915 - loss: 0.0253 - val_accuracy: 0.9933 - val_loss: 0.0269\n",
      "Epoch 9/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - accuracy: 0.9924 - loss: 0.0228\n",
      "Epoch 9: val_accuracy did not improve from 0.99350\n",
      "\n",
      "Epoch 9: val_accuracy did not improve from 0.99350\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 110ms/step - accuracy: 0.9919 - loss: 0.0240 - val_accuracy: 0.9933 - val_loss: 0.0277\n",
      "Epoch 10/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.9931 - loss: 0.0209\n",
      "Epoch 10: val_accuracy improved from 0.99350 to 0.99400, saving model to CNN_Models\\CNN_with_Dropout-lr_0.001-bs_64-20250917-102046\\best_model_epoch-10_val_acc-0.9940.keras\n",
      "\n",
      "Epoch 10: val_accuracy improved from 0.99350 to 0.99400, saving model to wandb_models/CNN_with_Dropout-lr_0.001-bs_64-20250917-102046/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 115ms/step - accuracy: 0.9930 - loss: 0.0209 - val_accuracy: 0.9940 - val_loss: 0.0289\n",
      "Epoch 11/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.9936 - loss: 0.0187\n",
      "Epoch 11: val_accuracy improved from 0.99400 to 0.99417, saving model to CNN_Models\\CNN_with_Dropout-lr_0.001-bs_64-20250917-102046\\best_model_epoch-11_val_acc-0.9942.keras\n",
      "\n",
      "Epoch 11: val_accuracy improved from 0.99400 to 0.99417, saving model to wandb_models/CNN_with_Dropout-lr_0.001-bs_64-20250917-102046/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 117ms/step - accuracy: 0.9935 - loss: 0.0194 - val_accuracy: 0.9942 - val_loss: 0.0233\n",
      "Epoch 12/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 198ms/step - accuracy: 0.9936 - loss: 0.0176\n",
      "Epoch 12: val_accuracy did not improve from 0.99417\n",
      "\n",
      "Epoch 12: val_accuracy did not improve from 0.99417\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m182s\u001b[0m 215ms/step - accuracy: 0.9935 - loss: 0.0187 - val_accuracy: 0.9935 - val_loss: 0.0270\n",
      "Epoch 13/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 200ms/step - accuracy: 0.9948 - loss: 0.0165\n",
      "Epoch 13: val_accuracy did not improve from 0.99417\n",
      "\n",
      "Epoch 13: val_accuracy did not improve from 0.99417\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m206s\u001b[0m 218ms/step - accuracy: 0.9941 - loss: 0.0173 - val_accuracy: 0.9938 - val_loss: 0.0279\n",
      "Epoch 14/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9940 - loss: 0.0172\n",
      "Epoch 14: val_accuracy did not improve from 0.99417\n",
      "\n",
      "Epoch 14: val_accuracy did not improve from 0.99417\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 81ms/step - accuracy: 0.9939 - loss: 0.0183 - val_accuracy: 0.9942 - val_loss: 0.0264\n",
      "Epoch 15/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9958 - loss: 0.0124\n",
      "Epoch 15: val_accuracy did not improve from 0.99417\n",
      "\n",
      "Epoch 15: val_accuracy did not improve from 0.99417\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 71ms/step - accuracy: 0.9951 - loss: 0.0142 - val_accuracy: 0.9930 - val_loss: 0.0309\n",
      "Epoch 16/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9952 - loss: 0.0136\n",
      "Epoch 16: val_accuracy improved from 0.99417 to 0.99483, saving model to CNN_Models\\CNN_with_Dropout-lr_0.001-bs_64-20250917-102046\\best_model_epoch-16_val_acc-0.9948.keras\n",
      "\n",
      "Epoch 16: val_accuracy improved from 0.99417 to 0.99483, saving model to wandb_models/CNN_with_Dropout-lr_0.001-bs_64-20250917-102046/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 71ms/step - accuracy: 0.9947 - loss: 0.0149 - val_accuracy: 0.9948 - val_loss: 0.0281\n",
      "Epoch 17/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.9953 - loss: 0.0134\n",
      "Epoch 17: val_accuracy did not improve from 0.99483\n",
      "\n",
      "Epoch 17: val_accuracy did not improve from 0.99483\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 44ms/step - accuracy: 0.9947 - loss: 0.0143 - val_accuracy: 0.9938 - val_loss: 0.0288\n",
      "Epoch 18/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9946 - loss: 0.0162\n",
      "Epoch 18: val_accuracy did not improve from 0.99483\n",
      "\n",
      "Epoch 18: val_accuracy did not improve from 0.99483\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 20ms/step - accuracy: 0.9950 - loss: 0.0146 - val_accuracy: 0.9933 - val_loss: 0.0275\n",
      "Epoch 19/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9943 - loss: 0.0144\n",
      "Epoch 19: val_accuracy did not improve from 0.99483\n",
      "\n",
      "Epoch 19: val_accuracy did not improve from 0.99483\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 15ms/step - accuracy: 0.9949 - loss: 0.0136 - val_accuracy: 0.9933 - val_loss: 0.0287\n",
      "Epoch 20/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9958 - loss: 0.0120\n",
      "Epoch 20: val_accuracy did not improve from 0.99483\n",
      "\n",
      "Epoch 20: val_accuracy did not improve from 0.99483\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 16ms/step - accuracy: 0.9960 - loss: 0.0123 - val_accuracy: 0.9940 - val_loss: 0.0284\n",
      "Epoch 21/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9961 - loss: 0.0112\n",
      "Epoch 21: val_accuracy did not improve from 0.99483\n",
      "\n",
      "Epoch 21: val_accuracy did not improve from 0.99483\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 15ms/step - accuracy: 0.9959 - loss: 0.0123 - val_accuracy: 0.9940 - val_loss: 0.0305\n",
      "Epoch 22/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9958 - loss: 0.0122\n",
      "Epoch 22: val_accuracy did not improve from 0.99483\n",
      "\n",
      "Epoch 22: val_accuracy did not improve from 0.99483\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 14ms/step - accuracy: 0.9955 - loss: 0.0131 - val_accuracy: 0.9932 - val_loss: 0.0320\n",
      "Epoch 23/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9969 - loss: 0.0088\n",
      "Epoch 23: val_accuracy did not improve from 0.99483\n",
      "\n",
      "Epoch 23: val_accuracy did not improve from 0.99483\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.9964 - loss: 0.0106 - val_accuracy: 0.9945 - val_loss: 0.0280\n",
      "Epoch 24/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9970 - loss: 0.0090\n",
      "Epoch 24: val_accuracy improved from 0.99483 to 0.99567, saving model to CNN_Models\\CNN_with_Dropout-lr_0.001-bs_64-20250917-102046\\best_model_epoch-24_val_acc-0.9957.keras\n",
      "\n",
      "Epoch 24: val_accuracy improved from 0.99483 to 0.99567, saving model to wandb_models/CNN_with_Dropout-lr_0.001-bs_64-20250917-102046/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.9970 - loss: 0.0096 - val_accuracy: 0.9957 - val_loss: 0.0263\n",
      "Epoch 25/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9967 - loss: 0.0102\n",
      "Epoch 25: val_accuracy did not improve from 0.99567\n",
      "\n",
      "Epoch 25: val_accuracy did not improve from 0.99567\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.9964 - loss: 0.0108 - val_accuracy: 0.9940 - val_loss: 0.0272\n",
      "Epoch 26/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9966 - loss: 0.0100\n",
      "Epoch 26: val_accuracy did not improve from 0.99567\n",
      "\n",
      "Epoch 26: val_accuracy did not improve from 0.99567\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.9966 - loss: 0.0100 - val_accuracy: 0.9942 - val_loss: 0.0304\n",
      "Epoch 27/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9962 - loss: 0.0122\n",
      "Epoch 27: val_accuracy did not improve from 0.99567\n",
      "\n",
      "Epoch 27: val_accuracy did not improve from 0.99567\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.9966 - loss: 0.0105 - val_accuracy: 0.9945 - val_loss: 0.0321\n",
      "Epoch 28/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9955 - loss: 0.0128\n",
      "Epoch 28: val_accuracy did not improve from 0.99567\n",
      "\n",
      "Epoch 28: val_accuracy did not improve from 0.99567\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.9957 - loss: 0.0122 - val_accuracy: 0.9943 - val_loss: 0.0295\n",
      "Epoch 29/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9964 - loss: 0.0112\n",
      "Epoch 29: val_accuracy did not improve from 0.99567\n",
      "\n",
      "Epoch 29: val_accuracy did not improve from 0.99567\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.9969 - loss: 0.0097 - val_accuracy: 0.9945 - val_loss: 0.0275\n",
      "Epoch 30/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9974 - loss: 0.0082\n",
      "Epoch 30: val_accuracy did not improve from 0.99567\n",
      "\n",
      "Epoch 30: val_accuracy did not improve from 0.99567\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 15ms/step - accuracy: 0.9970 - loss: 0.0091 - val_accuracy: 0.9945 - val_loss: 0.0278\n",
      "Epoch 31/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9965 - loss: 0.0099\n",
      "Epoch 31: val_accuracy did not improve from 0.99567\n",
      "\n",
      "Epoch 31: val_accuracy did not improve from 0.99567\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.9967 - loss: 0.0106 - val_accuracy: 0.9955 - val_loss: 0.0250\n",
      "Epoch 32/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9969 - loss: 0.0088\n",
      "Epoch 32: val_accuracy did not improve from 0.99567\n",
      "\n",
      "Epoch 32: val_accuracy did not improve from 0.99567\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 15ms/step - accuracy: 0.9969 - loss: 0.0092 - val_accuracy: 0.9948 - val_loss: 0.0297\n",
      "Epoch 33/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9969 - loss: 0.0095\n",
      "Epoch 33: val_accuracy did not improve from 0.99567\n",
      "\n",
      "Epoch 33: val_accuracy did not improve from 0.99567\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.9968 - loss: 0.0099 - val_accuracy: 0.9938 - val_loss: 0.0290\n",
      "Epoch 34/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9970 - loss: 0.0090\n",
      "Epoch 34: val_accuracy did not improve from 0.99567\n",
      "\n",
      "Epoch 34: val_accuracy did not improve from 0.99567\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.9970 - loss: 0.0087 - val_accuracy: 0.9948 - val_loss: 0.0321\n",
      "\n",
      "Training history saved to: CNN_Models\\CNN_with_Dropout-lr_0.001-bs_64-20250917-102046\\training_history.pkl\n",
      "\n",
      "--- Peak Performance Summary ---\n",
      "Best validation accuracy:           0.9957\n",
      "Associated training accuracy:       0.9970\n",
      "Occurred at epoch:                  24\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch/accuracy</td><td>▁▆▆▇▇▇▇▇▇▇████████████████████████</td></tr><tr><td>epoch/epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>epoch/learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/loss</td><td>█▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/val_accuracy</td><td>▁▄▄▆▆▆▆▆▆▇▇▆▆▇▆▇▆▆▆▇▇▆▇█▇▇▇▇▇▇█▇▆▇</td></tr><tr><td>epoch/val_loss</td><td>█▄▃▃▂▂▂▂▂▃▁▂▃▂▄▃▃▂▃▃▃▄▃▂▂▃▄▃▂▃▂▃▃▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch/accuracy</td><td>0.99702</td></tr><tr><td>epoch/epoch</td><td>33</td></tr><tr><td>epoch/learning_rate</td><td>0.001</td></tr><tr><td>epoch/loss</td><td>0.00866</td></tr><tr><td>epoch/val_accuracy</td><td>0.99483</td></tr><tr><td>epoch/val_loss</td><td>0.03209</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">CNN_with_Dropout-lr_0.001-bs_64-20250917-102046</strong> at: <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/kx62m6f3' target=\"_blank\">https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/kx62m6f3</a><br> View project at: <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2' target=\"_blank\">https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2</a><br>Synced 5 W&B file(s), 0 media file(s), 18 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250917_102047-kx62m6f3\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Adapting the normalisation layer...\n",
      "Adaptation complete.\n",
      "\n",
      "\n",
      "--- Starting Experiment: CNN_with_Dropout ---\n",
      "\n",
      "--- Model Architecture ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"CNN_with_Dropout\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"CNN_with_Dropout\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ normalization_25                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Normalization</span>)                 │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_25 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1600</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1600</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_106 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">204,928</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_107 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,290</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ normalization_25                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m1\u001b[0m)      │             \u001b[38;5;34m3\u001b[0m │\n",
       "│ (\u001b[38;5;33mNormalization\u001b[0m)                 │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_10 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m320\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_10 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_11 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_11 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_25 (\u001b[38;5;33mFlatten\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1600\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1600\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_106 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m204,928\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_107 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │         \u001b[38;5;34m1,290\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">225,037</span> (879.05 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m225,037\u001b[0m (879.05 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">225,034</span> (879.04 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m225,034\u001b[0m (879.04 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3</span> (16.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m3\u001b[0m (16.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Hyperparameters ---\n",
      "optimiser           : adamw\n",
      "learning_rate       : 0.0001\n",
      "epochs              : 100\n",
      "batch_size          : 64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "creating run (0.0s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\TimVos\\VSC Projects\\CSE5ML\\Assessment 2\\wandb\\run-20250917_105632-xdbps14t</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/xdbps14t' target=\"_blank\">CNN_with_Dropout-lr_0.0001-bs_64-20250917-105632</a></strong> to <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2' target=\"_blank\">https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/xdbps14t' target=\"_blank\">https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/xdbps14t</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m840/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.6698 - loss: 1.0478\n",
      "Epoch 1: val_accuracy improved from None to 0.96700, saving model to CNN_Models\\CNN_with_Dropout-lr_0.0001-bs_64-20250917-105632\\best_model_epoch-01_val_acc-0.9670.keras\n",
      "\n",
      "Epoch 1: val_accuracy improved from None to 0.96700, saving model to wandb_models/CNN_with_Dropout-lr_0.0001-bs_64-20250917-105632/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 17ms/step - accuracy: 0.8311 - loss: 0.5588 - val_accuracy: 0.9670 - val_loss: 0.1158\n",
      "Epoch 2/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9447 - loss: 0.1849\n",
      "Epoch 2: val_accuracy improved from 0.96700 to 0.98217, saving model to CNN_Models\\CNN_with_Dropout-lr_0.0001-bs_64-20250917-105632\\best_model_epoch-02_val_acc-0.9822.keras\n",
      "\n",
      "Epoch 2: val_accuracy improved from 0.96700 to 0.98217, saving model to wandb_models/CNN_with_Dropout-lr_0.0001-bs_64-20250917-105632/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 18ms/step - accuracy: 0.9503 - loss: 0.1671 - val_accuracy: 0.9822 - val_loss: 0.0724\n",
      "Epoch 3/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9627 - loss: 0.1216\n",
      "Epoch 3: val_accuracy improved from 0.98217 to 0.98317, saving model to CNN_Models\\CNN_with_Dropout-lr_0.0001-bs_64-20250917-105632\\best_model_epoch-03_val_acc-0.9832.keras\n",
      "\n",
      "Epoch 3: val_accuracy improved from 0.98217 to 0.98317, saving model to wandb_models/CNN_with_Dropout-lr_0.0001-bs_64-20250917-105632/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 17ms/step - accuracy: 0.9637 - loss: 0.1168 - val_accuracy: 0.9832 - val_loss: 0.0574\n",
      "Epoch 4/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9704 - loss: 0.0983\n",
      "Epoch 4: val_accuracy improved from 0.98317 to 0.98550, saving model to CNN_Models\\CNN_with_Dropout-lr_0.0001-bs_64-20250917-105632\\best_model_epoch-04_val_acc-0.9855.keras\n",
      "\n",
      "Epoch 4: val_accuracy improved from 0.98317 to 0.98550, saving model to wandb_models/CNN_with_Dropout-lr_0.0001-bs_64-20250917-105632/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.9715 - loss: 0.0942 - val_accuracy: 0.9855 - val_loss: 0.0496\n",
      "Epoch 5/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9739 - loss: 0.0837\n",
      "Epoch 5: val_accuracy improved from 0.98550 to 0.98850, saving model to CNN_Models\\CNN_with_Dropout-lr_0.0001-bs_64-20250917-105632\\best_model_epoch-05_val_acc-0.9885.keras\n",
      "\n",
      "Epoch 5: val_accuracy improved from 0.98550 to 0.98850, saving model to wandb_models/CNN_with_Dropout-lr_0.0001-bs_64-20250917-105632/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 19ms/step - accuracy: 0.9747 - loss: 0.0816 - val_accuracy: 0.9885 - val_loss: 0.0448\n",
      "Epoch 6/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9771 - loss: 0.0722\n",
      "Epoch 6: val_accuracy improved from 0.98850 to 0.98950, saving model to CNN_Models\\CNN_with_Dropout-lr_0.0001-bs_64-20250917-105632\\best_model_epoch-06_val_acc-0.9895.keras\n",
      "\n",
      "Epoch 6: val_accuracy improved from 0.98850 to 0.98950, saving model to wandb_models/CNN_with_Dropout-lr_0.0001-bs_64-20250917-105632/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 21ms/step - accuracy: 0.9774 - loss: 0.0720 - val_accuracy: 0.9895 - val_loss: 0.0404\n",
      "Epoch 7/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9795 - loss: 0.0671\n",
      "Epoch 7: val_accuracy improved from 0.98950 to 0.99117, saving model to CNN_Models\\CNN_with_Dropout-lr_0.0001-bs_64-20250917-105632\\best_model_epoch-07_val_acc-0.9912.keras\n",
      "\n",
      "Epoch 7: val_accuracy improved from 0.98950 to 0.99117, saving model to wandb_models/CNN_with_Dropout-lr_0.0001-bs_64-20250917-105632/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 18ms/step - accuracy: 0.9799 - loss: 0.0658 - val_accuracy: 0.9912 - val_loss: 0.0365\n",
      "Epoch 8/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9820 - loss: 0.0604\n",
      "Epoch 8: val_accuracy did not improve from 0.99117\n",
      "\n",
      "Epoch 8: val_accuracy did not improve from 0.99117\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 17ms/step - accuracy: 0.9814 - loss: 0.0603 - val_accuracy: 0.9910 - val_loss: 0.0357\n",
      "Epoch 9/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9833 - loss: 0.0540\n",
      "Epoch 9: val_accuracy did not improve from 0.99117\n",
      "\n",
      "Epoch 9: val_accuracy did not improve from 0.99117\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 16ms/step - accuracy: 0.9828 - loss: 0.0547 - val_accuracy: 0.9912 - val_loss: 0.0338\n",
      "Epoch 10/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9843 - loss: 0.0502\n",
      "Epoch 10: val_accuracy did not improve from 0.99117\n",
      "\n",
      "Epoch 10: val_accuracy did not improve from 0.99117\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.9839 - loss: 0.0512 - val_accuracy: 0.9903 - val_loss: 0.0325\n",
      "Epoch 11/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9851 - loss: 0.0489\n",
      "Epoch 11: val_accuracy improved from 0.99117 to 0.99150, saving model to CNN_Models\\CNN_with_Dropout-lr_0.0001-bs_64-20250917-105632\\best_model_epoch-11_val_acc-0.9915.keras\n",
      "\n",
      "Epoch 11: val_accuracy improved from 0.99117 to 0.99150, saving model to wandb_models/CNN_with_Dropout-lr_0.0001-bs_64-20250917-105632/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 16ms/step - accuracy: 0.9847 - loss: 0.0493 - val_accuracy: 0.9915 - val_loss: 0.0319\n",
      "Epoch 12/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9863 - loss: 0.0447\n",
      "Epoch 12: val_accuracy improved from 0.99150 to 0.99200, saving model to CNN_Models\\CNN_with_Dropout-lr_0.0001-bs_64-20250917-105632\\best_model_epoch-12_val_acc-0.9920.keras\n",
      "\n",
      "Epoch 12: val_accuracy improved from 0.99150 to 0.99200, saving model to wandb_models/CNN_with_Dropout-lr_0.0001-bs_64-20250917-105632/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 17ms/step - accuracy: 0.9861 - loss: 0.0450 - val_accuracy: 0.9920 - val_loss: 0.0311\n",
      "Epoch 13/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9871 - loss: 0.0412\n",
      "Epoch 13: val_accuracy did not improve from 0.99200\n",
      "\n",
      "Epoch 13: val_accuracy did not improve from 0.99200\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 16ms/step - accuracy: 0.9860 - loss: 0.0430 - val_accuracy: 0.9915 - val_loss: 0.0296\n",
      "Epoch 14/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9871 - loss: 0.0405\n",
      "Epoch 14: val_accuracy did not improve from 0.99200\n",
      "\n",
      "Epoch 14: val_accuracy did not improve from 0.99200\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 34ms/step - accuracy: 0.9873 - loss: 0.0398 - val_accuracy: 0.9918 - val_loss: 0.0282\n",
      "Epoch 15/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9880 - loss: 0.0380\n",
      "Epoch 15: val_accuracy did not improve from 0.99200\n",
      "\n",
      "Epoch 15: val_accuracy did not improve from 0.99200\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 23ms/step - accuracy: 0.9876 - loss: 0.0387 - val_accuracy: 0.9918 - val_loss: 0.0288\n",
      "Epoch 16/100\n",
      "\u001b[1m840/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9893 - loss: 0.0353\n",
      "Epoch 16: val_accuracy did not improve from 0.99200\n",
      "\n",
      "Epoch 16: val_accuracy did not improve from 0.99200\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.9892 - loss: 0.0350 - val_accuracy: 0.9917 - val_loss: 0.0286\n",
      "Epoch 17/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9885 - loss: 0.0353\n",
      "Epoch 17: val_accuracy improved from 0.99200 to 0.99233, saving model to CNN_Models\\CNN_with_Dropout-lr_0.0001-bs_64-20250917-105632\\best_model_epoch-17_val_acc-0.9923.keras\n",
      "\n",
      "Epoch 17: val_accuracy improved from 0.99200 to 0.99233, saving model to wandb_models/CNN_with_Dropout-lr_0.0001-bs_64-20250917-105632/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 18ms/step - accuracy: 0.9887 - loss: 0.0358 - val_accuracy: 0.9923 - val_loss: 0.0275\n",
      "Epoch 18/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9901 - loss: 0.0317\n",
      "Epoch 18: val_accuracy did not improve from 0.99233\n",
      "\n",
      "Epoch 18: val_accuracy did not improve from 0.99233\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 21ms/step - accuracy: 0.9898 - loss: 0.0320 - val_accuracy: 0.9923 - val_loss: 0.0272\n",
      "Epoch 19/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9897 - loss: 0.0305\n",
      "Epoch 19: val_accuracy did not improve from 0.99233\n",
      "\n",
      "Epoch 19: val_accuracy did not improve from 0.99233\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.9896 - loss: 0.0316 - val_accuracy: 0.9923 - val_loss: 0.0277\n",
      "Epoch 20/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9909 - loss: 0.0287\n",
      "Epoch 20: val_accuracy did not improve from 0.99233\n",
      "\n",
      "Epoch 20: val_accuracy did not improve from 0.99233\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 18ms/step - accuracy: 0.9903 - loss: 0.0299 - val_accuracy: 0.9917 - val_loss: 0.0272\n",
      "Epoch 21/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9910 - loss: 0.0291\n",
      "Epoch 21: val_accuracy did not improve from 0.99233\n",
      "\n",
      "Epoch 21: val_accuracy did not improve from 0.99233\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 17ms/step - accuracy: 0.9908 - loss: 0.0297 - val_accuracy: 0.9920 - val_loss: 0.0275\n",
      "Epoch 22/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9917 - loss: 0.0262\n",
      "Epoch 22: val_accuracy improved from 0.99233 to 0.99317, saving model to CNN_Models\\CNN_with_Dropout-lr_0.0001-bs_64-20250917-105632\\best_model_epoch-22_val_acc-0.9932.keras\n",
      "\n",
      "Epoch 22: val_accuracy improved from 0.99233 to 0.99317, saving model to wandb_models/CNN_with_Dropout-lr_0.0001-bs_64-20250917-105632/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 17ms/step - accuracy: 0.9912 - loss: 0.0275 - val_accuracy: 0.9932 - val_loss: 0.0263\n",
      "Epoch 23/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9921 - loss: 0.0246\n",
      "Epoch 23: val_accuracy did not improve from 0.99317\n",
      "\n",
      "Epoch 23: val_accuracy did not improve from 0.99317\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 16ms/step - accuracy: 0.9918 - loss: 0.0258 - val_accuracy: 0.9928 - val_loss: 0.0266\n",
      "Epoch 24/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9922 - loss: 0.0241\n",
      "Epoch 24: val_accuracy did not improve from 0.99317\n",
      "\n",
      "Epoch 24: val_accuracy did not improve from 0.99317\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 17ms/step - accuracy: 0.9914 - loss: 0.0260 - val_accuracy: 0.9930 - val_loss: 0.0264\n",
      "Epoch 25/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9929 - loss: 0.0233\n",
      "Epoch 25: val_accuracy did not improve from 0.99317\n",
      "\n",
      "Epoch 25: val_accuracy did not improve from 0.99317\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 17ms/step - accuracy: 0.9922 - loss: 0.0244 - val_accuracy: 0.9923 - val_loss: 0.0275\n",
      "Epoch 26/100\n",
      "\u001b[1m840/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9919 - loss: 0.0238\n",
      "Epoch 26: val_accuracy did not improve from 0.99317\n",
      "\n",
      "Epoch 26: val_accuracy did not improve from 0.99317\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.9917 - loss: 0.0244 - val_accuracy: 0.9928 - val_loss: 0.0251\n",
      "Epoch 27/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9929 - loss: 0.0233\n",
      "Epoch 27: val_accuracy did not improve from 0.99317\n",
      "\n",
      "Epoch 27: val_accuracy did not improve from 0.99317\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 18ms/step - accuracy: 0.9929 - loss: 0.0234 - val_accuracy: 0.9932 - val_loss: 0.0255\n",
      "Epoch 28/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9925 - loss: 0.0220\n",
      "Epoch 28: val_accuracy did not improve from 0.99317\n",
      "\n",
      "Epoch 28: val_accuracy did not improve from 0.99317\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 21ms/step - accuracy: 0.9922 - loss: 0.0229 - val_accuracy: 0.9930 - val_loss: 0.0242\n",
      "Epoch 29/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9937 - loss: 0.0206\n",
      "Epoch 29: val_accuracy improved from 0.99317 to 0.99333, saving model to CNN_Models\\CNN_with_Dropout-lr_0.0001-bs_64-20250917-105632\\best_model_epoch-29_val_acc-0.9933.keras\n",
      "\n",
      "Epoch 29: val_accuracy improved from 0.99317 to 0.99333, saving model to wandb_models/CNN_with_Dropout-lr_0.0001-bs_64-20250917-105632/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 20ms/step - accuracy: 0.9930 - loss: 0.0214 - val_accuracy: 0.9933 - val_loss: 0.0248\n",
      "Epoch 30/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9937 - loss: 0.0205\n",
      "Epoch 30: val_accuracy improved from 0.99333 to 0.99350, saving model to CNN_Models\\CNN_with_Dropout-lr_0.0001-bs_64-20250917-105632\\best_model_epoch-30_val_acc-0.9935.keras\n",
      "\n",
      "Epoch 30: val_accuracy improved from 0.99333 to 0.99350, saving model to wandb_models/CNN_with_Dropout-lr_0.0001-bs_64-20250917-105632/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 17ms/step - accuracy: 0.9929 - loss: 0.0215 - val_accuracy: 0.9935 - val_loss: 0.0244\n",
      "Epoch 31/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9933 - loss: 0.0202\n",
      "Epoch 31: val_accuracy did not improve from 0.99350\n",
      "\n",
      "Epoch 31: val_accuracy did not improve from 0.99350\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 16ms/step - accuracy: 0.9932 - loss: 0.0201 - val_accuracy: 0.9930 - val_loss: 0.0256\n",
      "Epoch 32/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9940 - loss: 0.0181\n",
      "Epoch 32: val_accuracy did not improve from 0.99350\n",
      "\n",
      "Epoch 32: val_accuracy did not improve from 0.99350\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 17ms/step - accuracy: 0.9935 - loss: 0.0195 - val_accuracy: 0.9930 - val_loss: 0.0247\n",
      "Epoch 33/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9944 - loss: 0.0164\n",
      "Epoch 33: val_accuracy did not improve from 0.99350\n",
      "\n",
      "Epoch 33: val_accuracy did not improve from 0.99350\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 16ms/step - accuracy: 0.9941 - loss: 0.0176 - val_accuracy: 0.9928 - val_loss: 0.0247\n",
      "Epoch 34/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9937 - loss: 0.0184\n",
      "Epoch 34: val_accuracy did not improve from 0.99350\n",
      "\n",
      "Epoch 34: val_accuracy did not improve from 0.99350\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 17ms/step - accuracy: 0.9936 - loss: 0.0184 - val_accuracy: 0.9932 - val_loss: 0.0247\n",
      "Epoch 35/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9935 - loss: 0.0177\n",
      "Epoch 35: val_accuracy improved from 0.99350 to 0.99383, saving model to CNN_Models\\CNN_with_Dropout-lr_0.0001-bs_64-20250917-105632\\best_model_epoch-35_val_acc-0.9938.keras\n",
      "\n",
      "Epoch 35: val_accuracy improved from 0.99350 to 0.99383, saving model to wandb_models/CNN_with_Dropout-lr_0.0001-bs_64-20250917-105632/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 17ms/step - accuracy: 0.9936 - loss: 0.0184 - val_accuracy: 0.9938 - val_loss: 0.0250\n",
      "Epoch 36/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9946 - loss: 0.0174\n",
      "Epoch 36: val_accuracy improved from 0.99383 to 0.99400, saving model to CNN_Models\\CNN_with_Dropout-lr_0.0001-bs_64-20250917-105632\\best_model_epoch-36_val_acc-0.9940.keras\n",
      "\n",
      "Epoch 36: val_accuracy improved from 0.99383 to 0.99400, saving model to wandb_models/CNN_with_Dropout-lr_0.0001-bs_64-20250917-105632/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.9944 - loss: 0.0175 - val_accuracy: 0.9940 - val_loss: 0.0255\n",
      "Epoch 37/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9952 - loss: 0.0148\n",
      "Epoch 37: val_accuracy did not improve from 0.99400\n",
      "\n",
      "Epoch 37: val_accuracy did not improve from 0.99400\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 17ms/step - accuracy: 0.9949 - loss: 0.0158 - val_accuracy: 0.9930 - val_loss: 0.0259\n",
      "Epoch 38/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9947 - loss: 0.0154\n",
      "Epoch 38: val_accuracy did not improve from 0.99400\n",
      "\n",
      "Epoch 38: val_accuracy did not improve from 0.99400\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 17ms/step - accuracy: 0.9946 - loss: 0.0158 - val_accuracy: 0.9937 - val_loss: 0.0258\n",
      "Epoch 39/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9952 - loss: 0.0142\n",
      "Epoch 39: val_accuracy did not improve from 0.99400\n",
      "\n",
      "Epoch 39: val_accuracy did not improve from 0.99400\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 16ms/step - accuracy: 0.9947 - loss: 0.0155 - val_accuracy: 0.9940 - val_loss: 0.0249\n",
      "Epoch 40/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9950 - loss: 0.0147\n",
      "Epoch 40: val_accuracy did not improve from 0.99400\n",
      "\n",
      "Epoch 40: val_accuracy did not improve from 0.99400\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 17ms/step - accuracy: 0.9952 - loss: 0.0148 - val_accuracy: 0.9932 - val_loss: 0.0270\n",
      "Epoch 41/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9948 - loss: 0.0156\n",
      "Epoch 41: val_accuracy did not improve from 0.99400\n",
      "\n",
      "Epoch 41: val_accuracy did not improve from 0.99400\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 19ms/step - accuracy: 0.9947 - loss: 0.0151 - val_accuracy: 0.9935 - val_loss: 0.0247\n",
      "Epoch 42/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9945 - loss: 0.0150\n",
      "Epoch 42: val_accuracy did not improve from 0.99400\n",
      "\n",
      "Epoch 42: val_accuracy did not improve from 0.99400\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 19ms/step - accuracy: 0.9946 - loss: 0.0153 - val_accuracy: 0.9930 - val_loss: 0.0256\n",
      "Epoch 43/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9954 - loss: 0.0136\n",
      "Epoch 43: val_accuracy did not improve from 0.99400\n",
      "\n",
      "Epoch 43: val_accuracy did not improve from 0.99400\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 17ms/step - accuracy: 0.9953 - loss: 0.0136 - val_accuracy: 0.9938 - val_loss: 0.0244\n",
      "Epoch 44/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9961 - loss: 0.0125\n",
      "Epoch 44: val_accuracy did not improve from 0.99400\n",
      "\n",
      "Epoch 44: val_accuracy did not improve from 0.99400\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 18ms/step - accuracy: 0.9956 - loss: 0.0133 - val_accuracy: 0.9937 - val_loss: 0.0242\n",
      "Epoch 45/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9954 - loss: 0.0131\n",
      "Epoch 45: val_accuracy improved from 0.99400 to 0.99417, saving model to CNN_Models\\CNN_with_Dropout-lr_0.0001-bs_64-20250917-105632\\best_model_epoch-45_val_acc-0.9942.keras\n",
      "\n",
      "Epoch 45: val_accuracy improved from 0.99400 to 0.99417, saving model to wandb_models/CNN_with_Dropout-lr_0.0001-bs_64-20250917-105632/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 19ms/step - accuracy: 0.9951 - loss: 0.0138 - val_accuracy: 0.9942 - val_loss: 0.0246\n",
      "Epoch 46/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9963 - loss: 0.0117\n",
      "Epoch 46: val_accuracy did not improve from 0.99417\n",
      "\n",
      "Epoch 46: val_accuracy did not improve from 0.99417\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 17ms/step - accuracy: 0.9958 - loss: 0.0125 - val_accuracy: 0.9937 - val_loss: 0.0255\n",
      "Epoch 47/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9957 - loss: 0.0117\n",
      "Epoch 47: val_accuracy improved from 0.99417 to 0.99450, saving model to CNN_Models\\CNN_with_Dropout-lr_0.0001-bs_64-20250917-105632\\best_model_epoch-47_val_acc-0.9945.keras\n",
      "\n",
      "Epoch 47: val_accuracy improved from 0.99417 to 0.99450, saving model to wandb_models/CNN_with_Dropout-lr_0.0001-bs_64-20250917-105632/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 17ms/step - accuracy: 0.9957 - loss: 0.0121 - val_accuracy: 0.9945 - val_loss: 0.0238\n",
      "Epoch 48/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9962 - loss: 0.0114\n",
      "Epoch 48: val_accuracy did not improve from 0.99450\n",
      "\n",
      "Epoch 48: val_accuracy did not improve from 0.99450\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 17ms/step - accuracy: 0.9957 - loss: 0.0128 - val_accuracy: 0.9943 - val_loss: 0.0253\n",
      "Epoch 49/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9957 - loss: 0.0129\n",
      "Epoch 49: val_accuracy did not improve from 0.99450\n",
      "\n",
      "Epoch 49: val_accuracy did not improve from 0.99450\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 17ms/step - accuracy: 0.9958 - loss: 0.0121 - val_accuracy: 0.9933 - val_loss: 0.0252\n",
      "Epoch 50/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9960 - loss: 0.0117\n",
      "Epoch 50: val_accuracy did not improve from 0.99450\n",
      "\n",
      "Epoch 50: val_accuracy did not improve from 0.99450\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 17ms/step - accuracy: 0.9958 - loss: 0.0118 - val_accuracy: 0.9935 - val_loss: 0.0251\n",
      "Epoch 51/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9955 - loss: 0.0121\n",
      "Epoch 51: val_accuracy did not improve from 0.99450\n",
      "\n",
      "Epoch 51: val_accuracy did not improve from 0.99450\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 18ms/step - accuracy: 0.9957 - loss: 0.0117 - val_accuracy: 0.9943 - val_loss: 0.0234\n",
      "Epoch 52/100\n",
      "\u001b[1m840/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9964 - loss: 0.0106\n",
      "Epoch 52: val_accuracy did not improve from 0.99450\n",
      "\n",
      "Epoch 52: val_accuracy did not improve from 0.99450\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 17ms/step - accuracy: 0.9961 - loss: 0.0110 - val_accuracy: 0.9942 - val_loss: 0.0248\n",
      "Epoch 53/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9968 - loss: 0.0094\n",
      "Epoch 53: val_accuracy improved from 0.99450 to 0.99467, saving model to CNN_Models\\CNN_with_Dropout-lr_0.0001-bs_64-20250917-105632\\best_model_epoch-53_val_acc-0.9947.keras\n",
      "\n",
      "Epoch 53: val_accuracy improved from 0.99450 to 0.99467, saving model to wandb_models/CNN_with_Dropout-lr_0.0001-bs_64-20250917-105632/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 17ms/step - accuracy: 0.9961 - loss: 0.0107 - val_accuracy: 0.9947 - val_loss: 0.0238\n",
      "Epoch 54/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9966 - loss: 0.0103\n",
      "Epoch 54: val_accuracy did not improve from 0.99467\n",
      "\n",
      "Epoch 54: val_accuracy did not improve from 0.99467\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 19ms/step - accuracy: 0.9967 - loss: 0.0102 - val_accuracy: 0.9947 - val_loss: 0.0237\n",
      "Epoch 55/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9967 - loss: 0.0104\n",
      "Epoch 55: val_accuracy did not improve from 0.99467\n",
      "\n",
      "Epoch 55: val_accuracy did not improve from 0.99467\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.9967 - loss: 0.0105 - val_accuracy: 0.9937 - val_loss: 0.0243\n",
      "Epoch 56/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9962 - loss: 0.0103\n",
      "Epoch 56: val_accuracy did not improve from 0.99467\n",
      "\n",
      "Epoch 56: val_accuracy did not improve from 0.99467\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 16ms/step - accuracy: 0.9963 - loss: 0.0104 - val_accuracy: 0.9938 - val_loss: 0.0247\n",
      "Epoch 57/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9970 - loss: 0.0092\n",
      "Epoch 57: val_accuracy did not improve from 0.99467\n",
      "\n",
      "Epoch 57: val_accuracy did not improve from 0.99467\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 16ms/step - accuracy: 0.9965 - loss: 0.0100 - val_accuracy: 0.9945 - val_loss: 0.0252\n",
      "Epoch 58/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9967 - loss: 0.0100\n",
      "Epoch 58: val_accuracy did not improve from 0.99467\n",
      "\n",
      "Epoch 58: val_accuracy did not improve from 0.99467\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 17ms/step - accuracy: 0.9968 - loss: 0.0095 - val_accuracy: 0.9942 - val_loss: 0.0257\n",
      "Epoch 59/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9970 - loss: 0.0097\n",
      "Epoch 59: val_accuracy did not improve from 0.99467\n",
      "\n",
      "Epoch 59: val_accuracy did not improve from 0.99467\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 17ms/step - accuracy: 0.9968 - loss: 0.0097 - val_accuracy: 0.9945 - val_loss: 0.0249\n",
      "Epoch 60/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9965 - loss: 0.0099\n",
      "Epoch 60: val_accuracy did not improve from 0.99467\n",
      "\n",
      "Epoch 60: val_accuracy did not improve from 0.99467\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 19ms/step - accuracy: 0.9967 - loss: 0.0098 - val_accuracy: 0.9947 - val_loss: 0.0239\n",
      "Epoch 61/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9968 - loss: 0.0092\n",
      "Epoch 61: val_accuracy improved from 0.99467 to 0.99500, saving model to CNN_Models\\CNN_with_Dropout-lr_0.0001-bs_64-20250917-105632\\best_model_epoch-61_val_acc-0.9950.keras\n",
      "\n",
      "Epoch 61: val_accuracy improved from 0.99467 to 0.99500, saving model to wandb_models/CNN_with_Dropout-lr_0.0001-bs_64-20250917-105632/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 19ms/step - accuracy: 0.9967 - loss: 0.0095 - val_accuracy: 0.9950 - val_loss: 0.0244\n",
      "Epoch 62/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9974 - loss: 0.0088\n",
      "Epoch 62: val_accuracy did not improve from 0.99500\n",
      "\n",
      "Epoch 62: val_accuracy did not improve from 0.99500\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 21ms/step - accuracy: 0.9969 - loss: 0.0096 - val_accuracy: 0.9938 - val_loss: 0.0272\n",
      "Epoch 63/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9970 - loss: 0.0091\n",
      "Epoch 63: val_accuracy did not improve from 0.99500\n",
      "\n",
      "Epoch 63: val_accuracy did not improve from 0.99500\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 19ms/step - accuracy: 0.9969 - loss: 0.0094 - val_accuracy: 0.9937 - val_loss: 0.0247\n",
      "Epoch 64/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9970 - loss: 0.0088\n",
      "Epoch 64: val_accuracy did not improve from 0.99500\n",
      "\n",
      "Epoch 64: val_accuracy did not improve from 0.99500\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 21ms/step - accuracy: 0.9969 - loss: 0.0089 - val_accuracy: 0.9943 - val_loss: 0.0266\n",
      "Epoch 65/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9972 - loss: 0.0095\n",
      "Epoch 65: val_accuracy did not improve from 0.99500\n",
      "\n",
      "Epoch 65: val_accuracy did not improve from 0.99500\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 19ms/step - accuracy: 0.9973 - loss: 0.0089 - val_accuracy: 0.9942 - val_loss: 0.0257\n",
      "Epoch 66/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9969 - loss: 0.0094\n",
      "Epoch 66: val_accuracy did not improve from 0.99500\n",
      "\n",
      "Epoch 66: val_accuracy did not improve from 0.99500\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 21ms/step - accuracy: 0.9969 - loss: 0.0091 - val_accuracy: 0.9930 - val_loss: 0.0258\n",
      "Epoch 67/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9973 - loss: 0.0079\n",
      "Epoch 67: val_accuracy did not improve from 0.99500\n",
      "\n",
      "Epoch 67: val_accuracy did not improve from 0.99500\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 24ms/step - accuracy: 0.9971 - loss: 0.0082 - val_accuracy: 0.9943 - val_loss: 0.0249\n",
      "Epoch 68/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9973 - loss: 0.0079\n",
      "Epoch 68: val_accuracy did not improve from 0.99500\n",
      "\n",
      "Epoch 68: val_accuracy did not improve from 0.99500\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 23ms/step - accuracy: 0.9971 - loss: 0.0083 - val_accuracy: 0.9942 - val_loss: 0.0257\n",
      "Epoch 69/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9974 - loss: 0.0078\n",
      "Epoch 69: val_accuracy did not improve from 0.99500\n",
      "\n",
      "Epoch 69: val_accuracy did not improve from 0.99500\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 22ms/step - accuracy: 0.9976 - loss: 0.0072 - val_accuracy: 0.9948 - val_loss: 0.0249\n",
      "Epoch 70/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9975 - loss: 0.0077\n",
      "Epoch 70: val_accuracy did not improve from 0.99500\n",
      "\n",
      "Epoch 70: val_accuracy did not improve from 0.99500\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 19ms/step - accuracy: 0.9972 - loss: 0.0081 - val_accuracy: 0.9943 - val_loss: 0.0246\n",
      "Epoch 71/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9973 - loss: 0.0074\n",
      "Epoch 71: val_accuracy did not improve from 0.99500\n",
      "\n",
      "Epoch 71: val_accuracy did not improve from 0.99500\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 19ms/step - accuracy: 0.9971 - loss: 0.0079 - val_accuracy: 0.9942 - val_loss: 0.0256\n",
      "\n",
      "Training history saved to: CNN_Models\\CNN_with_Dropout-lr_0.0001-bs_64-20250917-105632\\training_history.pkl\n",
      "\n",
      "--- Peak Performance Summary ---\n",
      "Best validation accuracy:           0.9950\n",
      "Associated training accuracy:       0.9967\n",
      "Occurred at epoch:                  61\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch/accuracy</td><td>▁▆▇▇▇▇▇▇████████████████████████████████</td></tr><tr><td>epoch/epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>epoch/learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/loss</td><td>█▇▅▄▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/val_accuracy</td><td>▁▅▆▇▇▇▇▇▇▇▇██▇▇█████████████████████████</td></tr><tr><td>epoch/val_loss</td><td>█▅▄▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch/accuracy</td><td>0.99715</td></tr><tr><td>epoch/epoch</td><td>70</td></tr><tr><td>epoch/learning_rate</td><td>0.0001</td></tr><tr><td>epoch/loss</td><td>0.00787</td></tr><tr><td>epoch/val_accuracy</td><td>0.99417</td></tr><tr><td>epoch/val_loss</td><td>0.02559</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">CNN_with_Dropout-lr_0.0001-bs_64-20250917-105632</strong> at: <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/xdbps14t' target=\"_blank\">https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/xdbps14t</a><br> View project at: <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2' target=\"_blank\">https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2</a><br>Synced 5 W&B file(s), 0 media file(s), 38 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250917_105632-xdbps14t\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Adapting the normalisation layer...\n",
      "Adaptation complete.\n",
      "\n",
      "\n",
      "--- Starting Experiment: CNN_with_Dropout ---\n",
      "\n",
      "--- Model Architecture ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"CNN_with_Dropout\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"CNN_with_Dropout\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ normalization_26                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Normalization</span>)                 │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_26 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1600</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1600</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_108 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">204,928</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_109 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,290</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ normalization_26                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m1\u001b[0m)      │             \u001b[38;5;34m3\u001b[0m │\n",
       "│ (\u001b[38;5;33mNormalization\u001b[0m)                 │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_12 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m320\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_12 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_13 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_13 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_26 (\u001b[38;5;33mFlatten\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1600\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1600\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_108 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m204,928\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_109 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │         \u001b[38;5;34m1,290\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">225,037</span> (879.05 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m225,037\u001b[0m (879.05 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">225,034</span> (879.04 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m225,034\u001b[0m (879.04 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3</span> (16.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m3\u001b[0m (16.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Hyperparameters ---\n",
      "optimiser           : SGD\n",
      "learning_rate       : 0.001\n",
      "epochs              : 100\n",
      "batch_size          : 64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "creating run (0.0s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\TimVos\\VSC Projects\\CSE5ML\\Assessment 2\\wandb\\run-20250917_111455-fhdhm21m</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/fhdhm21m' target=\"_blank\">CNN_with_Dropout-lr_0.001-bs_64-20250917-111455</a></strong> to <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2' target=\"_blank\">https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/fhdhm21m' target=\"_blank\">https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/fhdhm21m</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.2193 - loss: 2.1777\n",
      "Epoch 1: val_accuracy improved from None to 0.74417, saving model to CNN_Models\\CNN_with_Dropout-lr_0.001-bs_64-20250917-111455\\best_model_epoch-01_val_acc-0.7442.keras\n",
      "\n",
      "Epoch 1: val_accuracy improved from None to 0.74417, saving model to wandb_models/CNN_with_Dropout-lr_0.001-bs_64-20250917-111455/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 19ms/step - accuracy: 0.3445 - loss: 1.9696 - val_accuracy: 0.7442 - val_loss: 1.1931\n",
      "Epoch 2/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.6144 - loss: 1.2217\n",
      "Epoch 2: val_accuracy improved from 0.74417 to 0.88183, saving model to CNN_Models\\CNN_with_Dropout-lr_0.001-bs_64-20250917-111455\\best_model_epoch-02_val_acc-0.8818.keras\n",
      "\n",
      "Epoch 2: val_accuracy improved from 0.74417 to 0.88183, saving model to wandb_models/CNN_with_Dropout-lr_0.001-bs_64-20250917-111455/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 18ms/step - accuracy: 0.6671 - loss: 1.0440 - val_accuracy: 0.8818 - val_loss: 0.4763\n",
      "Epoch 3/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7667 - loss: 0.7214\n",
      "Epoch 3: val_accuracy improved from 0.88183 to 0.91533, saving model to CNN_Models\\CNN_with_Dropout-lr_0.001-bs_64-20250917-111455\\best_model_epoch-03_val_acc-0.9153.keras\n",
      "\n",
      "Epoch 3: val_accuracy improved from 0.88183 to 0.91533, saving model to wandb_models/CNN_with_Dropout-lr_0.001-bs_64-20250917-111455/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 18ms/step - accuracy: 0.7858 - loss: 0.6671 - val_accuracy: 0.9153 - val_loss: 0.3263\n",
      "Epoch 4/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8312 - loss: 0.5413\n",
      "Epoch 4: val_accuracy improved from 0.91533 to 0.93050, saving model to CNN_Models\\CNN_with_Dropout-lr_0.001-bs_64-20250917-111455\\best_model_epoch-04_val_acc-0.9305.keras\n",
      "\n",
      "Epoch 4: val_accuracy improved from 0.91533 to 0.93050, saving model to wandb_models/CNN_with_Dropout-lr_0.001-bs_64-20250917-111455/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 17ms/step - accuracy: 0.8400 - loss: 0.5164 - val_accuracy: 0.9305 - val_loss: 0.2609\n",
      "Epoch 5/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.8599 - loss: 0.4483\n",
      "Epoch 5: val_accuracy improved from 0.93050 to 0.93883, saving model to CNN_Models\\CNN_with_Dropout-lr_0.001-bs_64-20250917-111455\\best_model_epoch-05_val_acc-0.9388.keras\n",
      "\n",
      "Epoch 5: val_accuracy improved from 0.93050 to 0.93883, saving model to wandb_models/CNN_with_Dropout-lr_0.001-bs_64-20250917-111455/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 22ms/step - accuracy: 0.8634 - loss: 0.4365 - val_accuracy: 0.9388 - val_loss: 0.2223\n",
      "Epoch 6/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8767 - loss: 0.3954\n",
      "Epoch 6: val_accuracy improved from 0.93883 to 0.94400, saving model to CNN_Models\\CNN_with_Dropout-lr_0.001-bs_64-20250917-111455\\best_model_epoch-06_val_acc-0.9440.keras\n",
      "\n",
      "Epoch 6: val_accuracy improved from 0.93883 to 0.94400, saving model to wandb_models/CNN_with_Dropout-lr_0.001-bs_64-20250917-111455/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 18ms/step - accuracy: 0.8812 - loss: 0.3822 - val_accuracy: 0.9440 - val_loss: 0.1986\n",
      "Epoch 7/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8937 - loss: 0.3521\n",
      "Epoch 7: val_accuracy improved from 0.94400 to 0.94733, saving model to CNN_Models\\CNN_with_Dropout-lr_0.001-bs_64-20250917-111455\\best_model_epoch-07_val_acc-0.9473.keras\n",
      "\n",
      "Epoch 7: val_accuracy improved from 0.94400 to 0.94733, saving model to wandb_models/CNN_with_Dropout-lr_0.001-bs_64-20250917-111455/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 17ms/step - accuracy: 0.8956 - loss: 0.3451 - val_accuracy: 0.9473 - val_loss: 0.1801\n",
      "Epoch 8/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9041 - loss: 0.3179\n",
      "Epoch 8: val_accuracy improved from 0.94733 to 0.95217, saving model to CNN_Models\\CNN_with_Dropout-lr_0.001-bs_64-20250917-111455\\best_model_epoch-08_val_acc-0.9522.keras\n",
      "\n",
      "Epoch 8: val_accuracy improved from 0.94733 to 0.95217, saving model to wandb_models/CNN_with_Dropout-lr_0.001-bs_64-20250917-111455/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 21ms/step - accuracy: 0.9039 - loss: 0.3157 - val_accuracy: 0.9522 - val_loss: 0.1649\n",
      "Epoch 9/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9097 - loss: 0.2989\n",
      "Epoch 9: val_accuracy improved from 0.95217 to 0.95533, saving model to CNN_Models\\CNN_with_Dropout-lr_0.001-bs_64-20250917-111455\\best_model_epoch-09_val_acc-0.9553.keras\n",
      "\n",
      "Epoch 9: val_accuracy improved from 0.95217 to 0.95533, saving model to wandb_models/CNN_with_Dropout-lr_0.001-bs_64-20250917-111455/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 20ms/step - accuracy: 0.9097 - loss: 0.2958 - val_accuracy: 0.9553 - val_loss: 0.1540\n",
      "Epoch 10/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9155 - loss: 0.2781\n",
      "Epoch 10: val_accuracy improved from 0.95533 to 0.95850, saving model to CNN_Models\\CNN_with_Dropout-lr_0.001-bs_64-20250917-111455\\best_model_epoch-10_val_acc-0.9585.keras\n",
      "\n",
      "Epoch 10: val_accuracy improved from 0.95533 to 0.95850, saving model to wandb_models/CNN_with_Dropout-lr_0.001-bs_64-20250917-111455/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 22ms/step - accuracy: 0.9161 - loss: 0.2765 - val_accuracy: 0.9585 - val_loss: 0.1446\n",
      "Epoch 11/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9218 - loss: 0.2612\n",
      "Epoch 11: val_accuracy improved from 0.95850 to 0.96217, saving model to CNN_Models\\CNN_with_Dropout-lr_0.001-bs_64-20250917-111455\\best_model_epoch-11_val_acc-0.9622.keras\n",
      "\n",
      "Epoch 11: val_accuracy improved from 0.95850 to 0.96217, saving model to wandb_models/CNN_with_Dropout-lr_0.001-bs_64-20250917-111455/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 25ms/step - accuracy: 0.9213 - loss: 0.2607 - val_accuracy: 0.9622 - val_loss: 0.1368\n",
      "Epoch 12/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9270 - loss: 0.2441\n",
      "Epoch 12: val_accuracy improved from 0.96217 to 0.96417, saving model to CNN_Models\\CNN_with_Dropout-lr_0.001-bs_64-20250917-111455\\best_model_epoch-12_val_acc-0.9642.keras\n",
      "\n",
      "Epoch 12: val_accuracy improved from 0.96217 to 0.96417, saving model to wandb_models/CNN_with_Dropout-lr_0.001-bs_64-20250917-111455/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 19ms/step - accuracy: 0.9266 - loss: 0.2450 - val_accuracy: 0.9642 - val_loss: 0.1295\n",
      "Epoch 13/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9284 - loss: 0.2362\n",
      "Epoch 13: val_accuracy improved from 0.96417 to 0.96600, saving model to CNN_Models\\CNN_with_Dropout-lr_0.001-bs_64-20250917-111455\\best_model_epoch-13_val_acc-0.9660.keras\n",
      "\n",
      "Epoch 13: val_accuracy improved from 0.96417 to 0.96600, saving model to wandb_models/CNN_with_Dropout-lr_0.001-bs_64-20250917-111455/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 18ms/step - accuracy: 0.9282 - loss: 0.2349 - val_accuracy: 0.9660 - val_loss: 0.1244\n",
      "Epoch 14/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9316 - loss: 0.2289\n",
      "Epoch 14: val_accuracy improved from 0.96600 to 0.96733, saving model to CNN_Models\\CNN_with_Dropout-lr_0.001-bs_64-20250917-111455\\best_model_epoch-14_val_acc-0.9673.keras\n",
      "\n",
      "Epoch 14: val_accuracy improved from 0.96600 to 0.96733, saving model to wandb_models/CNN_with_Dropout-lr_0.001-bs_64-20250917-111455/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 20ms/step - accuracy: 0.9317 - loss: 0.2264 - val_accuracy: 0.9673 - val_loss: 0.1184\n",
      "Epoch 15/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9356 - loss: 0.2144\n",
      "Epoch 15: val_accuracy improved from 0.96733 to 0.96967, saving model to CNN_Models\\CNN_with_Dropout-lr_0.001-bs_64-20250917-111455\\best_model_epoch-15_val_acc-0.9697.keras\n",
      "\n",
      "Epoch 15: val_accuracy improved from 0.96733 to 0.96967, saving model to wandb_models/CNN_with_Dropout-lr_0.001-bs_64-20250917-111455/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 19ms/step - accuracy: 0.9346 - loss: 0.2160 - val_accuracy: 0.9697 - val_loss: 0.1141\n",
      "Epoch 16/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9358 - loss: 0.2071\n",
      "Epoch 16: val_accuracy improved from 0.96967 to 0.97000, saving model to CNN_Models\\CNN_with_Dropout-lr_0.001-bs_64-20250917-111455\\best_model_epoch-16_val_acc-0.9700.keras\n",
      "\n",
      "Epoch 16: val_accuracy improved from 0.96967 to 0.97000, saving model to wandb_models/CNN_with_Dropout-lr_0.001-bs_64-20250917-111455/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 19ms/step - accuracy: 0.9362 - loss: 0.2079 - val_accuracy: 0.9700 - val_loss: 0.1097\n",
      "Epoch 17/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9407 - loss: 0.1957\n",
      "Epoch 17: val_accuracy improved from 0.97000 to 0.97150, saving model to CNN_Models\\CNN_with_Dropout-lr_0.001-bs_64-20250917-111455\\best_model_epoch-17_val_acc-0.9715.keras\n",
      "\n",
      "Epoch 17: val_accuracy improved from 0.97000 to 0.97150, saving model to wandb_models/CNN_with_Dropout-lr_0.001-bs_64-20250917-111455/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 20ms/step - accuracy: 0.9394 - loss: 0.1996 - val_accuracy: 0.9715 - val_loss: 0.1059\n",
      "Epoch 18/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9420 - loss: 0.1932\n",
      "Epoch 18: val_accuracy improved from 0.97150 to 0.97250, saving model to CNN_Models\\CNN_with_Dropout-lr_0.001-bs_64-20250917-111455\\best_model_epoch-18_val_acc-0.9725.keras\n",
      "\n",
      "Epoch 18: val_accuracy improved from 0.97150 to 0.97250, saving model to wandb_models/CNN_with_Dropout-lr_0.001-bs_64-20250917-111455/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 20ms/step - accuracy: 0.9417 - loss: 0.1920 - val_accuracy: 0.9725 - val_loss: 0.1024\n",
      "Epoch 19/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9426 - loss: 0.1886\n",
      "Epoch 19: val_accuracy improved from 0.97250 to 0.97333, saving model to CNN_Models\\CNN_with_Dropout-lr_0.001-bs_64-20250917-111455\\best_model_epoch-19_val_acc-0.9733.keras\n",
      "\n",
      "Epoch 19: val_accuracy improved from 0.97250 to 0.97333, saving model to wandb_models/CNN_with_Dropout-lr_0.001-bs_64-20250917-111455/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 19ms/step - accuracy: 0.9429 - loss: 0.1876 - val_accuracy: 0.9733 - val_loss: 0.0992\n",
      "Epoch 20/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9460 - loss: 0.1781\n",
      "Epoch 20: val_accuracy improved from 0.97333 to 0.97367, saving model to CNN_Models\\CNN_with_Dropout-lr_0.001-bs_64-20250917-111455\\best_model_epoch-20_val_acc-0.9737.keras\n",
      "\n",
      "Epoch 20: val_accuracy improved from 0.97333 to 0.97367, saving model to wandb_models/CNN_with_Dropout-lr_0.001-bs_64-20250917-111455/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 18ms/step - accuracy: 0.9450 - loss: 0.1815 - val_accuracy: 0.9737 - val_loss: 0.0967\n",
      "Epoch 21/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9466 - loss: 0.1724\n",
      "Epoch 21: val_accuracy improved from 0.97367 to 0.97567, saving model to CNN_Models\\CNN_with_Dropout-lr_0.001-bs_64-20250917-111455\\best_model_epoch-21_val_acc-0.9757.keras\n",
      "\n",
      "Epoch 21: val_accuracy improved from 0.97367 to 0.97567, saving model to wandb_models/CNN_with_Dropout-lr_0.001-bs_64-20250917-111455/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 21ms/step - accuracy: 0.9460 - loss: 0.1756 - val_accuracy: 0.9757 - val_loss: 0.0940\n",
      "Epoch 22/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9469 - loss: 0.1729\n",
      "Epoch 22: val_accuracy did not improve from 0.97567\n",
      "\n",
      "Epoch 22: val_accuracy did not improve from 0.97567\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 20ms/step - accuracy: 0.9472 - loss: 0.1720 - val_accuracy: 0.9753 - val_loss: 0.0916\n",
      "Epoch 23/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9504 - loss: 0.1648\n",
      "Epoch 23: val_accuracy did not improve from 0.97567\n",
      "\n",
      "Epoch 23: val_accuracy did not improve from 0.97567\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 16ms/step - accuracy: 0.9486 - loss: 0.1678 - val_accuracy: 0.9755 - val_loss: 0.0899\n",
      "Epoch 24/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9504 - loss: 0.1652\n",
      "Epoch 24: val_accuracy improved from 0.97567 to 0.97650, saving model to CNN_Models\\CNN_with_Dropout-lr_0.001-bs_64-20250917-111455\\best_model_epoch-24_val_acc-0.9765.keras\n",
      "\n",
      "Epoch 24: val_accuracy improved from 0.97567 to 0.97650, saving model to wandb_models/CNN_with_Dropout-lr_0.001-bs_64-20250917-111455/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 15ms/step - accuracy: 0.9497 - loss: 0.1659 - val_accuracy: 0.9765 - val_loss: 0.0870\n",
      "Epoch 25/100\n",
      "\u001b[1m840/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9526 - loss: 0.1581\n",
      "Epoch 25: val_accuracy improved from 0.97650 to 0.97683, saving model to CNN_Models\\CNN_with_Dropout-lr_0.001-bs_64-20250917-111455\\best_model_epoch-25_val_acc-0.9768.keras\n",
      "\n",
      "Epoch 25: val_accuracy improved from 0.97650 to 0.97683, saving model to wandb_models/CNN_with_Dropout-lr_0.001-bs_64-20250917-111455/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 15ms/step - accuracy: 0.9515 - loss: 0.1597 - val_accuracy: 0.9768 - val_loss: 0.0852\n",
      "Epoch 26/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9538 - loss: 0.1522\n",
      "Epoch 26: val_accuracy improved from 0.97683 to 0.97767, saving model to CNN_Models\\CNN_with_Dropout-lr_0.001-bs_64-20250917-111455\\best_model_epoch-26_val_acc-0.9777.keras\n",
      "\n",
      "Epoch 26: val_accuracy improved from 0.97683 to 0.97767, saving model to wandb_models/CNN_with_Dropout-lr_0.001-bs_64-20250917-111455/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 15ms/step - accuracy: 0.9525 - loss: 0.1543 - val_accuracy: 0.9777 - val_loss: 0.0834\n",
      "Epoch 27/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9555 - loss: 0.1475\n",
      "Epoch 27: val_accuracy improved from 0.97767 to 0.97800, saving model to CNN_Models\\CNN_with_Dropout-lr_0.001-bs_64-20250917-111455\\best_model_epoch-27_val_acc-0.9780.keras\n",
      "\n",
      "Epoch 27: val_accuracy improved from 0.97767 to 0.97800, saving model to wandb_models/CNN_with_Dropout-lr_0.001-bs_64-20250917-111455/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 15ms/step - accuracy: 0.9548 - loss: 0.1507 - val_accuracy: 0.9780 - val_loss: 0.0817\n",
      "Epoch 28/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9546 - loss: 0.1479\n",
      "Epoch 28: val_accuracy did not improve from 0.97800\n",
      "\n",
      "Epoch 28: val_accuracy did not improve from 0.97800\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.9546 - loss: 0.1483 - val_accuracy: 0.9780 - val_loss: 0.0808\n",
      "Epoch 29/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9554 - loss: 0.1452\n",
      "Epoch 29: val_accuracy improved from 0.97800 to 0.97883, saving model to CNN_Models\\CNN_with_Dropout-lr_0.001-bs_64-20250917-111455\\best_model_epoch-29_val_acc-0.9788.keras\n",
      "\n",
      "Epoch 29: val_accuracy improved from 0.97800 to 0.97883, saving model to wandb_models/CNN_with_Dropout-lr_0.001-bs_64-20250917-111455/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 16ms/step - accuracy: 0.9546 - loss: 0.1458 - val_accuracy: 0.9788 - val_loss: 0.0790\n",
      "Epoch 30/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9569 - loss: 0.1407\n",
      "Epoch 30: val_accuracy improved from 0.97883 to 0.97900, saving model to CNN_Models\\CNN_with_Dropout-lr_0.001-bs_64-20250917-111455\\best_model_epoch-30_val_acc-0.9790.keras\n",
      "\n",
      "Epoch 30: val_accuracy improved from 0.97883 to 0.97900, saving model to wandb_models/CNN_with_Dropout-lr_0.001-bs_64-20250917-111455/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 19ms/step - accuracy: 0.9560 - loss: 0.1435 - val_accuracy: 0.9790 - val_loss: 0.0776\n",
      "Epoch 31/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9578 - loss: 0.1393\n",
      "Epoch 31: val_accuracy improved from 0.97900 to 0.97967, saving model to CNN_Models\\CNN_with_Dropout-lr_0.001-bs_64-20250917-111455\\best_model_epoch-31_val_acc-0.9797.keras\n",
      "\n",
      "Epoch 31: val_accuracy improved from 0.97900 to 0.97967, saving model to wandb_models/CNN_with_Dropout-lr_0.001-bs_64-20250917-111455/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 17ms/step - accuracy: 0.9567 - loss: 0.1414 - val_accuracy: 0.9797 - val_loss: 0.0760\n",
      "Epoch 32/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9592 - loss: 0.1349\n",
      "Epoch 32: val_accuracy improved from 0.97967 to 0.98017, saving model to CNN_Models\\CNN_with_Dropout-lr_0.001-bs_64-20250917-111455\\best_model_epoch-32_val_acc-0.9802.keras\n",
      "\n",
      "Epoch 32: val_accuracy improved from 0.97967 to 0.98017, saving model to wandb_models/CNN_with_Dropout-lr_0.001-bs_64-20250917-111455/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 20ms/step - accuracy: 0.9592 - loss: 0.1367 - val_accuracy: 0.9802 - val_loss: 0.0748\n",
      "Epoch 33/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9603 - loss: 0.1332\n",
      "Epoch 33: val_accuracy improved from 0.98017 to 0.98100, saving model to CNN_Models\\CNN_with_Dropout-lr_0.001-bs_64-20250917-111455\\best_model_epoch-33_val_acc-0.9810.keras\n",
      "\n",
      "Epoch 33: val_accuracy improved from 0.98017 to 0.98100, saving model to wandb_models/CNN_with_Dropout-lr_0.001-bs_64-20250917-111455/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 21ms/step - accuracy: 0.9586 - loss: 0.1358 - val_accuracy: 0.9810 - val_loss: 0.0729\n",
      "Epoch 34/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9592 - loss: 0.1337\n",
      "Epoch 34: val_accuracy did not improve from 0.98100\n",
      "\n",
      "Epoch 34: val_accuracy did not improve from 0.98100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 17ms/step - accuracy: 0.9589 - loss: 0.1348 - val_accuracy: 0.9807 - val_loss: 0.0722\n",
      "Epoch 35/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9607 - loss: 0.1292\n",
      "Epoch 35: val_accuracy improved from 0.98100 to 0.98133, saving model to CNN_Models\\CNN_with_Dropout-lr_0.001-bs_64-20250917-111455\\best_model_epoch-35_val_acc-0.9813.keras\n",
      "\n",
      "Epoch 35: val_accuracy improved from 0.98100 to 0.98133, saving model to wandb_models/CNN_with_Dropout-lr_0.001-bs_64-20250917-111455/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 17ms/step - accuracy: 0.9597 - loss: 0.1321 - val_accuracy: 0.9813 - val_loss: 0.0712\n",
      "Epoch 36/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9630 - loss: 0.1225\n",
      "Epoch 36: val_accuracy did not improve from 0.98133\n",
      "\n",
      "Epoch 36: val_accuracy did not improve from 0.98133\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 16ms/step - accuracy: 0.9614 - loss: 0.1262 - val_accuracy: 0.9810 - val_loss: 0.0706\n",
      "Epoch 37/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9627 - loss: 0.1241\n",
      "Epoch 37: val_accuracy improved from 0.98133 to 0.98167, saving model to CNN_Models\\CNN_with_Dropout-lr_0.001-bs_64-20250917-111455\\best_model_epoch-37_val_acc-0.9817.keras\n",
      "\n",
      "Epoch 37: val_accuracy improved from 0.98133 to 0.98167, saving model to wandb_models/CNN_with_Dropout-lr_0.001-bs_64-20250917-111455/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 16ms/step - accuracy: 0.9611 - loss: 0.1267 - val_accuracy: 0.9817 - val_loss: 0.0691\n",
      "Epoch 38/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9629 - loss: 0.1210\n",
      "Epoch 38: val_accuracy improved from 0.98167 to 0.98183, saving model to CNN_Models\\CNN_with_Dropout-lr_0.001-bs_64-20250917-111455\\best_model_epoch-38_val_acc-0.9818.keras\n",
      "\n",
      "Epoch 38: val_accuracy improved from 0.98167 to 0.98183, saving model to wandb_models/CNN_with_Dropout-lr_0.001-bs_64-20250917-111455/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 16ms/step - accuracy: 0.9617 - loss: 0.1245 - val_accuracy: 0.9818 - val_loss: 0.0678\n",
      "Epoch 39/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9628 - loss: 0.1214\n",
      "Epoch 39: val_accuracy did not improve from 0.98183\n",
      "\n",
      "Epoch 39: val_accuracy did not improve from 0.98183\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 16ms/step - accuracy: 0.9619 - loss: 0.1227 - val_accuracy: 0.9817 - val_loss: 0.0673\n",
      "Epoch 40/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9632 - loss: 0.1193\n",
      "Epoch 40: val_accuracy did not improve from 0.98183\n",
      "\n",
      "Epoch 40: val_accuracy did not improve from 0.98183\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 16ms/step - accuracy: 0.9620 - loss: 0.1217 - val_accuracy: 0.9817 - val_loss: 0.0667\n",
      "Epoch 41/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9640 - loss: 0.1178\n",
      "Epoch 41: val_accuracy improved from 0.98183 to 0.98233, saving model to CNN_Models\\CNN_with_Dropout-lr_0.001-bs_64-20250917-111455\\best_model_epoch-41_val_acc-0.9823.keras\n",
      "\n",
      "Epoch 41: val_accuracy improved from 0.98183 to 0.98233, saving model to wandb_models/CNN_with_Dropout-lr_0.001-bs_64-20250917-111455/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 20ms/step - accuracy: 0.9635 - loss: 0.1195 - val_accuracy: 0.9823 - val_loss: 0.0658\n",
      "Epoch 42/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9652 - loss: 0.1164\n",
      "Epoch 42: val_accuracy did not improve from 0.98233\n",
      "\n",
      "Epoch 42: val_accuracy did not improve from 0.98233\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 21ms/step - accuracy: 0.9641 - loss: 0.1182 - val_accuracy: 0.9823 - val_loss: 0.0650\n",
      "Epoch 43/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9655 - loss: 0.1159\n",
      "Epoch 43: val_accuracy improved from 0.98233 to 0.98333, saving model to CNN_Models\\CNN_with_Dropout-lr_0.001-bs_64-20250917-111455\\best_model_epoch-43_val_acc-0.9833.keras\n",
      "\n",
      "Epoch 43: val_accuracy improved from 0.98233 to 0.98333, saving model to wandb_models/CNN_with_Dropout-lr_0.001-bs_64-20250917-111455/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 21ms/step - accuracy: 0.9642 - loss: 0.1167 - val_accuracy: 0.9833 - val_loss: 0.0635\n",
      "Epoch 44/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9646 - loss: 0.1140\n",
      "Epoch 44: val_accuracy did not improve from 0.98333\n",
      "\n",
      "Epoch 44: val_accuracy did not improve from 0.98333\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 23ms/step - accuracy: 0.9650 - loss: 0.1143 - val_accuracy: 0.9827 - val_loss: 0.0635\n",
      "Epoch 45/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9646 - loss: 0.1165\n",
      "Epoch 45: val_accuracy did not improve from 0.98333\n",
      "\n",
      "Epoch 45: val_accuracy did not improve from 0.98333\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 20ms/step - accuracy: 0.9643 - loss: 0.1154 - val_accuracy: 0.9830 - val_loss: 0.0623\n",
      "Epoch 46/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9671 - loss: 0.1093\n",
      "Epoch 46: val_accuracy did not improve from 0.98333\n",
      "\n",
      "Epoch 46: val_accuracy did not improve from 0.98333\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 21ms/step - accuracy: 0.9654 - loss: 0.1125 - val_accuracy: 0.9828 - val_loss: 0.0622\n",
      "Epoch 47/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9669 - loss: 0.1101\n",
      "Epoch 47: val_accuracy improved from 0.98333 to 0.98350, saving model to CNN_Models\\CNN_with_Dropout-lr_0.001-bs_64-20250917-111455\\best_model_epoch-47_val_acc-0.9835.keras\n",
      "\n",
      "Epoch 47: val_accuracy improved from 0.98333 to 0.98350, saving model to wandb_models/CNN_with_Dropout-lr_0.001-bs_64-20250917-111455/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 18ms/step - accuracy: 0.9663 - loss: 0.1112 - val_accuracy: 0.9835 - val_loss: 0.0614\n",
      "Epoch 48/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9678 - loss: 0.1090\n",
      "Epoch 48: val_accuracy improved from 0.98350 to 0.98367, saving model to CNN_Models\\CNN_with_Dropout-lr_0.001-bs_64-20250917-111455\\best_model_epoch-48_val_acc-0.9837.keras\n",
      "\n",
      "Epoch 48: val_accuracy improved from 0.98350 to 0.98367, saving model to wandb_models/CNN_with_Dropout-lr_0.001-bs_64-20250917-111455/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 17ms/step - accuracy: 0.9665 - loss: 0.1105 - val_accuracy: 0.9837 - val_loss: 0.0605\n",
      "Epoch 49/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9684 - loss: 0.1034\n",
      "Epoch 49: val_accuracy improved from 0.98367 to 0.98383, saving model to CNN_Models\\CNN_with_Dropout-lr_0.001-bs_64-20250917-111455\\best_model_epoch-49_val_acc-0.9838.keras\n",
      "\n",
      "Epoch 49: val_accuracy improved from 0.98367 to 0.98383, saving model to wandb_models/CNN_with_Dropout-lr_0.001-bs_64-20250917-111455/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 18ms/step - accuracy: 0.9666 - loss: 0.1072 - val_accuracy: 0.9838 - val_loss: 0.0603\n",
      "Epoch 50/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9679 - loss: 0.1064\n",
      "Epoch 50: val_accuracy did not improve from 0.98383\n",
      "\n",
      "Epoch 50: val_accuracy did not improve from 0.98383\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 19ms/step - accuracy: 0.9675 - loss: 0.1069 - val_accuracy: 0.9835 - val_loss: 0.0596\n",
      "Epoch 51/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9678 - loss: 0.1049\n",
      "Epoch 51: val_accuracy did not improve from 0.98383\n",
      "\n",
      "Epoch 51: val_accuracy did not improve from 0.98383\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 19ms/step - accuracy: 0.9673 - loss: 0.1084 - val_accuracy: 0.9835 - val_loss: 0.0585\n",
      "Epoch 52/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9682 - loss: 0.1020\n",
      "Epoch 52: val_accuracy improved from 0.98383 to 0.98400, saving model to CNN_Models\\CNN_with_Dropout-lr_0.001-bs_64-20250917-111455\\best_model_epoch-52_val_acc-0.9840.keras\n",
      "\n",
      "Epoch 52: val_accuracy improved from 0.98383 to 0.98400, saving model to wandb_models/CNN_with_Dropout-lr_0.001-bs_64-20250917-111455/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 20ms/step - accuracy: 0.9680 - loss: 0.1033 - val_accuracy: 0.9840 - val_loss: 0.0583\n",
      "Epoch 53/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9682 - loss: 0.1039\n",
      "Epoch 53: val_accuracy improved from 0.98400 to 0.98450, saving model to CNN_Models\\CNN_with_Dropout-lr_0.001-bs_64-20250917-111455\\best_model_epoch-53_val_acc-0.9845.keras\n",
      "\n",
      "Epoch 53: val_accuracy improved from 0.98400 to 0.98450, saving model to wandb_models/CNN_with_Dropout-lr_0.001-bs_64-20250917-111455/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 18ms/step - accuracy: 0.9673 - loss: 0.1061 - val_accuracy: 0.9845 - val_loss: 0.0579\n",
      "Epoch 54/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9698 - loss: 0.1014\n",
      "Epoch 54: val_accuracy improved from 0.98450 to 0.98467, saving model to CNN_Models\\CNN_with_Dropout-lr_0.001-bs_64-20250917-111455\\best_model_epoch-54_val_acc-0.9847.keras\n",
      "\n",
      "Epoch 54: val_accuracy improved from 0.98450 to 0.98467, saving model to wandb_models/CNN_with_Dropout-lr_0.001-bs_64-20250917-111455/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 15ms/step - accuracy: 0.9689 - loss: 0.1018 - val_accuracy: 0.9847 - val_loss: 0.0576\n",
      "Epoch 55/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9691 - loss: 0.1026\n",
      "Epoch 55: val_accuracy did not improve from 0.98467\n",
      "\n",
      "Epoch 55: val_accuracy did not improve from 0.98467\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.9685 - loss: 0.1025 - val_accuracy: 0.9847 - val_loss: 0.0569\n",
      "Epoch 56/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9708 - loss: 0.0968\n",
      "Epoch 56: val_accuracy improved from 0.98467 to 0.98483, saving model to CNN_Models\\CNN_with_Dropout-lr_0.001-bs_64-20250917-111455\\best_model_epoch-56_val_acc-0.9848.keras\n",
      "\n",
      "Epoch 56: val_accuracy improved from 0.98467 to 0.98483, saving model to wandb_models/CNN_with_Dropout-lr_0.001-bs_64-20250917-111455/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 15ms/step - accuracy: 0.9700 - loss: 0.0987 - val_accuracy: 0.9848 - val_loss: 0.0563\n",
      "Epoch 57/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9691 - loss: 0.0989\n",
      "Epoch 57: val_accuracy did not improve from 0.98483\n",
      "\n",
      "Epoch 57: val_accuracy did not improve from 0.98483\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.9688 - loss: 0.1005 - val_accuracy: 0.9847 - val_loss: 0.0557\n",
      "Epoch 58/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9706 - loss: 0.0966\n",
      "Epoch 58: val_accuracy improved from 0.98483 to 0.98517, saving model to CNN_Models\\CNN_with_Dropout-lr_0.001-bs_64-20250917-111455\\best_model_epoch-58_val_acc-0.9852.keras\n",
      "\n",
      "Epoch 58: val_accuracy improved from 0.98483 to 0.98517, saving model to wandb_models/CNN_with_Dropout-lr_0.001-bs_64-20250917-111455/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 25ms/step - accuracy: 0.9695 - loss: 0.0992 - val_accuracy: 0.9852 - val_loss: 0.0553\n",
      "Epoch 59/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9705 - loss: 0.0945\n",
      "Epoch 59: val_accuracy did not improve from 0.98517\n",
      "\n",
      "Epoch 59: val_accuracy did not improve from 0.98517\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 17ms/step - accuracy: 0.9693 - loss: 0.0978 - val_accuracy: 0.9848 - val_loss: 0.0550\n",
      "Epoch 60/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9710 - loss: 0.0953\n",
      "Epoch 60: val_accuracy did not improve from 0.98517\n",
      "\n",
      "Epoch 60: val_accuracy did not improve from 0.98517\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 15ms/step - accuracy: 0.9696 - loss: 0.0977 - val_accuracy: 0.9852 - val_loss: 0.0546\n",
      "Epoch 61/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9701 - loss: 0.0979\n",
      "Epoch 61: val_accuracy did not improve from 0.98517\n",
      "\n",
      "Epoch 61: val_accuracy did not improve from 0.98517\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 19ms/step - accuracy: 0.9696 - loss: 0.0977 - val_accuracy: 0.9852 - val_loss: 0.0540\n",
      "Epoch 62/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9714 - loss: 0.0938\n",
      "Epoch 62: val_accuracy did not improve from 0.98517\n",
      "\n",
      "Epoch 62: val_accuracy did not improve from 0.98517\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 22ms/step - accuracy: 0.9705 - loss: 0.0944 - val_accuracy: 0.9850 - val_loss: 0.0535\n",
      "Epoch 63/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9716 - loss: 0.0937\n",
      "Epoch 63: val_accuracy improved from 0.98517 to 0.98533, saving model to CNN_Models\\CNN_with_Dropout-lr_0.001-bs_64-20250917-111455\\best_model_epoch-63_val_acc-0.9853.keras\n",
      "\n",
      "Epoch 63: val_accuracy improved from 0.98517 to 0.98533, saving model to wandb_models/CNN_with_Dropout-lr_0.001-bs_64-20250917-111455/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 16ms/step - accuracy: 0.9701 - loss: 0.0959 - val_accuracy: 0.9853 - val_loss: 0.0537\n",
      "Epoch 64/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9720 - loss: 0.0910\n",
      "Epoch 64: val_accuracy improved from 0.98533 to 0.98567, saving model to CNN_Models\\CNN_with_Dropout-lr_0.001-bs_64-20250917-111455\\best_model_epoch-64_val_acc-0.9857.keras\n",
      "\n",
      "Epoch 64: val_accuracy improved from 0.98533 to 0.98567, saving model to wandb_models/CNN_with_Dropout-lr_0.001-bs_64-20250917-111455/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.9712 - loss: 0.0930 - val_accuracy: 0.9857 - val_loss: 0.0528\n",
      "Epoch 65/100\n",
      "\u001b[1m840/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9723 - loss: 0.0904\n",
      "Epoch 65: val_accuracy did not improve from 0.98567\n",
      "\n",
      "Epoch 65: val_accuracy did not improve from 0.98567\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 14ms/step - accuracy: 0.9720 - loss: 0.0919 - val_accuracy: 0.9857 - val_loss: 0.0525\n",
      "Epoch 66/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9712 - loss: 0.0894\n",
      "Epoch 66: val_accuracy did not improve from 0.98567\n",
      "\n",
      "Epoch 66: val_accuracy did not improve from 0.98567\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 19ms/step - accuracy: 0.9711 - loss: 0.0913 - val_accuracy: 0.9853 - val_loss: 0.0523\n",
      "Epoch 67/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9732 - loss: 0.0880\n",
      "Epoch 67: val_accuracy did not improve from 0.98567\n",
      "\n",
      "Epoch 67: val_accuracy did not improve from 0.98567\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 18ms/step - accuracy: 0.9717 - loss: 0.0907 - val_accuracy: 0.9855 - val_loss: 0.0521\n",
      "Epoch 68/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9727 - loss: 0.0883\n",
      "Epoch 68: val_accuracy improved from 0.98567 to 0.98583, saving model to CNN_Models\\CNN_with_Dropout-lr_0.001-bs_64-20250917-111455\\best_model_epoch-68_val_acc-0.9858.keras\n",
      "\n",
      "Epoch 68: val_accuracy improved from 0.98567 to 0.98583, saving model to wandb_models/CNN_with_Dropout-lr_0.001-bs_64-20250917-111455/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 17ms/step - accuracy: 0.9718 - loss: 0.0907 - val_accuracy: 0.9858 - val_loss: 0.0516\n",
      "Epoch 69/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9738 - loss: 0.0860\n",
      "Epoch 69: val_accuracy improved from 0.98583 to 0.98600, saving model to CNN_Models\\CNN_with_Dropout-lr_0.001-bs_64-20250917-111455\\best_model_epoch-69_val_acc-0.9860.keras\n",
      "\n",
      "Epoch 69: val_accuracy improved from 0.98583 to 0.98600, saving model to wandb_models/CNN_with_Dropout-lr_0.001-bs_64-20250917-111455/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 17ms/step - accuracy: 0.9720 - loss: 0.0891 - val_accuracy: 0.9860 - val_loss: 0.0513\n",
      "Epoch 70/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9739 - loss: 0.0865\n",
      "Epoch 70: val_accuracy improved from 0.98600 to 0.98633, saving model to CNN_Models\\CNN_with_Dropout-lr_0.001-bs_64-20250917-111455\\best_model_epoch-70_val_acc-0.9863.keras\n",
      "\n",
      "Epoch 70: val_accuracy improved from 0.98600 to 0.98633, saving model to wandb_models/CNN_with_Dropout-lr_0.001-bs_64-20250917-111455/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 16ms/step - accuracy: 0.9734 - loss: 0.0883 - val_accuracy: 0.9863 - val_loss: 0.0512\n",
      "Epoch 71/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9732 - loss: 0.0868\n",
      "Epoch 71: val_accuracy did not improve from 0.98633\n",
      "\n",
      "Epoch 71: val_accuracy did not improve from 0.98633\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 17ms/step - accuracy: 0.9729 - loss: 0.0875 - val_accuracy: 0.9863 - val_loss: 0.0505\n",
      "Epoch 72/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9729 - loss: 0.0875\n",
      "Epoch 72: val_accuracy improved from 0.98633 to 0.98650, saving model to CNN_Models\\CNN_with_Dropout-lr_0.001-bs_64-20250917-111455\\best_model_epoch-72_val_acc-0.9865.keras\n",
      "\n",
      "Epoch 72: val_accuracy improved from 0.98633 to 0.98650, saving model to wandb_models/CNN_with_Dropout-lr_0.001-bs_64-20250917-111455/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 17ms/step - accuracy: 0.9731 - loss: 0.0872 - val_accuracy: 0.9865 - val_loss: 0.0504\n",
      "Epoch 73/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9741 - loss: 0.0844\n",
      "Epoch 73: val_accuracy did not improve from 0.98650\n",
      "\n",
      "Epoch 73: val_accuracy did not improve from 0.98650\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 18ms/step - accuracy: 0.9737 - loss: 0.0848 - val_accuracy: 0.9862 - val_loss: 0.0498\n",
      "Epoch 74/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9757 - loss: 0.0824\n",
      "Epoch 74: val_accuracy did not improve from 0.98650\n",
      "\n",
      "Epoch 74: val_accuracy did not improve from 0.98650\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 18ms/step - accuracy: 0.9742 - loss: 0.0842 - val_accuracy: 0.9863 - val_loss: 0.0495\n",
      "Epoch 75/100\n",
      "\u001b[1m840/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9738 - loss: 0.0829\n",
      "Epoch 75: val_accuracy did not improve from 0.98650\n",
      "\n",
      "Epoch 75: val_accuracy did not improve from 0.98650\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 16ms/step - accuracy: 0.9735 - loss: 0.0846 - val_accuracy: 0.9862 - val_loss: 0.0499\n",
      "Epoch 76/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9735 - loss: 0.0864\n",
      "Epoch 76: val_accuracy improved from 0.98650 to 0.98667, saving model to CNN_Models\\CNN_with_Dropout-lr_0.001-bs_64-20250917-111455\\best_model_epoch-76_val_acc-0.9867.keras\n",
      "\n",
      "Epoch 76: val_accuracy improved from 0.98650 to 0.98667, saving model to wandb_models/CNN_with_Dropout-lr_0.001-bs_64-20250917-111455/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 17ms/step - accuracy: 0.9733 - loss: 0.0873 - val_accuracy: 0.9867 - val_loss: 0.0496\n",
      "Epoch 77/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9740 - loss: 0.0855\n",
      "Epoch 77: val_accuracy did not improve from 0.98667\n",
      "\n",
      "Epoch 77: val_accuracy did not improve from 0.98667\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 17ms/step - accuracy: 0.9730 - loss: 0.0869 - val_accuracy: 0.9865 - val_loss: 0.0493\n",
      "Epoch 78/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9751 - loss: 0.0821\n",
      "Epoch 78: val_accuracy did not improve from 0.98667\n",
      "\n",
      "Epoch 78: val_accuracy did not improve from 0.98667\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 16ms/step - accuracy: 0.9740 - loss: 0.0835 - val_accuracy: 0.9867 - val_loss: 0.0485\n",
      "Epoch 79/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9760 - loss: 0.0780\n",
      "Epoch 79: val_accuracy improved from 0.98667 to 0.98733, saving model to CNN_Models\\CNN_with_Dropout-lr_0.001-bs_64-20250917-111455\\best_model_epoch-79_val_acc-0.9873.keras\n",
      "\n",
      "Epoch 79: val_accuracy improved from 0.98667 to 0.98733, saving model to wandb_models/CNN_with_Dropout-lr_0.001-bs_64-20250917-111455/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 17ms/step - accuracy: 0.9748 - loss: 0.0821 - val_accuracy: 0.9873 - val_loss: 0.0484\n",
      "Epoch 80/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9760 - loss: 0.0806\n",
      "Epoch 80: val_accuracy did not improve from 0.98733\n",
      "\n",
      "Epoch 80: val_accuracy did not improve from 0.98733\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 18ms/step - accuracy: 0.9753 - loss: 0.0813 - val_accuracy: 0.9867 - val_loss: 0.0483\n",
      "Epoch 81/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9756 - loss: 0.0809\n",
      "Epoch 81: val_accuracy did not improve from 0.98733\n",
      "\n",
      "Epoch 81: val_accuracy did not improve from 0.98733\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 18ms/step - accuracy: 0.9750 - loss: 0.0819 - val_accuracy: 0.9873 - val_loss: 0.0477\n",
      "Epoch 82/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9760 - loss: 0.0797\n",
      "Epoch 82: val_accuracy improved from 0.98733 to 0.98750, saving model to CNN_Models\\CNN_with_Dropout-lr_0.001-bs_64-20250917-111455\\best_model_epoch-82_val_acc-0.9875.keras\n",
      "\n",
      "Epoch 82: val_accuracy improved from 0.98733 to 0.98750, saving model to wandb_models/CNN_with_Dropout-lr_0.001-bs_64-20250917-111455/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 26ms/step - accuracy: 0.9753 - loss: 0.0811 - val_accuracy: 0.9875 - val_loss: 0.0476\n",
      "Epoch 83/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9756 - loss: 0.0809\n",
      "Epoch 83: val_accuracy did not improve from 0.98750\n",
      "\n",
      "Epoch 83: val_accuracy did not improve from 0.98750\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 30ms/step - accuracy: 0.9748 - loss: 0.0816 - val_accuracy: 0.9872 - val_loss: 0.0474\n",
      "Epoch 84/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9758 - loss: 0.0774\n",
      "Epoch 84: val_accuracy did not improve from 0.98750\n",
      "\n",
      "Epoch 84: val_accuracy did not improve from 0.98750\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 29ms/step - accuracy: 0.9749 - loss: 0.0793 - val_accuracy: 0.9875 - val_loss: 0.0469\n",
      "Epoch 85/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9759 - loss: 0.0765\n",
      "Epoch 85: val_accuracy did not improve from 0.98750\n",
      "\n",
      "Epoch 85: val_accuracy did not improve from 0.98750\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 22ms/step - accuracy: 0.9747 - loss: 0.0792 - val_accuracy: 0.9875 - val_loss: 0.0468\n",
      "Epoch 86/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9765 - loss: 0.0763\n",
      "Epoch 86: val_accuracy did not improve from 0.98750\n",
      "\n",
      "Epoch 86: val_accuracy did not improve from 0.98750\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 19ms/step - accuracy: 0.9760 - loss: 0.0788 - val_accuracy: 0.9873 - val_loss: 0.0469\n",
      "Epoch 87/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9763 - loss: 0.0777\n",
      "Epoch 87: val_accuracy did not improve from 0.98750\n",
      "\n",
      "Epoch 87: val_accuracy did not improve from 0.98750\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 24ms/step - accuracy: 0.9756 - loss: 0.0780 - val_accuracy: 0.9875 - val_loss: 0.0464\n",
      "Epoch 88/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9767 - loss: 0.0772\n",
      "Epoch 88: val_accuracy improved from 0.98750 to 0.98767, saving model to CNN_Models\\CNN_with_Dropout-lr_0.001-bs_64-20250917-111455\\best_model_epoch-88_val_acc-0.9877.keras\n",
      "\n",
      "Epoch 88: val_accuracy improved from 0.98750 to 0.98767, saving model to wandb_models/CNN_with_Dropout-lr_0.001-bs_64-20250917-111455/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 22ms/step - accuracy: 0.9759 - loss: 0.0788 - val_accuracy: 0.9877 - val_loss: 0.0459\n",
      "Epoch 89/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9755 - loss: 0.0786\n",
      "Epoch 89: val_accuracy did not improve from 0.98767\n",
      "\n",
      "Epoch 89: val_accuracy did not improve from 0.98767\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 23ms/step - accuracy: 0.9757 - loss: 0.0793 - val_accuracy: 0.9873 - val_loss: 0.0458\n",
      "Epoch 90/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9765 - loss: 0.0769\n",
      "Epoch 90: val_accuracy did not improve from 0.98767\n",
      "\n",
      "Epoch 90: val_accuracy did not improve from 0.98767\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 23ms/step - accuracy: 0.9760 - loss: 0.0781 - val_accuracy: 0.9875 - val_loss: 0.0454\n",
      "Epoch 91/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9769 - loss: 0.0743\n",
      "Epoch 91: val_accuracy did not improve from 0.98767\n",
      "\n",
      "Epoch 91: val_accuracy did not improve from 0.98767\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 24ms/step - accuracy: 0.9757 - loss: 0.0768 - val_accuracy: 0.9877 - val_loss: 0.0456\n",
      "Epoch 92/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9768 - loss: 0.0749\n",
      "Epoch 92: val_accuracy improved from 0.98767 to 0.98850, saving model to CNN_Models\\CNN_with_Dropout-lr_0.001-bs_64-20250917-111455\\best_model_epoch-92_val_acc-0.9885.keras\n",
      "\n",
      "Epoch 92: val_accuracy improved from 0.98767 to 0.98850, saving model to wandb_models/CNN_with_Dropout-lr_0.001-bs_64-20250917-111455/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 29ms/step - accuracy: 0.9764 - loss: 0.0748 - val_accuracy: 0.9885 - val_loss: 0.0449\n",
      "Epoch 93/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9767 - loss: 0.0733\n",
      "Epoch 93: val_accuracy did not improve from 0.98850\n",
      "\n",
      "Epoch 93: val_accuracy did not improve from 0.98850\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 25ms/step - accuracy: 0.9760 - loss: 0.0750 - val_accuracy: 0.9880 - val_loss: 0.0454\n",
      "Epoch 94/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9769 - loss: 0.0736\n",
      "Epoch 94: val_accuracy did not improve from 0.98850\n",
      "\n",
      "Epoch 94: val_accuracy did not improve from 0.98850\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 21ms/step - accuracy: 0.9760 - loss: 0.0756 - val_accuracy: 0.9878 - val_loss: 0.0452\n",
      "Epoch 95/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9770 - loss: 0.0737\n",
      "Epoch 95: val_accuracy did not improve from 0.98850\n",
      "\n",
      "Epoch 95: val_accuracy did not improve from 0.98850\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 21ms/step - accuracy: 0.9765 - loss: 0.0754 - val_accuracy: 0.9878 - val_loss: 0.0450\n",
      "Epoch 96/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9766 - loss: 0.0740\n",
      "Epoch 96: val_accuracy did not improve from 0.98850\n",
      "\n",
      "Epoch 96: val_accuracy did not improve from 0.98850\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 29ms/step - accuracy: 0.9761 - loss: 0.0755 - val_accuracy: 0.9877 - val_loss: 0.0449\n",
      "Epoch 97/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9780 - loss: 0.0713\n",
      "Epoch 97: val_accuracy did not improve from 0.98850\n",
      "\n",
      "Epoch 97: val_accuracy did not improve from 0.98850\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 24ms/step - accuracy: 0.9778 - loss: 0.0729 - val_accuracy: 0.9875 - val_loss: 0.0440\n",
      "Epoch 98/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9793 - loss: 0.0691\n",
      "Epoch 98: val_accuracy did not improve from 0.98850\n",
      "\n",
      "Epoch 98: val_accuracy did not improve from 0.98850\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 26ms/step - accuracy: 0.9783 - loss: 0.0719 - val_accuracy: 0.9878 - val_loss: 0.0443\n",
      "Epoch 99/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9784 - loss: 0.0721\n",
      "Epoch 99: val_accuracy did not improve from 0.98850\n",
      "\n",
      "Epoch 99: val_accuracy did not improve from 0.98850\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 26ms/step - accuracy: 0.9779 - loss: 0.0732 - val_accuracy: 0.9878 - val_loss: 0.0439\n",
      "Epoch 100/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9785 - loss: 0.0685\n",
      "Epoch 100: val_accuracy did not improve from 0.98850\n",
      "\n",
      "Epoch 100: val_accuracy did not improve from 0.98850\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 29ms/step - accuracy: 0.9778 - loss: 0.0707 - val_accuracy: 0.9880 - val_loss: 0.0439\n",
      "\n",
      "Training history saved to: CNN_Models\\CNN_with_Dropout-lr_0.001-bs_64-20250917-111455\\training_history.pkl\n",
      "\n",
      "--- Peak Performance Summary ---\n",
      "Best validation accuracy:           0.9885\n",
      "Associated training accuracy:       0.9764\n",
      "Occurred at epoch:                  92\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch/accuracy</td><td>▁▅▆▇▇▇▇█████████████████████████████████</td></tr><tr><td>epoch/epoch</td><td>▁▁▁▁▁▂▂▂▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇█████</td></tr><tr><td>epoch/learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/loss</td><td>█▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/val_accuracy</td><td>▁▃▄▅▆▇▇▇▇▇▇▇▇▇▇█████████████████████████</td></tr><tr><td>epoch/val_loss</td><td>█▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch/accuracy</td><td>0.97776</td></tr><tr><td>epoch/epoch</td><td>99</td></tr><tr><td>epoch/learning_rate</td><td>0.001</td></tr><tr><td>epoch/loss</td><td>0.07066</td></tr><tr><td>epoch/val_accuracy</td><td>0.988</td></tr><tr><td>epoch/val_loss</td><td>0.0439</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">CNN_with_Dropout-lr_0.001-bs_64-20250917-111455</strong> at: <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/fhdhm21m' target=\"_blank\">https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/fhdhm21m</a><br> View project at: <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2' target=\"_blank\">https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2</a><br>Synced 5 W&B file(s), 0 media file(s), 108 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250917_111455-fhdhm21m\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Adapting the normalisation layer...\n",
      "Adaptation complete.\n",
      "\n",
      "\n",
      "--- Starting Experiment: CNN_with_Dropout ---\n",
      "\n",
      "--- Model Architecture ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"CNN_with_Dropout\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"CNN_with_Dropout\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ normalization_27                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Normalization</span>)                 │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_27 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1600</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1600</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_110 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">204,928</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_111 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,290</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ normalization_27                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m1\u001b[0m)      │             \u001b[38;5;34m3\u001b[0m │\n",
       "│ (\u001b[38;5;33mNormalization\u001b[0m)                 │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_14 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m320\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_14 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_15 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_15 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_27 (\u001b[38;5;33mFlatten\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1600\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1600\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_110 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m204,928\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_111 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │         \u001b[38;5;34m1,290\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">225,037</span> (879.05 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m225,037\u001b[0m (879.05 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">225,034</span> (879.04 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m225,034\u001b[0m (879.04 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3</span> (16.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m3\u001b[0m (16.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Hyperparameters ---\n",
      "optimiser           : SGD\n",
      "learning_rate       : 0.0001\n",
      "epochs              : 100\n",
      "batch_size          : 64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "creating run (0.2s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\TimVos\\VSC Projects\\CSE5ML\\Assessment 2\\wandb\\run-20250917_114229-b1b5ksk2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/b1b5ksk2' target=\"_blank\">CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229</a></strong> to <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2' target=\"_blank\">https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/b1b5ksk2' target=\"_blank\">https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/b1b5ksk2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.1320 - loss: 2.3339\n",
      "Epoch 1: val_accuracy improved from None to 0.23267, saving model to CNN_Models\\CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229\\best_model_epoch-01_val_acc-0.2327.keras\n",
      "\n",
      "Epoch 1: val_accuracy improved from None to 0.23267, saving model to wandb_models/CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 24ms/step - accuracy: 0.1432 - loss: 2.3052 - val_accuracy: 0.2327 - val_loss: 2.1991\n",
      "Epoch 2/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.1845 - loss: 2.2345\n",
      "Epoch 2: val_accuracy improved from 0.23267 to 0.36467, saving model to CNN_Models\\CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229\\best_model_epoch-02_val_acc-0.3647.keras\n",
      "\n",
      "Epoch 2: val_accuracy improved from 0.23267 to 0.36467, saving model to wandb_models/CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 19ms/step - accuracy: 0.1977 - loss: 2.2200 - val_accuracy: 0.3647 - val_loss: 2.1182\n",
      "Epoch 3/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.2415 - loss: 2.1663\n",
      "Epoch 3: val_accuracy improved from 0.36467 to 0.46750, saving model to CNN_Models\\CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229\\best_model_epoch-03_val_acc-0.4675.keras\n",
      "\n",
      "Epoch 3: val_accuracy improved from 0.36467 to 0.46750, saving model to wandb_models/CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 26ms/step - accuracy: 0.2571 - loss: 2.1497 - val_accuracy: 0.4675 - val_loss: 2.0364\n",
      "Epoch 4/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.3002 - loss: 2.0965\n",
      "Epoch 4: val_accuracy improved from 0.46750 to 0.54750, saving model to CNN_Models\\CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229\\best_model_epoch-04_val_acc-0.5475.keras\n",
      "\n",
      "Epoch 4: val_accuracy improved from 0.46750 to 0.54750, saving model to wandb_models/CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 22ms/step - accuracy: 0.3163 - loss: 2.0783 - val_accuracy: 0.5475 - val_loss: 1.9418\n",
      "Epoch 5/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.3618 - loss: 2.0162\n",
      "Epoch 5: val_accuracy improved from 0.54750 to 0.62467, saving model to CNN_Models\\CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229\\best_model_epoch-05_val_acc-0.6247.keras\n",
      "\n",
      "Epoch 5: val_accuracy improved from 0.54750 to 0.62467, saving model to wandb_models/CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 22ms/step - accuracy: 0.3752 - loss: 1.9945 - val_accuracy: 0.6247 - val_loss: 1.8275\n",
      "Epoch 6/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.4141 - loss: 1.9206\n",
      "Epoch 6: val_accuracy improved from 0.62467 to 0.66983, saving model to CNN_Models\\CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229\\best_model_epoch-06_val_acc-0.6698.keras\n",
      "\n",
      "Epoch 6: val_accuracy improved from 0.62467 to 0.66983, saving model to wandb_models/CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 27ms/step - accuracy: 0.4250 - loss: 1.8957 - val_accuracy: 0.6698 - val_loss: 1.6914\n",
      "Epoch 7/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.4544 - loss: 1.8136\n",
      "Epoch 7: val_accuracy improved from 0.66983 to 0.70400, saving model to CNN_Models\\CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229\\best_model_epoch-07_val_acc-0.7040.keras\n",
      "\n",
      "Epoch 7: val_accuracy improved from 0.66983 to 0.70400, saving model to wandb_models/CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 24ms/step - accuracy: 0.4672 - loss: 1.7837 - val_accuracy: 0.7040 - val_loss: 1.5360\n",
      "Epoch 8/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.5001 - loss: 1.6875\n",
      "Epoch 8: val_accuracy improved from 0.70400 to 0.73300, saving model to CNN_Models\\CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229\\best_model_epoch-08_val_acc-0.7330.keras\n",
      "\n",
      "Epoch 8: val_accuracy improved from 0.70400 to 0.73300, saving model to wandb_models/CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 19ms/step - accuracy: 0.5109 - loss: 1.6547 - val_accuracy: 0.7330 - val_loss: 1.3683\n",
      "Epoch 9/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.5315 - loss: 1.5592\n",
      "Epoch 9: val_accuracy improved from 0.73300 to 0.76150, saving model to CNN_Models\\CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229\\best_model_epoch-09_val_acc-0.7615.keras\n",
      "\n",
      "Epoch 9: val_accuracy improved from 0.73300 to 0.76150, saving model to wandb_models/CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 32ms/step - accuracy: 0.5399 - loss: 1.5269 - val_accuracy: 0.7615 - val_loss: 1.2043\n",
      "Epoch 10/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.5677 - loss: 1.4256\n",
      "Epoch 10: val_accuracy improved from 0.76150 to 0.78967, saving model to CNN_Models\\CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229\\best_model_epoch-10_val_acc-0.7897.keras\n",
      "\n",
      "Epoch 10: val_accuracy improved from 0.76150 to 0.78967, saving model to wandb_models/CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 30ms/step - accuracy: 0.5738 - loss: 1.3982 - val_accuracy: 0.7897 - val_loss: 1.0560\n",
      "Epoch 11/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.5904 - loss: 1.3215\n",
      "Epoch 11: val_accuracy improved from 0.78967 to 0.80983, saving model to CNN_Models\\CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229\\best_model_epoch-11_val_acc-0.8098.keras\n",
      "\n",
      "Epoch 11: val_accuracy improved from 0.78967 to 0.80983, saving model to wandb_models/CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 25ms/step - accuracy: 0.5971 - loss: 1.2958 - val_accuracy: 0.8098 - val_loss: 0.9318\n",
      "Epoch 12/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.6203 - loss: 1.2164\n",
      "Epoch 12: val_accuracy improved from 0.80983 to 0.82917, saving model to CNN_Models\\CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229\\best_model_epoch-12_val_acc-0.8292.keras\n",
      "\n",
      "Epoch 12: val_accuracy improved from 0.80983 to 0.82917, saving model to wandb_models/CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 20ms/step - accuracy: 0.6257 - loss: 1.1948 - val_accuracy: 0.8292 - val_loss: 0.8290\n",
      "Epoch 13/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.6433 - loss: 1.1288\n",
      "Epoch 13: val_accuracy improved from 0.82917 to 0.84100, saving model to CNN_Models\\CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229\\best_model_epoch-13_val_acc-0.8410.keras\n",
      "\n",
      "Epoch 13: val_accuracy improved from 0.82917 to 0.84100, saving model to wandb_models/CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 24ms/step - accuracy: 0.6486 - loss: 1.1107 - val_accuracy: 0.8410 - val_loss: 0.7455\n",
      "Epoch 14/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.6647 - loss: 1.0555\n",
      "Epoch 14: val_accuracy improved from 0.84100 to 0.85000, saving model to CNN_Models\\CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229\\best_model_epoch-14_val_acc-0.8500.keras\n",
      "\n",
      "Epoch 14: val_accuracy improved from 0.84100 to 0.85000, saving model to wandb_models/CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 25ms/step - accuracy: 0.6698 - loss: 1.0400 - val_accuracy: 0.8500 - val_loss: 0.6768\n",
      "Epoch 15/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.6822 - loss: 0.9916\n",
      "Epoch 15: val_accuracy improved from 0.85000 to 0.85667, saving model to CNN_Models\\CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229\\best_model_epoch-15_val_acc-0.8567.keras\n",
      "\n",
      "Epoch 15: val_accuracy improved from 0.85000 to 0.85667, saving model to wandb_models/CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 23ms/step - accuracy: 0.6854 - loss: 0.9794 - val_accuracy: 0.8567 - val_loss: 0.6214\n",
      "Epoch 16/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.7030 - loss: 0.9344\n",
      "Epoch 16: val_accuracy improved from 0.85667 to 0.86717, saving model to CNN_Models\\CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229\\best_model_epoch-16_val_acc-0.8672.keras\n",
      "\n",
      "Epoch 16: val_accuracy improved from 0.85667 to 0.86717, saving model to wandb_models/CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 22ms/step - accuracy: 0.7054 - loss: 0.9237 - val_accuracy: 0.8672 - val_loss: 0.5738\n",
      "Epoch 17/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7112 - loss: 0.8906\n",
      "Epoch 17: val_accuracy improved from 0.86717 to 0.87350, saving model to CNN_Models\\CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229\\best_model_epoch-17_val_acc-0.8735.keras\n",
      "\n",
      "Epoch 17: val_accuracy improved from 0.86717 to 0.87350, saving model to wandb_models/CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 18ms/step - accuracy: 0.7153 - loss: 0.8794 - val_accuracy: 0.8735 - val_loss: 0.5348\n",
      "Epoch 18/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7287 - loss: 0.8442\n",
      "Epoch 18: val_accuracy improved from 0.87350 to 0.87850, saving model to CNN_Models\\CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229\\best_model_epoch-18_val_acc-0.8785.keras\n",
      "\n",
      "Epoch 18: val_accuracy improved from 0.87350 to 0.87850, saving model to wandb_models/CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 19ms/step - accuracy: 0.7327 - loss: 0.8330 - val_accuracy: 0.8785 - val_loss: 0.5013\n",
      "Epoch 19/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.7408 - loss: 0.8055\n",
      "Epoch 19: val_accuracy improved from 0.87850 to 0.88483, saving model to CNN_Models\\CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229\\best_model_epoch-19_val_acc-0.8848.keras\n",
      "\n",
      "Epoch 19: val_accuracy improved from 0.87850 to 0.88483, saving model to wandb_models/CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 20ms/step - accuracy: 0.7436 - loss: 0.7996 - val_accuracy: 0.8848 - val_loss: 0.4728\n",
      "Epoch 20/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.7539 - loss: 0.7701\n",
      "Epoch 20: val_accuracy improved from 0.88483 to 0.88867, saving model to CNN_Models\\CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229\\best_model_epoch-20_val_acc-0.8887.keras\n",
      "\n",
      "Epoch 20: val_accuracy improved from 0.88483 to 0.88867, saving model to wandb_models/CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 26ms/step - accuracy: 0.7557 - loss: 0.7630 - val_accuracy: 0.8887 - val_loss: 0.4482\n",
      "Epoch 21/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.7609 - loss: 0.7420\n",
      "Epoch 21: val_accuracy improved from 0.88867 to 0.89483, saving model to CNN_Models\\CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229\\best_model_epoch-21_val_acc-0.8948.keras\n",
      "\n",
      "Epoch 21: val_accuracy improved from 0.88867 to 0.89483, saving model to wandb_models/CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 22ms/step - accuracy: 0.7652 - loss: 0.7331 - val_accuracy: 0.8948 - val_loss: 0.4250\n",
      "Epoch 22/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7740 - loss: 0.7105\n",
      "Epoch 22: val_accuracy improved from 0.89483 to 0.89850, saving model to CNN_Models\\CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229\\best_model_epoch-22_val_acc-0.8985.keras\n",
      "\n",
      "Epoch 22: val_accuracy improved from 0.89483 to 0.89850, saving model to wandb_models/CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 18ms/step - accuracy: 0.7742 - loss: 0.7083 - val_accuracy: 0.8985 - val_loss: 0.4065\n",
      "Epoch 23/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.7826 - loss: 0.6866\n",
      "Epoch 23: val_accuracy improved from 0.89850 to 0.90250, saving model to CNN_Models\\CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229\\best_model_epoch-23_val_acc-0.9025.keras\n",
      "\n",
      "Epoch 23: val_accuracy improved from 0.89850 to 0.90250, saving model to wandb_models/CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 20ms/step - accuracy: 0.7849 - loss: 0.6822 - val_accuracy: 0.9025 - val_loss: 0.3891\n",
      "Epoch 24/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7880 - loss: 0.6671\n",
      "Epoch 24: val_accuracy improved from 0.90250 to 0.90550, saving model to CNN_Models\\CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229\\best_model_epoch-24_val_acc-0.9055.keras\n",
      "\n",
      "Epoch 24: val_accuracy improved from 0.90250 to 0.90550, saving model to wandb_models/CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 18ms/step - accuracy: 0.7885 - loss: 0.6620 - val_accuracy: 0.9055 - val_loss: 0.3736\n",
      "Epoch 25/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7984 - loss: 0.6404\n",
      "Epoch 25: val_accuracy improved from 0.90550 to 0.90783, saving model to CNN_Models\\CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229\\best_model_epoch-25_val_acc-0.9078.keras\n",
      "\n",
      "Epoch 25: val_accuracy improved from 0.90550 to 0.90783, saving model to wandb_models/CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 17ms/step - accuracy: 0.7986 - loss: 0.6358 - val_accuracy: 0.9078 - val_loss: 0.3600\n",
      "Epoch 26/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8047 - loss: 0.6177\n",
      "Epoch 26: val_accuracy improved from 0.90783 to 0.91100, saving model to CNN_Models\\CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229\\best_model_epoch-26_val_acc-0.9110.keras\n",
      "\n",
      "Epoch 26: val_accuracy improved from 0.90783 to 0.91100, saving model to wandb_models/CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 18ms/step - accuracy: 0.8071 - loss: 0.6145 - val_accuracy: 0.9110 - val_loss: 0.3468\n",
      "Epoch 27/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8085 - loss: 0.6065\n",
      "Epoch 27: val_accuracy improved from 0.91100 to 0.91267, saving model to CNN_Models\\CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229\\best_model_epoch-27_val_acc-0.9127.keras\n",
      "\n",
      "Epoch 27: val_accuracy improved from 0.91100 to 0.91267, saving model to wandb_models/CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 17ms/step - accuracy: 0.8098 - loss: 0.6023 - val_accuracy: 0.9127 - val_loss: 0.3347\n",
      "Epoch 28/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8150 - loss: 0.5866\n",
      "Epoch 28: val_accuracy improved from 0.91267 to 0.91483, saving model to CNN_Models\\CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229\\best_model_epoch-28_val_acc-0.9148.keras\n",
      "\n",
      "Epoch 28: val_accuracy improved from 0.91267 to 0.91483, saving model to wandb_models/CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 19ms/step - accuracy: 0.8177 - loss: 0.5825 - val_accuracy: 0.9148 - val_loss: 0.3247\n",
      "Epoch 29/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8201 - loss: 0.5698\n",
      "Epoch 29: val_accuracy improved from 0.91483 to 0.91650, saving model to CNN_Models\\CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229\\best_model_epoch-29_val_acc-0.9165.keras\n",
      "\n",
      "Epoch 29: val_accuracy improved from 0.91483 to 0.91650, saving model to wandb_models/CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 19ms/step - accuracy: 0.8226 - loss: 0.5645 - val_accuracy: 0.9165 - val_loss: 0.3155\n",
      "Epoch 30/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8226 - loss: 0.5569\n",
      "Epoch 30: val_accuracy improved from 0.91650 to 0.91917, saving model to CNN_Models\\CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229\\best_model_epoch-30_val_acc-0.9192.keras\n",
      "\n",
      "Epoch 30: val_accuracy improved from 0.91650 to 0.91917, saving model to wandb_models/CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 18ms/step - accuracy: 0.8247 - loss: 0.5522 - val_accuracy: 0.9192 - val_loss: 0.3061\n",
      "Epoch 31/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8296 - loss: 0.5406\n",
      "Epoch 31: val_accuracy improved from 0.91917 to 0.92217, saving model to CNN_Models\\CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229\\best_model_epoch-31_val_acc-0.9222.keras\n",
      "\n",
      "Epoch 31: val_accuracy improved from 0.91917 to 0.92217, saving model to wandb_models/CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 18ms/step - accuracy: 0.8307 - loss: 0.5376 - val_accuracy: 0.9222 - val_loss: 0.2977\n",
      "Epoch 32/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8346 - loss: 0.5281\n",
      "Epoch 32: val_accuracy improved from 0.92217 to 0.92250, saving model to CNN_Models\\CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229\\best_model_epoch-32_val_acc-0.9225.keras\n",
      "\n",
      "Epoch 32: val_accuracy improved from 0.92217 to 0.92250, saving model to wandb_models/CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 17ms/step - accuracy: 0.8351 - loss: 0.5268 - val_accuracy: 0.9225 - val_loss: 0.2902\n",
      "Epoch 33/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8363 - loss: 0.5198\n",
      "Epoch 33: val_accuracy improved from 0.92250 to 0.92417, saving model to CNN_Models\\CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229\\best_model_epoch-33_val_acc-0.9242.keras\n",
      "\n",
      "Epoch 33: val_accuracy improved from 0.92250 to 0.92417, saving model to wandb_models/CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 18ms/step - accuracy: 0.8384 - loss: 0.5178 - val_accuracy: 0.9242 - val_loss: 0.2830\n",
      "Epoch 34/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8434 - loss: 0.5021\n",
      "Epoch 34: val_accuracy improved from 0.92417 to 0.92517, saving model to CNN_Models\\CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229\\best_model_epoch-34_val_acc-0.9252.keras\n",
      "\n",
      "Epoch 34: val_accuracy improved from 0.92417 to 0.92517, saving model to wandb_models/CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 19ms/step - accuracy: 0.8423 - loss: 0.5014 - val_accuracy: 0.9252 - val_loss: 0.2762\n",
      "Epoch 35/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8448 - loss: 0.5026\n",
      "Epoch 35: val_accuracy improved from 0.92517 to 0.92750, saving model to CNN_Models\\CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229\\best_model_epoch-35_val_acc-0.9275.keras\n",
      "\n",
      "Epoch 35: val_accuracy improved from 0.92517 to 0.92750, saving model to wandb_models/CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 17ms/step - accuracy: 0.8453 - loss: 0.4989 - val_accuracy: 0.9275 - val_loss: 0.2704\n",
      "Epoch 36/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.8487 - loss: 0.4845\n",
      "Epoch 36: val_accuracy improved from 0.92750 to 0.92817, saving model to CNN_Models\\CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229\\best_model_epoch-36_val_acc-0.9282.keras\n",
      "\n",
      "Epoch 36: val_accuracy improved from 0.92750 to 0.92817, saving model to wandb_models/CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 21ms/step - accuracy: 0.8507 - loss: 0.4827 - val_accuracy: 0.9282 - val_loss: 0.2643\n",
      "Epoch 37/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8507 - loss: 0.4799\n",
      "Epoch 37: val_accuracy improved from 0.92817 to 0.92883, saving model to CNN_Models\\CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229\\best_model_epoch-37_val_acc-0.9288.keras\n",
      "\n",
      "Epoch 37: val_accuracy improved from 0.92817 to 0.92883, saving model to wandb_models/CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 17ms/step - accuracy: 0.8512 - loss: 0.4760 - val_accuracy: 0.9288 - val_loss: 0.2589\n",
      "Epoch 38/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8529 - loss: 0.4669\n",
      "Epoch 38: val_accuracy improved from 0.92883 to 0.93100, saving model to CNN_Models\\CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229\\best_model_epoch-38_val_acc-0.9310.keras\n",
      "\n",
      "Epoch 38: val_accuracy improved from 0.92883 to 0.93100, saving model to wandb_models/CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 18ms/step - accuracy: 0.8551 - loss: 0.4639 - val_accuracy: 0.9310 - val_loss: 0.2531\n",
      "Epoch 39/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8560 - loss: 0.4600\n",
      "Epoch 39: val_accuracy improved from 0.93100 to 0.93183, saving model to CNN_Models\\CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229\\best_model_epoch-39_val_acc-0.9318.keras\n",
      "\n",
      "Epoch 39: val_accuracy improved from 0.93100 to 0.93183, saving model to wandb_models/CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 17ms/step - accuracy: 0.8563 - loss: 0.4582 - val_accuracy: 0.9318 - val_loss: 0.2488\n",
      "Epoch 40/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8580 - loss: 0.4516\n",
      "Epoch 40: val_accuracy improved from 0.93183 to 0.93417, saving model to CNN_Models\\CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229\\best_model_epoch-40_val_acc-0.9342.keras\n",
      "\n",
      "Epoch 40: val_accuracy improved from 0.93183 to 0.93417, saving model to wandb_models/CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 17ms/step - accuracy: 0.8591 - loss: 0.4505 - val_accuracy: 0.9342 - val_loss: 0.2442\n",
      "Epoch 41/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8610 - loss: 0.4419\n",
      "Epoch 41: val_accuracy improved from 0.93417 to 0.93483, saving model to CNN_Models\\CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229\\best_model_epoch-41_val_acc-0.9348.keras\n",
      "\n",
      "Epoch 41: val_accuracy improved from 0.93417 to 0.93483, saving model to wandb_models/CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 17ms/step - accuracy: 0.8624 - loss: 0.4392 - val_accuracy: 0.9348 - val_loss: 0.2395\n",
      "Epoch 42/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8636 - loss: 0.4359\n",
      "Epoch 42: val_accuracy improved from 0.93483 to 0.93633, saving model to CNN_Models\\CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229\\best_model_epoch-42_val_acc-0.9363.keras\n",
      "\n",
      "Epoch 42: val_accuracy improved from 0.93483 to 0.93633, saving model to wandb_models/CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 18ms/step - accuracy: 0.8649 - loss: 0.4317 - val_accuracy: 0.9363 - val_loss: 0.2353\n",
      "Epoch 43/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8669 - loss: 0.4257\n",
      "Epoch 43: val_accuracy improved from 0.93633 to 0.93700, saving model to CNN_Models\\CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229\\best_model_epoch-43_val_acc-0.9370.keras\n",
      "\n",
      "Epoch 43: val_accuracy improved from 0.93633 to 0.93700, saving model to wandb_models/CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 17ms/step - accuracy: 0.8680 - loss: 0.4264 - val_accuracy: 0.9370 - val_loss: 0.2311\n",
      "Epoch 44/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8719 - loss: 0.4153\n",
      "Epoch 44: val_accuracy improved from 0.93700 to 0.93800, saving model to CNN_Models\\CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229\\best_model_epoch-44_val_acc-0.9380.keras\n",
      "\n",
      "Epoch 44: val_accuracy improved from 0.93700 to 0.93800, saving model to wandb_models/CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 18ms/step - accuracy: 0.8701 - loss: 0.4190 - val_accuracy: 0.9380 - val_loss: 0.2272\n",
      "Epoch 45/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8731 - loss: 0.4123\n",
      "Epoch 45: val_accuracy did not improve from 0.93800\n",
      "\n",
      "Epoch 45: val_accuracy did not improve from 0.93800\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 18ms/step - accuracy: 0.8724 - loss: 0.4125 - val_accuracy: 0.9377 - val_loss: 0.2238\n",
      "Epoch 46/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8743 - loss: 0.4071\n",
      "Epoch 46: val_accuracy improved from 0.93800 to 0.93917, saving model to CNN_Models\\CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229\\best_model_epoch-46_val_acc-0.9392.keras\n",
      "\n",
      "Epoch 46: val_accuracy improved from 0.93800 to 0.93917, saving model to wandb_models/CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 18ms/step - accuracy: 0.8734 - loss: 0.4072 - val_accuracy: 0.9392 - val_loss: 0.2205\n",
      "Epoch 47/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8725 - loss: 0.4023\n",
      "Epoch 47: val_accuracy improved from 0.93917 to 0.94017, saving model to CNN_Models\\CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229\\best_model_epoch-47_val_acc-0.9402.keras\n",
      "\n",
      "Epoch 47: val_accuracy improved from 0.93917 to 0.94017, saving model to wandb_models/CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 18ms/step - accuracy: 0.8745 - loss: 0.4017 - val_accuracy: 0.9402 - val_loss: 0.2170\n",
      "Epoch 48/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8773 - loss: 0.3949\n",
      "Epoch 48: val_accuracy improved from 0.94017 to 0.94117, saving model to CNN_Models\\CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229\\best_model_epoch-48_val_acc-0.9412.keras\n",
      "\n",
      "Epoch 48: val_accuracy improved from 0.94017 to 0.94117, saving model to wandb_models/CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 19ms/step - accuracy: 0.8781 - loss: 0.3960 - val_accuracy: 0.9412 - val_loss: 0.2142\n",
      "Epoch 49/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8781 - loss: 0.3885\n",
      "Epoch 49: val_accuracy improved from 0.94117 to 0.94183, saving model to CNN_Models\\CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229\\best_model_epoch-49_val_acc-0.9418.keras\n",
      "\n",
      "Epoch 49: val_accuracy improved from 0.94117 to 0.94183, saving model to wandb_models/CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 17ms/step - accuracy: 0.8796 - loss: 0.3871 - val_accuracy: 0.9418 - val_loss: 0.2106\n",
      "Epoch 50/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8793 - loss: 0.3909\n",
      "Epoch 50: val_accuracy improved from 0.94183 to 0.94233, saving model to CNN_Models\\CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229\\best_model_epoch-50_val_acc-0.9423.keras\n",
      "\n",
      "Epoch 50: val_accuracy improved from 0.94183 to 0.94233, saving model to wandb_models/CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 17ms/step - accuracy: 0.8799 - loss: 0.3880 - val_accuracy: 0.9423 - val_loss: 0.2079\n",
      "Epoch 51/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.8829 - loss: 0.3822\n",
      "Epoch 51: val_accuracy improved from 0.94233 to 0.94250, saving model to CNN_Models\\CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229\\best_model_epoch-51_val_acc-0.9425.keras\n",
      "\n",
      "Epoch 51: val_accuracy improved from 0.94233 to 0.94250, saving model to wandb_models/CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 21ms/step - accuracy: 0.8832 - loss: 0.3811 - val_accuracy: 0.9425 - val_loss: 0.2057\n",
      "Epoch 52/100\n",
      "\u001b[1m840/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8815 - loss: 0.3748\n",
      "Epoch 52: val_accuracy improved from 0.94250 to 0.94417, saving model to CNN_Models\\CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229\\best_model_epoch-52_val_acc-0.9442.keras\n",
      "\n",
      "Epoch 52: val_accuracy improved from 0.94250 to 0.94417, saving model to wandb_models/CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 16ms/step - accuracy: 0.8828 - loss: 0.3753 - val_accuracy: 0.9442 - val_loss: 0.2025\n",
      "Epoch 53/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8833 - loss: 0.3723\n",
      "Epoch 53: val_accuracy improved from 0.94417 to 0.94517, saving model to CNN_Models\\CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229\\best_model_epoch-53_val_acc-0.9452.keras\n",
      "\n",
      "Epoch 53: val_accuracy improved from 0.94417 to 0.94517, saving model to wandb_models/CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 17ms/step - accuracy: 0.8837 - loss: 0.3723 - val_accuracy: 0.9452 - val_loss: 0.1999\n",
      "Epoch 54/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8869 - loss: 0.3649\n",
      "Epoch 54: val_accuracy did not improve from 0.94517\n",
      "\n",
      "Epoch 54: val_accuracy did not improve from 0.94517\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 17ms/step - accuracy: 0.8863 - loss: 0.3663 - val_accuracy: 0.9452 - val_loss: 0.1974\n",
      "Epoch 55/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.8881 - loss: 0.3607\n",
      "Epoch 55: val_accuracy improved from 0.94517 to 0.94600, saving model to CNN_Models\\CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229\\best_model_epoch-55_val_acc-0.9460.keras\n",
      "\n",
      "Epoch 55: val_accuracy improved from 0.94517 to 0.94600, saving model to wandb_models/CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 21ms/step - accuracy: 0.8881 - loss: 0.3619 - val_accuracy: 0.9460 - val_loss: 0.1952\n",
      "Epoch 56/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.8872 - loss: 0.3623\n",
      "Epoch 56: val_accuracy improved from 0.94600 to 0.94633, saving model to CNN_Models\\CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229\\best_model_epoch-56_val_acc-0.9463.keras\n",
      "\n",
      "Epoch 56: val_accuracy improved from 0.94600 to 0.94633, saving model to wandb_models/CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 30ms/step - accuracy: 0.8885 - loss: 0.3596 - val_accuracy: 0.9463 - val_loss: 0.1929\n",
      "Epoch 57/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.8906 - loss: 0.3514\n",
      "Epoch 57: val_accuracy improved from 0.94633 to 0.94700, saving model to CNN_Models\\CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229\\best_model_epoch-57_val_acc-0.9470.keras\n",
      "\n",
      "Epoch 57: val_accuracy improved from 0.94633 to 0.94700, saving model to wandb_models/CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 25ms/step - accuracy: 0.8899 - loss: 0.3519 - val_accuracy: 0.9470 - val_loss: 0.1907\n",
      "Epoch 58/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8921 - loss: 0.3503\n",
      "Epoch 58: val_accuracy improved from 0.94700 to 0.94750, saving model to CNN_Models\\CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229\\best_model_epoch-58_val_acc-0.9475.keras\n",
      "\n",
      "Epoch 58: val_accuracy improved from 0.94700 to 0.94750, saving model to wandb_models/CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 17ms/step - accuracy: 0.8915 - loss: 0.3508 - val_accuracy: 0.9475 - val_loss: 0.1886\n",
      "Epoch 59/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8914 - loss: 0.3473\n",
      "Epoch 59: val_accuracy improved from 0.94750 to 0.94783, saving model to CNN_Models\\CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229\\best_model_epoch-59_val_acc-0.9478.keras\n",
      "\n",
      "Epoch 59: val_accuracy improved from 0.94750 to 0.94783, saving model to wandb_models/CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 20ms/step - accuracy: 0.8926 - loss: 0.3441 - val_accuracy: 0.9478 - val_loss: 0.1862\n",
      "Epoch 60/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8945 - loss: 0.3435\n",
      "Epoch 60: val_accuracy improved from 0.94783 to 0.94817, saving model to CNN_Models\\CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229\\best_model_epoch-60_val_acc-0.9482.keras\n",
      "\n",
      "Epoch 60: val_accuracy improved from 0.94783 to 0.94817, saving model to wandb_models/CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 20ms/step - accuracy: 0.8941 - loss: 0.3436 - val_accuracy: 0.9482 - val_loss: 0.1844\n",
      "Epoch 61/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8973 - loss: 0.3383\n",
      "Epoch 61: val_accuracy improved from 0.94817 to 0.94833, saving model to CNN_Models\\CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229\\best_model_epoch-61_val_acc-0.9483.keras\n",
      "\n",
      "Epoch 61: val_accuracy improved from 0.94817 to 0.94833, saving model to wandb_models/CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 32ms/step - accuracy: 0.8964 - loss: 0.3384 - val_accuracy: 0.9483 - val_loss: 0.1826\n",
      "Epoch 62/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8952 - loss: 0.3347\n",
      "Epoch 62: val_accuracy improved from 0.94833 to 0.94900, saving model to CNN_Models\\CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229\\best_model_epoch-62_val_acc-0.9490.keras\n",
      "\n",
      "Epoch 62: val_accuracy improved from 0.94833 to 0.94900, saving model to wandb_models/CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 20ms/step - accuracy: 0.8966 - loss: 0.3341 - val_accuracy: 0.9490 - val_loss: 0.1808\n",
      "Epoch 63/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9001 - loss: 0.3308\n",
      "Epoch 63: val_accuracy improved from 0.94900 to 0.94950, saving model to CNN_Models\\CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229\\best_model_epoch-63_val_acc-0.9495.keras\n",
      "\n",
      "Epoch 63: val_accuracy improved from 0.94900 to 0.94950, saving model to wandb_models/CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 22ms/step - accuracy: 0.8986 - loss: 0.3320 - val_accuracy: 0.9495 - val_loss: 0.1789\n",
      "Epoch 64/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9015 - loss: 0.3252\n",
      "Epoch 64: val_accuracy improved from 0.94950 to 0.95067, saving model to CNN_Models\\CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229\\best_model_epoch-64_val_acc-0.9507.keras\n",
      "\n",
      "Epoch 64: val_accuracy improved from 0.94950 to 0.95067, saving model to wandb_models/CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 18ms/step - accuracy: 0.9003 - loss: 0.3276 - val_accuracy: 0.9507 - val_loss: 0.1771\n",
      "Epoch 65/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9025 - loss: 0.3213\n",
      "Epoch 65: val_accuracy improved from 0.95067 to 0.95083, saving model to CNN_Models\\CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229\\best_model_epoch-65_val_acc-0.9508.keras\n",
      "\n",
      "Epoch 65: val_accuracy improved from 0.95067 to 0.95083, saving model to wandb_models/CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 16ms/step - accuracy: 0.9014 - loss: 0.3226 - val_accuracy: 0.9508 - val_loss: 0.1753\n",
      "Epoch 66/100\n",
      "\u001b[1m840/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9005 - loss: 0.3212\n",
      "Epoch 66: val_accuracy improved from 0.95083 to 0.95100, saving model to CNN_Models\\CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229\\best_model_epoch-66_val_acc-0.9510.keras\n",
      "\n",
      "Epoch 66: val_accuracy improved from 0.95083 to 0.95100, saving model to wandb_models/CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 17ms/step - accuracy: 0.9004 - loss: 0.3230 - val_accuracy: 0.9510 - val_loss: 0.1738\n",
      "Epoch 67/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9004 - loss: 0.3194\n",
      "Epoch 67: val_accuracy improved from 0.95100 to 0.95150, saving model to CNN_Models\\CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229\\best_model_epoch-67_val_acc-0.9515.keras\n",
      "\n",
      "Epoch 67: val_accuracy improved from 0.95100 to 0.95150, saving model to wandb_models/CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 17ms/step - accuracy: 0.9000 - loss: 0.3210 - val_accuracy: 0.9515 - val_loss: 0.1720\n",
      "Epoch 68/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9043 - loss: 0.3131\n",
      "Epoch 68: val_accuracy improved from 0.95150 to 0.95200, saving model to CNN_Models\\CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229\\best_model_epoch-68_val_acc-0.9520.keras\n",
      "\n",
      "Epoch 68: val_accuracy improved from 0.95150 to 0.95200, saving model to wandb_models/CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 18ms/step - accuracy: 0.9036 - loss: 0.3148 - val_accuracy: 0.9520 - val_loss: 0.1703\n",
      "Epoch 69/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9039 - loss: 0.3147\n",
      "Epoch 69: val_accuracy improved from 0.95200 to 0.95250, saving model to CNN_Models\\CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229\\best_model_epoch-69_val_acc-0.9525.keras\n",
      "\n",
      "Epoch 69: val_accuracy improved from 0.95200 to 0.95250, saving model to wandb_models/CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.9029 - loss: 0.3153 - val_accuracy: 0.9525 - val_loss: 0.1687\n",
      "Epoch 70/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9053 - loss: 0.3048\n",
      "Epoch 70: val_accuracy did not improve from 0.95250\n",
      "\n",
      "Epoch 70: val_accuracy did not improve from 0.95250\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 18ms/step - accuracy: 0.9037 - loss: 0.3079 - val_accuracy: 0.9525 - val_loss: 0.1674\n",
      "Epoch 71/100\n",
      "\u001b[1m840/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9076 - loss: 0.3041\n",
      "Epoch 71: val_accuracy improved from 0.95250 to 0.95333, saving model to CNN_Models\\CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229\\best_model_epoch-71_val_acc-0.9533.keras\n",
      "\n",
      "Epoch 71: val_accuracy improved from 0.95250 to 0.95333, saving model to wandb_models/CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.9046 - loss: 0.3080 - val_accuracy: 0.9533 - val_loss: 0.1656\n",
      "Epoch 72/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9056 - loss: 0.3044\n",
      "Epoch 72: val_accuracy improved from 0.95333 to 0.95367, saving model to CNN_Models\\CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229\\best_model_epoch-72_val_acc-0.9537.keras\n",
      "\n",
      "Epoch 72: val_accuracy improved from 0.95333 to 0.95367, saving model to wandb_models/CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 16ms/step - accuracy: 0.9064 - loss: 0.3035 - val_accuracy: 0.9537 - val_loss: 0.1644\n",
      "Epoch 73/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9059 - loss: 0.3025\n",
      "Epoch 73: val_accuracy did not improve from 0.95367\n",
      "\n",
      "Epoch 73: val_accuracy did not improve from 0.95367\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.9061 - loss: 0.3020 - val_accuracy: 0.9537 - val_loss: 0.1632\n",
      "Epoch 74/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9082 - loss: 0.2987\n",
      "Epoch 74: val_accuracy did not improve from 0.95367\n",
      "\n",
      "Epoch 74: val_accuracy did not improve from 0.95367\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.9079 - loss: 0.2998 - val_accuracy: 0.9535 - val_loss: 0.1615\n",
      "Epoch 75/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9099 - loss: 0.2934\n",
      "Epoch 75: val_accuracy improved from 0.95367 to 0.95400, saving model to CNN_Models\\CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229\\best_model_epoch-75_val_acc-0.9540.keras\n",
      "\n",
      "Epoch 75: val_accuracy improved from 0.95367 to 0.95400, saving model to wandb_models/CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.9094 - loss: 0.2952 - val_accuracy: 0.9540 - val_loss: 0.1599\n",
      "Epoch 76/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9101 - loss: 0.2926\n",
      "Epoch 76: val_accuracy improved from 0.95400 to 0.95467, saving model to CNN_Models\\CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229\\best_model_epoch-76_val_acc-0.9547.keras\n",
      "\n",
      "Epoch 76: val_accuracy improved from 0.95400 to 0.95467, saving model to wandb_models/CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 16ms/step - accuracy: 0.9104 - loss: 0.2914 - val_accuracy: 0.9547 - val_loss: 0.1589\n",
      "Epoch 77/100\n",
      "\u001b[1m840/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9098 - loss: 0.2926\n",
      "Epoch 77: val_accuracy improved from 0.95467 to 0.95483, saving model to CNN_Models\\CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229\\best_model_epoch-77_val_acc-0.9548.keras\n",
      "\n",
      "Epoch 77: val_accuracy improved from 0.95467 to 0.95483, saving model to wandb_models/CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 17ms/step - accuracy: 0.9092 - loss: 0.2945 - val_accuracy: 0.9548 - val_loss: 0.1578\n",
      "Epoch 78/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9105 - loss: 0.2913\n",
      "Epoch 78: val_accuracy did not improve from 0.95483\n",
      "\n",
      "Epoch 78: val_accuracy did not improve from 0.95483\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.9102 - loss: 0.2918 - val_accuracy: 0.9547 - val_loss: 0.1564\n",
      "Epoch 79/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9119 - loss: 0.2859\n",
      "Epoch 79: val_accuracy improved from 0.95483 to 0.95500, saving model to CNN_Models\\CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229\\best_model_epoch-79_val_acc-0.9550.keras\n",
      "\n",
      "Epoch 79: val_accuracy improved from 0.95483 to 0.95500, saving model to wandb_models/CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 17ms/step - accuracy: 0.9110 - loss: 0.2867 - val_accuracy: 0.9550 - val_loss: 0.1551\n",
      "Epoch 80/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9112 - loss: 0.2864\n",
      "Epoch 80: val_accuracy improved from 0.95500 to 0.95550, saving model to CNN_Models\\CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229\\best_model_epoch-80_val_acc-0.9555.keras\n",
      "\n",
      "Epoch 80: val_accuracy improved from 0.95500 to 0.95550, saving model to wandb_models/CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 16ms/step - accuracy: 0.9117 - loss: 0.2861 - val_accuracy: 0.9555 - val_loss: 0.1539\n",
      "Epoch 81/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9135 - loss: 0.2790\n",
      "Epoch 81: val_accuracy improved from 0.95550 to 0.95583, saving model to CNN_Models\\CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229\\best_model_epoch-81_val_acc-0.9558.keras\n",
      "\n",
      "Epoch 81: val_accuracy improved from 0.95550 to 0.95583, saving model to wandb_models/CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 16ms/step - accuracy: 0.9141 - loss: 0.2819 - val_accuracy: 0.9558 - val_loss: 0.1531\n",
      "Epoch 82/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9137 - loss: 0.2790\n",
      "Epoch 82: val_accuracy improved from 0.95583 to 0.95617, saving model to CNN_Models\\CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229\\best_model_epoch-82_val_acc-0.9562.keras\n",
      "\n",
      "Epoch 82: val_accuracy improved from 0.95583 to 0.95617, saving model to wandb_models/CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 18ms/step - accuracy: 0.9136 - loss: 0.2813 - val_accuracy: 0.9562 - val_loss: 0.1519\n",
      "Epoch 83/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9129 - loss: 0.2795\n",
      "Epoch 83: val_accuracy improved from 0.95617 to 0.95667, saving model to CNN_Models\\CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229\\best_model_epoch-83_val_acc-0.9567.keras\n",
      "\n",
      "Epoch 83: val_accuracy improved from 0.95617 to 0.95667, saving model to wandb_models/CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 17ms/step - accuracy: 0.9137 - loss: 0.2804 - val_accuracy: 0.9567 - val_loss: 0.1508\n",
      "Epoch 84/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9171 - loss: 0.2732\n",
      "Epoch 84: val_accuracy improved from 0.95667 to 0.95717, saving model to CNN_Models\\CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229\\best_model_epoch-84_val_acc-0.9572.keras\n",
      "\n",
      "Epoch 84: val_accuracy improved from 0.95667 to 0.95717, saving model to wandb_models/CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 18ms/step - accuracy: 0.9155 - loss: 0.2766 - val_accuracy: 0.9572 - val_loss: 0.1495\n",
      "Epoch 85/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9157 - loss: 0.2755\n",
      "Epoch 85: val_accuracy improved from 0.95717 to 0.95767, saving model to CNN_Models\\CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229\\best_model_epoch-85_val_acc-0.9577.keras\n",
      "\n",
      "Epoch 85: val_accuracy improved from 0.95717 to 0.95767, saving model to wandb_models/CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 18ms/step - accuracy: 0.9156 - loss: 0.2762 - val_accuracy: 0.9577 - val_loss: 0.1486\n",
      "Epoch 86/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9155 - loss: 0.2735\n",
      "Epoch 86: val_accuracy improved from 0.95767 to 0.95783, saving model to CNN_Models\\CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229\\best_model_epoch-86_val_acc-0.9578.keras\n",
      "\n",
      "Epoch 86: val_accuracy improved from 0.95767 to 0.95783, saving model to wandb_models/CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 20ms/step - accuracy: 0.9153 - loss: 0.2743 - val_accuracy: 0.9578 - val_loss: 0.1477\n",
      "Epoch 87/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9176 - loss: 0.2702\n",
      "Epoch 87: val_accuracy improved from 0.95783 to 0.95833, saving model to CNN_Models\\CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229\\best_model_epoch-87_val_acc-0.9583.keras\n",
      "\n",
      "Epoch 87: val_accuracy improved from 0.95783 to 0.95833, saving model to wandb_models/CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 20ms/step - accuracy: 0.9175 - loss: 0.2716 - val_accuracy: 0.9583 - val_loss: 0.1468\n",
      "Epoch 88/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9163 - loss: 0.2675\n",
      "Epoch 88: val_accuracy improved from 0.95833 to 0.95883, saving model to CNN_Models\\CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229\\best_model_epoch-88_val_acc-0.9588.keras\n",
      "\n",
      "Epoch 88: val_accuracy improved from 0.95833 to 0.95883, saving model to wandb_models/CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 18ms/step - accuracy: 0.9168 - loss: 0.2697 - val_accuracy: 0.9588 - val_loss: 0.1459\n",
      "Epoch 89/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9167 - loss: 0.2665\n",
      "Epoch 89: val_accuracy did not improve from 0.95883\n",
      "\n",
      "Epoch 89: val_accuracy did not improve from 0.95883\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 19ms/step - accuracy: 0.9164 - loss: 0.2690 - val_accuracy: 0.9585 - val_loss: 0.1448\n",
      "Epoch 90/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9174 - loss: 0.2650\n",
      "Epoch 90: val_accuracy improved from 0.95883 to 0.95933, saving model to CNN_Models\\CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229\\best_model_epoch-90_val_acc-0.9593.keras\n",
      "\n",
      "Epoch 90: val_accuracy improved from 0.95883 to 0.95933, saving model to wandb_models/CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 17ms/step - accuracy: 0.9189 - loss: 0.2645 - val_accuracy: 0.9593 - val_loss: 0.1437\n",
      "Epoch 91/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9194 - loss: 0.2640\n",
      "Epoch 91: val_accuracy improved from 0.95933 to 0.95950, saving model to CNN_Models\\CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229\\best_model_epoch-91_val_acc-0.9595.keras\n",
      "\n",
      "Epoch 91: val_accuracy improved from 0.95933 to 0.95950, saving model to wandb_models/CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 17ms/step - accuracy: 0.9190 - loss: 0.2644 - val_accuracy: 0.9595 - val_loss: 0.1430\n",
      "Epoch 92/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9197 - loss: 0.2620\n",
      "Epoch 92: val_accuracy improved from 0.95950 to 0.96017, saving model to CNN_Models\\CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229\\best_model_epoch-92_val_acc-0.9602.keras\n",
      "\n",
      "Epoch 92: val_accuracy improved from 0.95950 to 0.96017, saving model to wandb_models/CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 18ms/step - accuracy: 0.9188 - loss: 0.2645 - val_accuracy: 0.9602 - val_loss: 0.1417\n",
      "Epoch 93/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9210 - loss: 0.2571\n",
      "Epoch 93: val_accuracy did not improve from 0.96017\n",
      "\n",
      "Epoch 93: val_accuracy did not improve from 0.96017\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 18ms/step - accuracy: 0.9202 - loss: 0.2612 - val_accuracy: 0.9600 - val_loss: 0.1411\n",
      "Epoch 94/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9228 - loss: 0.2558\n",
      "Epoch 94: val_accuracy did not improve from 0.96017\n",
      "\n",
      "Epoch 94: val_accuracy did not improve from 0.96017\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 17ms/step - accuracy: 0.9214 - loss: 0.2588 - val_accuracy: 0.9600 - val_loss: 0.1402\n",
      "Epoch 95/100\n",
      "\u001b[1m842/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9206 - loss: 0.2559\n",
      "Epoch 95: val_accuracy did not improve from 0.96017\n",
      "\n",
      "Epoch 95: val_accuracy did not improve from 0.96017\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 18ms/step - accuracy: 0.9216 - loss: 0.2581 - val_accuracy: 0.9600 - val_loss: 0.1393\n",
      "Epoch 96/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9234 - loss: 0.2532\n",
      "Epoch 96: val_accuracy improved from 0.96017 to 0.96050, saving model to CNN_Models\\CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229\\best_model_epoch-96_val_acc-0.9605.keras\n",
      "\n",
      "Epoch 96: val_accuracy improved from 0.96017 to 0.96050, saving model to wandb_models/CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 19ms/step - accuracy: 0.9226 - loss: 0.2545 - val_accuracy: 0.9605 - val_loss: 0.1384\n",
      "Epoch 97/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9239 - loss: 0.2489\n",
      "Epoch 97: val_accuracy improved from 0.96050 to 0.96133, saving model to CNN_Models\\CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229\\best_model_epoch-97_val_acc-0.9613.keras\n",
      "\n",
      "Epoch 97: val_accuracy improved from 0.96050 to 0.96133, saving model to wandb_models/CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 18ms/step - accuracy: 0.9229 - loss: 0.2497 - val_accuracy: 0.9613 - val_loss: 0.1375\n",
      "Epoch 98/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9226 - loss: 0.2527\n",
      "Epoch 98: val_accuracy improved from 0.96133 to 0.96183, saving model to CNN_Models\\CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229\\best_model_epoch-98_val_acc-0.9618.keras\n",
      "\n",
      "Epoch 98: val_accuracy improved from 0.96133 to 0.96183, saving model to wandb_models/CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 18ms/step - accuracy: 0.9225 - loss: 0.2541 - val_accuracy: 0.9618 - val_loss: 0.1368\n",
      "Epoch 99/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9232 - loss: 0.2496\n",
      "Epoch 99: val_accuracy did not improve from 0.96183\n",
      "\n",
      "Epoch 99: val_accuracy did not improve from 0.96183\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 17ms/step - accuracy: 0.9232 - loss: 0.2513 - val_accuracy: 0.9618 - val_loss: 0.1362\n",
      "Epoch 100/100\n",
      "\u001b[1m841/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9251 - loss: 0.2481\n",
      "Epoch 100: val_accuracy did not improve from 0.96183\n",
      "\n",
      "Epoch 100: val_accuracy did not improve from 0.96183\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 18ms/step - accuracy: 0.9238 - loss: 0.2497 - val_accuracy: 0.9612 - val_loss: 0.1354\n",
      "\n",
      "Training history saved to: CNN_Models\\CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229\\training_history.pkl\n",
      "\n",
      "--- Peak Performance Summary ---\n",
      "Best validation accuracy:           0.9618\n",
      "Associated training accuracy:       0.9225\n",
      "Occurred at epoch:                  98\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch/accuracy</td><td>▁▃▃▅▆▆▆▇▇▇▇▇▇▇▇█████████████████████████</td></tr><tr><td>epoch/epoch</td><td>▁▁▁▁▁▂▂▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇████</td></tr><tr><td>epoch/learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/loss</td><td>█▇▆▄▄▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/val_accuracy</td><td>▁▂▅▆▇▇▇▇▇▇▇█████████████████████████████</td></tr><tr><td>epoch/val_loss</td><td>█▇▆▅▅▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch/accuracy</td><td>0.92376</td></tr><tr><td>epoch/epoch</td><td>99</td></tr><tr><td>epoch/learning_rate</td><td>0.0001</td></tr><tr><td>epoch/loss</td><td>0.2497</td></tr><tr><td>epoch/val_accuracy</td><td>0.96117</td></tr><tr><td>epoch/val_loss</td><td>0.13543</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">CNN_with_Dropout-lr_0.0001-bs_64-20250917-114229</strong> at: <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/b1b5ksk2' target=\"_blank\">https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/b1b5ksk2</a><br> View project at: <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2' target=\"_blank\">https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2</a><br>Synced 5 W&B file(s), 0 media file(s), 176 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250917_114229-b1b5ksk2\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Adapting the normalisation layer...\n",
      "Adaptation complete.\n",
      "\n",
      "\n",
      "--- Starting Experiment: Wide_CNN_Model ---\n",
      "\n",
      "--- Model Architecture ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"Wide_CNN_Model\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"Wide_CNN_Model\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ normalization_28                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Normalization</span>)                 │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">640</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_28 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3200</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3200</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_112 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">819,456</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_113 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,570</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ normalization_28                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m1\u001b[0m)      │             \u001b[38;5;34m3\u001b[0m │\n",
       "│ (\u001b[38;5;33mNormalization\u001b[0m)                 │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_16 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │           \u001b[38;5;34m640\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_16 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_17 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │        \u001b[38;5;34m73,856\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_17 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_28 (\u001b[38;5;33mFlatten\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3200\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3200\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_112 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m819,456\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_113 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │         \u001b[38;5;34m2,570\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">896,525</span> (3.42 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m896,525\u001b[0m (3.42 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">896,522</span> (3.42 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m896,522\u001b[0m (3.42 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3</span> (16.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m3\u001b[0m (16.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Hyperparameters ---\n",
      "optimiser           : adamw\n",
      "learning_rate       : 0.001\n",
      "epochs              : 100\n",
      "batch_size          : 64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "creating run (0.0s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\TimVos\\VSC Projects\\CSE5ML\\Assessment 2\\wandb\\run-20250917_120956-oa7kigvo</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/oa7kigvo' target=\"_blank\">Wide_CNN_Model-lr_0.001-bs_64-20250917-120956</a></strong> to <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2' target=\"_blank\">https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/oa7kigvo' target=\"_blank\">https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/oa7kigvo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.9005 - loss: 0.3098\n",
      "Epoch 1: val_accuracy improved from None to 0.99050, saving model to CNN_Models\\Wide_CNN_Model-lr_0.001-bs_64-20250917-120956\\best_model_epoch-01_val_acc-0.9905.keras\n",
      "\n",
      "Epoch 1: val_accuracy improved from None to 0.99050, saving model to wandb_models/Wide_CNN_Model-lr_0.001-bs_64-20250917-120956/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 44ms/step - accuracy: 0.9547 - loss: 0.1443 - val_accuracy: 0.9905 - val_loss: 0.0376\n",
      "Epoch 2/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.9823 - loss: 0.0569\n",
      "Epoch 2: val_accuracy improved from 0.99050 to 0.99083, saving model to CNN_Models\\Wide_CNN_Model-lr_0.001-bs_64-20250917-120956\\best_model_epoch-02_val_acc-0.9908.keras\n",
      "\n",
      "Epoch 2: val_accuracy improved from 0.99050 to 0.99083, saving model to wandb_models/Wide_CNN_Model-lr_0.001-bs_64-20250917-120956/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 46ms/step - accuracy: 0.9834 - loss: 0.0537 - val_accuracy: 0.9908 - val_loss: 0.0349\n",
      "Epoch 3/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.9872 - loss: 0.0391\n",
      "Epoch 3: val_accuracy improved from 0.99083 to 0.99167, saving model to CNN_Models\\Wide_CNN_Model-lr_0.001-bs_64-20250917-120956\\best_model_epoch-03_val_acc-0.9917.keras\n",
      "\n",
      "Epoch 3: val_accuracy improved from 0.99083 to 0.99167, saving model to wandb_models/Wide_CNN_Model-lr_0.001-bs_64-20250917-120956/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 42ms/step - accuracy: 0.9872 - loss: 0.0390 - val_accuracy: 0.9917 - val_loss: 0.0285\n",
      "Epoch 4/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.9886 - loss: 0.0339\n",
      "Epoch 4: val_accuracy improved from 0.99167 to 0.99417, saving model to CNN_Models\\Wide_CNN_Model-lr_0.001-bs_64-20250917-120956\\best_model_epoch-04_val_acc-0.9942.keras\n",
      "\n",
      "Epoch 4: val_accuracy improved from 0.99167 to 0.99417, saving model to wandb_models/Wide_CNN_Model-lr_0.001-bs_64-20250917-120956/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 44ms/step - accuracy: 0.9890 - loss: 0.0328 - val_accuracy: 0.9942 - val_loss: 0.0267\n",
      "Epoch 5/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.9915 - loss: 0.0260\n",
      "Epoch 5: val_accuracy did not improve from 0.99417\n",
      "\n",
      "Epoch 5: val_accuracy did not improve from 0.99417\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 42ms/step - accuracy: 0.9916 - loss: 0.0259 - val_accuracy: 0.9933 - val_loss: 0.0271\n",
      "Epoch 6/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.9922 - loss: 0.0239\n",
      "Epoch 6: val_accuracy did not improve from 0.99417\n",
      "\n",
      "Epoch 6: val_accuracy did not improve from 0.99417\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 43ms/step - accuracy: 0.9920 - loss: 0.0239 - val_accuracy: 0.9933 - val_loss: 0.0277\n",
      "Epoch 7/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.9936 - loss: 0.0195\n",
      "Epoch 7: val_accuracy improved from 0.99417 to 0.99483, saving model to CNN_Models\\Wide_CNN_Model-lr_0.001-bs_64-20250917-120956\\best_model_epoch-07_val_acc-0.9948.keras\n",
      "\n",
      "Epoch 7: val_accuracy improved from 0.99417 to 0.99483, saving model to wandb_models/Wide_CNN_Model-lr_0.001-bs_64-20250917-120956/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 43ms/step - accuracy: 0.9935 - loss: 0.0199 - val_accuracy: 0.9948 - val_loss: 0.0244\n",
      "Epoch 8/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.9942 - loss: 0.0169\n",
      "Epoch 8: val_accuracy did not improve from 0.99483\n",
      "\n",
      "Epoch 8: val_accuracy did not improve from 0.99483\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 44ms/step - accuracy: 0.9937 - loss: 0.0188 - val_accuracy: 0.9925 - val_loss: 0.0352\n",
      "Epoch 9/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.9939 - loss: 0.0164\n",
      "Epoch 9: val_accuracy did not improve from 0.99483\n",
      "\n",
      "Epoch 9: val_accuracy did not improve from 0.99483\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 43ms/step - accuracy: 0.9942 - loss: 0.0164 - val_accuracy: 0.9930 - val_loss: 0.0337\n",
      "Epoch 10/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.9948 - loss: 0.0152\n",
      "Epoch 10: val_accuracy did not improve from 0.99483\n",
      "\n",
      "Epoch 10: val_accuracy did not improve from 0.99483\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 44ms/step - accuracy: 0.9944 - loss: 0.0160 - val_accuracy: 0.9935 - val_loss: 0.0307\n",
      "Epoch 11/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.9958 - loss: 0.0118\n",
      "Epoch 11: val_accuracy did not improve from 0.99483\n",
      "\n",
      "Epoch 11: val_accuracy did not improve from 0.99483\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 45ms/step - accuracy: 0.9954 - loss: 0.0131 - val_accuracy: 0.9935 - val_loss: 0.0316\n",
      "Epoch 12/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.9955 - loss: 0.0138\n",
      "Epoch 12: val_accuracy did not improve from 0.99483\n",
      "\n",
      "Epoch 12: val_accuracy did not improve from 0.99483\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 47ms/step - accuracy: 0.9955 - loss: 0.0134 - val_accuracy: 0.9932 - val_loss: 0.0353\n",
      "Epoch 13/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.9960 - loss: 0.0115\n",
      "Epoch 13: val_accuracy did not improve from 0.99483\n",
      "\n",
      "Epoch 13: val_accuracy did not improve from 0.99483\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 42ms/step - accuracy: 0.9960 - loss: 0.0117 - val_accuracy: 0.9927 - val_loss: 0.0298\n",
      "Epoch 14/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.9962 - loss: 0.0113\n",
      "Epoch 14: val_accuracy did not improve from 0.99483\n",
      "\n",
      "Epoch 14: val_accuracy did not improve from 0.99483\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 44ms/step - accuracy: 0.9961 - loss: 0.0114 - val_accuracy: 0.9922 - val_loss: 0.0342\n",
      "Epoch 15/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.9962 - loss: 0.0114\n",
      "Epoch 15: val_accuracy did not improve from 0.99483\n",
      "\n",
      "Epoch 15: val_accuracy did not improve from 0.99483\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 49ms/step - accuracy: 0.9964 - loss: 0.0105 - val_accuracy: 0.9938 - val_loss: 0.0291\n",
      "Epoch 16/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.9964 - loss: 0.0111\n",
      "Epoch 16: val_accuracy did not improve from 0.99483\n",
      "\n",
      "Epoch 16: val_accuracy did not improve from 0.99483\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 47ms/step - accuracy: 0.9966 - loss: 0.0106 - val_accuracy: 0.9947 - val_loss: 0.0327\n",
      "Epoch 17/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.9968 - loss: 0.0094\n",
      "Epoch 17: val_accuracy did not improve from 0.99483\n",
      "\n",
      "Epoch 17: val_accuracy did not improve from 0.99483\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 48ms/step - accuracy: 0.9968 - loss: 0.0091 - val_accuracy: 0.9943 - val_loss: 0.0326\n",
      "\n",
      "Training history saved to: CNN_Models\\Wide_CNN_Model-lr_0.001-bs_64-20250917-120956\\training_history.pkl\n",
      "\n",
      "--- Peak Performance Summary ---\n",
      "Best validation accuracy:           0.9948\n",
      "Associated training accuracy:       0.9935\n",
      "Occurred at epoch:                  7\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch/accuracy</td><td>▁▆▆▇▇▇▇▇█████████</td></tr><tr><td>epoch/epoch</td><td>▁▁▂▂▃▃▄▄▅▅▅▆▆▇▇██</td></tr><tr><td>epoch/learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/loss</td><td>█▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/val_accuracy</td><td>▁▂▃▇▆▆█▄▅▆▆▅▅▄▆█▇</td></tr><tr><td>epoch/val_loss</td><td>█▇▃▂▂▃▁▇▆▄▅▇▄▆▃▅▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch/accuracy</td><td>0.9968</td></tr><tr><td>epoch/epoch</td><td>16</td></tr><tr><td>epoch/learning_rate</td><td>0.001</td></tr><tr><td>epoch/loss</td><td>0.00905</td></tr><tr><td>epoch/val_accuracy</td><td>0.99433</td></tr><tr><td>epoch/val_loss</td><td>0.03255</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Wide_CNN_Model-lr_0.001-bs_64-20250917-120956</strong> at: <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/oa7kigvo' target=\"_blank\">https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/oa7kigvo</a><br> View project at: <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2' target=\"_blank\">https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2</a><br>Synced 5 W&B file(s), 0 media file(s), 10 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250917_120956-oa7kigvo\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Adapting the normalisation layer...\n",
      "Adaptation complete.\n",
      "\n",
      "\n",
      "--- Starting Experiment: Wide_CNN_Model ---\n",
      "\n",
      "--- Model Architecture ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"Wide_CNN_Model\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"Wide_CNN_Model\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ normalization_29                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Normalization</span>)                 │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">640</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_29 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3200</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3200</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_114 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">819,456</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_115 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,570</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ normalization_29                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m1\u001b[0m)      │             \u001b[38;5;34m3\u001b[0m │\n",
       "│ (\u001b[38;5;33mNormalization\u001b[0m)                 │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_18 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │           \u001b[38;5;34m640\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_18 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_19 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │        \u001b[38;5;34m73,856\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_19 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_29 (\u001b[38;5;33mFlatten\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3200\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3200\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_114 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m819,456\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_115 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │         \u001b[38;5;34m2,570\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">896,525</span> (3.42 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m896,525\u001b[0m (3.42 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">896,522</span> (3.42 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m896,522\u001b[0m (3.42 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3</span> (16.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m3\u001b[0m (16.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Hyperparameters ---\n",
      "optimiser           : adamw\n",
      "learning_rate       : 0.0001\n",
      "epochs              : 100\n",
      "batch_size          : 64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "creating run (0.0s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\TimVos\\VSC Projects\\CSE5ML\\Assessment 2\\wandb\\run-20250917_122041-ho4pjio5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/ho4pjio5' target=\"_blank\">Wide_CNN_Model-lr_0.0001-bs_64-20250917-122041</a></strong> to <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2' target=\"_blank\">https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/ho4pjio5' target=\"_blank\">https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/ho4pjio5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.7860 - loss: 0.7241\n",
      "Epoch 1: val_accuracy improved from None to 0.97917, saving model to CNN_Models\\Wide_CNN_Model-lr_0.0001-bs_64-20250917-122041\\best_model_epoch-01_val_acc-0.9792.keras\n",
      "\n",
      "Epoch 1: val_accuracy improved from None to 0.97917, saving model to wandb_models/Wide_CNN_Model-lr_0.0001-bs_64-20250917-122041/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 49ms/step - accuracy: 0.8979 - loss: 0.3475 - val_accuracy: 0.9792 - val_loss: 0.0784\n",
      "Epoch 2/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.9676 - loss: 0.1117\n",
      "Epoch 2: val_accuracy improved from 0.97917 to 0.98583, saving model to CNN_Models\\Wide_CNN_Model-lr_0.0001-bs_64-20250917-122041\\best_model_epoch-02_val_acc-0.9858.keras\n",
      "\n",
      "Epoch 2: val_accuracy improved from 0.97917 to 0.98583, saving model to wandb_models/Wide_CNN_Model-lr_0.0001-bs_64-20250917-122041/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 47ms/step - accuracy: 0.9694 - loss: 0.1022 - val_accuracy: 0.9858 - val_loss: 0.0522\n",
      "Epoch 3/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.9769 - loss: 0.0754\n",
      "Epoch 3: val_accuracy improved from 0.98583 to 0.98883, saving model to CNN_Models\\Wide_CNN_Model-lr_0.0001-bs_64-20250917-122041\\best_model_epoch-03_val_acc-0.9888.keras\n",
      "\n",
      "Epoch 3: val_accuracy improved from 0.98583 to 0.98883, saving model to wandb_models/Wide_CNN_Model-lr_0.0001-bs_64-20250917-122041/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 49ms/step - accuracy: 0.9773 - loss: 0.0740 - val_accuracy: 0.9888 - val_loss: 0.0414\n",
      "Epoch 4/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.9818 - loss: 0.0604\n",
      "Epoch 4: val_accuracy improved from 0.98883 to 0.99050, saving model to CNN_Models\\Wide_CNN_Model-lr_0.0001-bs_64-20250917-122041\\best_model_epoch-04_val_acc-0.9905.keras\n",
      "\n",
      "Epoch 4: val_accuracy improved from 0.98883 to 0.99050, saving model to wandb_models/Wide_CNN_Model-lr_0.0001-bs_64-20250917-122041/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 57ms/step - accuracy: 0.9822 - loss: 0.0595 - val_accuracy: 0.9905 - val_loss: 0.0371\n",
      "Epoch 5/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.9831 - loss: 0.0534\n",
      "Epoch 5: val_accuracy did not improve from 0.99050\n",
      "\n",
      "Epoch 5: val_accuracy did not improve from 0.99050\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 47ms/step - accuracy: 0.9836 - loss: 0.0523 - val_accuracy: 0.9905 - val_loss: 0.0339\n",
      "Epoch 6/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.9861 - loss: 0.0445\n",
      "Epoch 6: val_accuracy improved from 0.99050 to 0.99083, saving model to CNN_Models\\Wide_CNN_Model-lr_0.0001-bs_64-20250917-122041\\best_model_epoch-06_val_acc-0.9908.keras\n",
      "\n",
      "Epoch 6: val_accuracy improved from 0.99050 to 0.99083, saving model to wandb_models/Wide_CNN_Model-lr_0.0001-bs_64-20250917-122041/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 49ms/step - accuracy: 0.9859 - loss: 0.0442 - val_accuracy: 0.9908 - val_loss: 0.0329\n",
      "Epoch 7/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.9882 - loss: 0.0384\n",
      "Epoch 7: val_accuracy improved from 0.99083 to 0.99150, saving model to CNN_Models\\Wide_CNN_Model-lr_0.0001-bs_64-20250917-122041\\best_model_epoch-07_val_acc-0.9915.keras\n",
      "\n",
      "Epoch 7: val_accuracy improved from 0.99083 to 0.99150, saving model to wandb_models/Wide_CNN_Model-lr_0.0001-bs_64-20250917-122041/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 53ms/step - accuracy: 0.9878 - loss: 0.0387 - val_accuracy: 0.9915 - val_loss: 0.0309\n",
      "Epoch 8/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.9893 - loss: 0.0352\n",
      "Epoch 8: val_accuracy improved from 0.99150 to 0.99250, saving model to CNN_Models\\Wide_CNN_Model-lr_0.0001-bs_64-20250917-122041\\best_model_epoch-08_val_acc-0.9925.keras\n",
      "\n",
      "Epoch 8: val_accuracy improved from 0.99150 to 0.99250, saving model to wandb_models/Wide_CNN_Model-lr_0.0001-bs_64-20250917-122041/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 53ms/step - accuracy: 0.9891 - loss: 0.0358 - val_accuracy: 0.9925 - val_loss: 0.0287\n",
      "Epoch 9/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.9907 - loss: 0.0304\n",
      "Epoch 9: val_accuracy did not improve from 0.99250\n",
      "\n",
      "Epoch 9: val_accuracy did not improve from 0.99250\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 51ms/step - accuracy: 0.9901 - loss: 0.0321 - val_accuracy: 0.9922 - val_loss: 0.0284\n",
      "Epoch 10/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.9915 - loss: 0.0273\n",
      "Epoch 10: val_accuracy did not improve from 0.99250\n",
      "\n",
      "Epoch 10: val_accuracy did not improve from 0.99250\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 59ms/step - accuracy: 0.9911 - loss: 0.0280 - val_accuracy: 0.9923 - val_loss: 0.0282\n",
      "Epoch 11/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.9926 - loss: 0.0257\n",
      "Epoch 11: val_accuracy did not improve from 0.99250\n",
      "\n",
      "Epoch 11: val_accuracy did not improve from 0.99250\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 52ms/step - accuracy: 0.9917 - loss: 0.0264 - val_accuracy: 0.9918 - val_loss: 0.0270\n",
      "Epoch 12/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.9919 - loss: 0.0240\n",
      "Epoch 12: val_accuracy improved from 0.99250 to 0.99267, saving model to CNN_Models\\Wide_CNN_Model-lr_0.0001-bs_64-20250917-122041\\best_model_epoch-12_val_acc-0.9927.keras\n",
      "\n",
      "Epoch 12: val_accuracy improved from 0.99250 to 0.99267, saving model to wandb_models/Wide_CNN_Model-lr_0.0001-bs_64-20250917-122041/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 50ms/step - accuracy: 0.9921 - loss: 0.0242 - val_accuracy: 0.9927 - val_loss: 0.0272\n",
      "Epoch 13/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.9930 - loss: 0.0211\n",
      "Epoch 13: val_accuracy did not improve from 0.99267\n",
      "\n",
      "Epoch 13: val_accuracy did not improve from 0.99267\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 54ms/step - accuracy: 0.9933 - loss: 0.0212 - val_accuracy: 0.9927 - val_loss: 0.0272\n",
      "Epoch 14/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.9934 - loss: 0.0195\n",
      "Epoch 14: val_accuracy did not improve from 0.99267\n",
      "\n",
      "Epoch 14: val_accuracy did not improve from 0.99267\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 45ms/step - accuracy: 0.9931 - loss: 0.0203 - val_accuracy: 0.9927 - val_loss: 0.0262\n",
      "Epoch 15/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.9940 - loss: 0.0190\n",
      "Epoch 15: val_accuracy improved from 0.99267 to 0.99300, saving model to CNN_Models\\Wide_CNN_Model-lr_0.0001-bs_64-20250917-122041\\best_model_epoch-15_val_acc-0.9930.keras\n",
      "\n",
      "Epoch 15: val_accuracy improved from 0.99267 to 0.99300, saving model to wandb_models/Wide_CNN_Model-lr_0.0001-bs_64-20250917-122041/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 45ms/step - accuracy: 0.9936 - loss: 0.0191 - val_accuracy: 0.9930 - val_loss: 0.0266\n",
      "Epoch 16/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.9938 - loss: 0.0177\n",
      "Epoch 16: val_accuracy did not improve from 0.99300\n",
      "\n",
      "Epoch 16: val_accuracy did not improve from 0.99300\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 42ms/step - accuracy: 0.9940 - loss: 0.0180 - val_accuracy: 0.9922 - val_loss: 0.0278\n",
      "Epoch 17/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9942 - loss: 0.0161\n",
      "Epoch 17: val_accuracy did not improve from 0.99300\n",
      "\n",
      "Epoch 17: val_accuracy did not improve from 0.99300\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 42ms/step - accuracy: 0.9947 - loss: 0.0160 - val_accuracy: 0.9928 - val_loss: 0.0258\n",
      "Epoch 18/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.9954 - loss: 0.0141\n",
      "Epoch 18: val_accuracy did not improve from 0.99300\n",
      "\n",
      "Epoch 18: val_accuracy did not improve from 0.99300\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 43ms/step - accuracy: 0.9953 - loss: 0.0142 - val_accuracy: 0.9927 - val_loss: 0.0279\n",
      "Epoch 19/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.9954 - loss: 0.0144\n",
      "Epoch 19: val_accuracy did not improve from 0.99300\n",
      "\n",
      "Epoch 19: val_accuracy did not improve from 0.99300\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 47ms/step - accuracy: 0.9951 - loss: 0.0141 - val_accuracy: 0.9928 - val_loss: 0.0271\n",
      "Epoch 20/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.9960 - loss: 0.0127\n",
      "Epoch 20: val_accuracy improved from 0.99300 to 0.99333, saving model to CNN_Models\\Wide_CNN_Model-lr_0.0001-bs_64-20250917-122041\\best_model_epoch-20_val_acc-0.9933.keras\n",
      "\n",
      "Epoch 20: val_accuracy improved from 0.99300 to 0.99333, saving model to wandb_models/Wide_CNN_Model-lr_0.0001-bs_64-20250917-122041/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 46ms/step - accuracy: 0.9957 - loss: 0.0133 - val_accuracy: 0.9933 - val_loss: 0.0264\n",
      "Epoch 21/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.9964 - loss: 0.0118\n",
      "Epoch 21: val_accuracy improved from 0.99333 to 0.99450, saving model to CNN_Models\\Wide_CNN_Model-lr_0.0001-bs_64-20250917-122041\\best_model_epoch-21_val_acc-0.9945.keras\n",
      "\n",
      "Epoch 21: val_accuracy improved from 0.99333 to 0.99450, saving model to wandb_models/Wide_CNN_Model-lr_0.0001-bs_64-20250917-122041/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 43ms/step - accuracy: 0.9958 - loss: 0.0126 - val_accuracy: 0.9945 - val_loss: 0.0262\n",
      "Epoch 22/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9967 - loss: 0.0114\n",
      "Epoch 22: val_accuracy did not improve from 0.99450\n",
      "\n",
      "Epoch 22: val_accuracy did not improve from 0.99450\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 41ms/step - accuracy: 0.9965 - loss: 0.0109 - val_accuracy: 0.9942 - val_loss: 0.0261\n",
      "Epoch 23/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.9968 - loss: 0.0104\n",
      "Epoch 23: val_accuracy did not improve from 0.99450\n",
      "\n",
      "Epoch 23: val_accuracy did not improve from 0.99450\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 43ms/step - accuracy: 0.9963 - loss: 0.0113 - val_accuracy: 0.9930 - val_loss: 0.0277\n",
      "Epoch 24/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9964 - loss: 0.0110\n",
      "Epoch 24: val_accuracy did not improve from 0.99450\n",
      "\n",
      "Epoch 24: val_accuracy did not improve from 0.99450\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 41ms/step - accuracy: 0.9968 - loss: 0.0103 - val_accuracy: 0.9933 - val_loss: 0.0278\n",
      "Epoch 25/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.9966 - loss: 0.0097\n",
      "Epoch 25: val_accuracy did not improve from 0.99450\n",
      "\n",
      "Epoch 25: val_accuracy did not improve from 0.99450\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 43ms/step - accuracy: 0.9965 - loss: 0.0099 - val_accuracy: 0.9933 - val_loss: 0.0287\n",
      "Epoch 26/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.9970 - loss: 0.0090\n",
      "Epoch 26: val_accuracy did not improve from 0.99450\n",
      "\n",
      "Epoch 26: val_accuracy did not improve from 0.99450\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 43ms/step - accuracy: 0.9969 - loss: 0.0090 - val_accuracy: 0.9940 - val_loss: 0.0286\n",
      "Epoch 27/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.9970 - loss: 0.0086\n",
      "Epoch 27: val_accuracy did not improve from 0.99450\n",
      "\n",
      "Epoch 27: val_accuracy did not improve from 0.99450\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 43ms/step - accuracy: 0.9968 - loss: 0.0095 - val_accuracy: 0.9940 - val_loss: 0.0269\n",
      "Epoch 28/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.9973 - loss: 0.0083\n",
      "Epoch 28: val_accuracy did not improve from 0.99450\n",
      "\n",
      "Epoch 28: val_accuracy did not improve from 0.99450\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 42ms/step - accuracy: 0.9972 - loss: 0.0086 - val_accuracy: 0.9942 - val_loss: 0.0259\n",
      "Epoch 29/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.9974 - loss: 0.0073\n",
      "Epoch 29: val_accuracy did not improve from 0.99450\n",
      "\n",
      "Epoch 29: val_accuracy did not improve from 0.99450\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 51ms/step - accuracy: 0.9975 - loss: 0.0072 - val_accuracy: 0.9937 - val_loss: 0.0273\n",
      "Epoch 30/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.9973 - loss: 0.0076\n",
      "Epoch 30: val_accuracy did not improve from 0.99450\n",
      "\n",
      "Epoch 30: val_accuracy did not improve from 0.99450\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 46ms/step - accuracy: 0.9974 - loss: 0.0073 - val_accuracy: 0.9937 - val_loss: 0.0272\n",
      "Epoch 31/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.9972 - loss: 0.0074\n",
      "Epoch 31: val_accuracy did not improve from 0.99450\n",
      "\n",
      "Epoch 31: val_accuracy did not improve from 0.99450\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 44ms/step - accuracy: 0.9976 - loss: 0.0068 - val_accuracy: 0.9935 - val_loss: 0.0283\n",
      "\n",
      "Training history saved to: CNN_Models\\Wide_CNN_Model-lr_0.0001-bs_64-20250917-122041\\training_history.pkl\n",
      "\n",
      "--- Peak Performance Summary ---\n",
      "Best validation accuracy:           0.9945\n",
      "Associated training accuracy:       0.9958\n",
      "Occurred at epoch:                  21\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch/accuracy</td><td>▁▆▇▇▇▇▇▇▇██████████████████████</td></tr><tr><td>epoch/epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇███</td></tr><tr><td>epoch/learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/loss</td><td>█▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/val_accuracy</td><td>▁▄▅▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇██▇▇▇██████</td></tr><tr><td>epoch/val_loss</td><td>█▅▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch/accuracy</td><td>0.99757</td></tr><tr><td>epoch/epoch</td><td>30</td></tr><tr><td>epoch/learning_rate</td><td>0.0001</td></tr><tr><td>epoch/loss</td><td>0.00678</td></tr><tr><td>epoch/val_accuracy</td><td>0.9935</td></tr><tr><td>epoch/val_loss</td><td>0.0283</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Wide_CNN_Model-lr_0.0001-bs_64-20250917-122041</strong> at: <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/ho4pjio5' target=\"_blank\">https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/ho4pjio5</a><br> View project at: <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2' target=\"_blank\">https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2</a><br>Synced 5 W&B file(s), 0 media file(s), 22 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250917_122041-ho4pjio5\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Adapting the normalisation layer...\n",
      "Adaptation complete.\n",
      "\n",
      "\n",
      "--- Starting Experiment: Wide_CNN_Model ---\n",
      "\n",
      "--- Model Architecture ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"Wide_CNN_Model\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"Wide_CNN_Model\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ normalization_30                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Normalization</span>)                 │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">640</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_30 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3200</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3200</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_116 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">819,456</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_117 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,570</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ normalization_30                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m1\u001b[0m)      │             \u001b[38;5;34m3\u001b[0m │\n",
       "│ (\u001b[38;5;33mNormalization\u001b[0m)                 │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_20 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │           \u001b[38;5;34m640\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_20 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_21 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │        \u001b[38;5;34m73,856\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_21 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_30 (\u001b[38;5;33mFlatten\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3200\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_6 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3200\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_116 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m819,456\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_117 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │         \u001b[38;5;34m2,570\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">896,525</span> (3.42 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m896,525\u001b[0m (3.42 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">896,522</span> (3.42 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m896,522\u001b[0m (3.42 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3</span> (16.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m3\u001b[0m (16.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Hyperparameters ---\n",
      "optimiser           : SGD\n",
      "learning_rate       : 0.001\n",
      "epochs              : 100\n",
      "batch_size          : 64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "creating run (0.1s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\TimVos\\VSC Projects\\CSE5ML\\Assessment 2\\wandb\\run-20250917_124118-le61us34</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/le61us34' target=\"_blank\">Wide_CNN_Model-lr_0.001-bs_64-20250917-124118</a></strong> to <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2' target=\"_blank\">https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/le61us34' target=\"_blank\">https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/le61us34</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.2715 - loss: 2.1451\n",
      "Epoch 1: val_accuracy improved from None to 0.83783, saving model to CNN_Models\\Wide_CNN_Model-lr_0.001-bs_64-20250917-124118\\best_model_epoch-01_val_acc-0.8378.keras\n",
      "\n",
      "Epoch 1: val_accuracy improved from None to 0.83783, saving model to wandb_models/Wide_CNN_Model-lr_0.001-bs_64-20250917-124118/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 38ms/step - accuracy: 0.4218 - loss: 1.9122 - val_accuracy: 0.8378 - val_loss: 1.0188\n",
      "Epoch 2/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.7065 - loss: 1.0186\n",
      "Epoch 2: val_accuracy improved from 0.83783 to 0.90950, saving model to CNN_Models\\Wide_CNN_Model-lr_0.001-bs_64-20250917-124118\\best_model_epoch-02_val_acc-0.9095.keras\n",
      "\n",
      "Epoch 2: val_accuracy improved from 0.83783 to 0.90950, saving model to wandb_models/Wide_CNN_Model-lr_0.001-bs_64-20250917-124118/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 38ms/step - accuracy: 0.7445 - loss: 0.8548 - val_accuracy: 0.9095 - val_loss: 0.3734\n",
      "Epoch 3/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.8213 - loss: 0.5760\n",
      "Epoch 3: val_accuracy improved from 0.90950 to 0.93033, saving model to CNN_Models\\Wide_CNN_Model-lr_0.001-bs_64-20250917-124118\\best_model_epoch-03_val_acc-0.9303.keras\n",
      "\n",
      "Epoch 3: val_accuracy improved from 0.90950 to 0.93033, saving model to wandb_models/Wide_CNN_Model-lr_0.001-bs_64-20250917-124118/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 40ms/step - accuracy: 0.8354 - loss: 0.5342 - val_accuracy: 0.9303 - val_loss: 0.2616\n",
      "Epoch 4/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8649 - loss: 0.4421\n",
      "Epoch 4: val_accuracy improved from 0.93033 to 0.94183, saving model to CNN_Models\\Wide_CNN_Model-lr_0.001-bs_64-20250917-124118\\best_model_epoch-04_val_acc-0.9418.keras\n",
      "\n",
      "Epoch 4: val_accuracy improved from 0.93033 to 0.94183, saving model to wandb_models/Wide_CNN_Model-lr_0.001-bs_64-20250917-124118/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 37ms/step - accuracy: 0.8716 - loss: 0.4203 - val_accuracy: 0.9418 - val_loss: 0.2118\n",
      "Epoch 5/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8903 - loss: 0.3596\n",
      "Epoch 5: val_accuracy improved from 0.94183 to 0.95050, saving model to CNN_Models\\Wide_CNN_Model-lr_0.001-bs_64-20250917-124118\\best_model_epoch-05_val_acc-0.9505.keras\n",
      "\n",
      "Epoch 5: val_accuracy improved from 0.94183 to 0.95050, saving model to wandb_models/Wide_CNN_Model-lr_0.001-bs_64-20250917-124118/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 38ms/step - accuracy: 0.8944 - loss: 0.3481 - val_accuracy: 0.9505 - val_loss: 0.1831\n",
      "Epoch 6/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.9069 - loss: 0.3114\n",
      "Epoch 6: val_accuracy improved from 0.95050 to 0.95550, saving model to CNN_Models\\Wide_CNN_Model-lr_0.001-bs_64-20250917-124118\\best_model_epoch-06_val_acc-0.9555.keras\n",
      "\n",
      "Epoch 6: val_accuracy improved from 0.95050 to 0.95550, saving model to wandb_models/Wide_CNN_Model-lr_0.001-bs_64-20250917-124118/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 52ms/step - accuracy: 0.9084 - loss: 0.3042 - val_accuracy: 0.9555 - val_loss: 0.1627\n",
      "Epoch 7/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9146 - loss: 0.2797\n",
      "Epoch 7: val_accuracy improved from 0.95550 to 0.96033, saving model to CNN_Models\\Wide_CNN_Model-lr_0.001-bs_64-20250917-124118\\best_model_epoch-07_val_acc-0.9603.keras\n",
      "\n",
      "Epoch 7: val_accuracy improved from 0.95550 to 0.96033, saving model to wandb_models/Wide_CNN_Model-lr_0.001-bs_64-20250917-124118/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 40ms/step - accuracy: 0.9163 - loss: 0.2744 - val_accuracy: 0.9603 - val_loss: 0.1473\n",
      "Epoch 8/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.9267 - loss: 0.2500\n",
      "Epoch 8: val_accuracy improved from 0.96033 to 0.96317, saving model to CNN_Models\\Wide_CNN_Model-lr_0.001-bs_64-20250917-124118\\best_model_epoch-08_val_acc-0.9632.keras\n",
      "\n",
      "Epoch 8: val_accuracy improved from 0.96033 to 0.96317, saving model to wandb_models/Wide_CNN_Model-lr_0.001-bs_64-20250917-124118/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 61ms/step - accuracy: 0.9262 - loss: 0.2472 - val_accuracy: 0.9632 - val_loss: 0.1360\n",
      "Epoch 9/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.9317 - loss: 0.2306\n",
      "Epoch 9: val_accuracy improved from 0.96317 to 0.96517, saving model to CNN_Models\\Wide_CNN_Model-lr_0.001-bs_64-20250917-124118\\best_model_epoch-09_val_acc-0.9652.keras\n",
      "\n",
      "Epoch 9: val_accuracy improved from 0.96317 to 0.96517, saving model to wandb_models/Wide_CNN_Model-lr_0.001-bs_64-20250917-124118/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 61ms/step - accuracy: 0.9309 - loss: 0.2300 - val_accuracy: 0.9652 - val_loss: 0.1268\n",
      "Epoch 10/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.9350 - loss: 0.2144\n",
      "Epoch 10: val_accuracy improved from 0.96517 to 0.96783, saving model to CNN_Models\\Wide_CNN_Model-lr_0.001-bs_64-20250917-124118\\best_model_epoch-10_val_acc-0.9678.keras\n",
      "\n",
      "Epoch 10: val_accuracy improved from 0.96517 to 0.96783, saving model to wandb_models/Wide_CNN_Model-lr_0.001-bs_64-20250917-124118/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 46ms/step - accuracy: 0.9346 - loss: 0.2145 - val_accuracy: 0.9678 - val_loss: 0.1192\n",
      "Epoch 11/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.9416 - loss: 0.2031\n",
      "Epoch 11: val_accuracy improved from 0.96783 to 0.96900, saving model to CNN_Models\\Wide_CNN_Model-lr_0.001-bs_64-20250917-124118\\best_model_epoch-11_val_acc-0.9690.keras\n",
      "\n",
      "Epoch 11: val_accuracy improved from 0.96783 to 0.96900, saving model to wandb_models/Wide_CNN_Model-lr_0.001-bs_64-20250917-124118/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 50ms/step - accuracy: 0.9408 - loss: 0.2012 - val_accuracy: 0.9690 - val_loss: 0.1127\n",
      "Epoch 12/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.9436 - loss: 0.1911\n",
      "Epoch 12: val_accuracy improved from 0.96900 to 0.97033, saving model to CNN_Models\\Wide_CNN_Model-lr_0.001-bs_64-20250917-124118\\best_model_epoch-12_val_acc-0.9703.keras\n",
      "\n",
      "Epoch 12: val_accuracy improved from 0.96900 to 0.97033, saving model to wandb_models/Wide_CNN_Model-lr_0.001-bs_64-20250917-124118/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 47ms/step - accuracy: 0.9430 - loss: 0.1908 - val_accuracy: 0.9703 - val_loss: 0.1075\n",
      "Epoch 13/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.9471 - loss: 0.1774\n",
      "Epoch 13: val_accuracy improved from 0.97033 to 0.97200, saving model to CNN_Models\\Wide_CNN_Model-lr_0.001-bs_64-20250917-124118\\best_model_epoch-13_val_acc-0.9720.keras\n",
      "\n",
      "Epoch 13: val_accuracy improved from 0.97033 to 0.97200, saving model to wandb_models/Wide_CNN_Model-lr_0.001-bs_64-20250917-124118/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 61ms/step - accuracy: 0.9464 - loss: 0.1790 - val_accuracy: 0.9720 - val_loss: 0.1024\n",
      "Epoch 14/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.9504 - loss: 0.1709\n",
      "Epoch 14: val_accuracy improved from 0.97200 to 0.97317, saving model to CNN_Models\\Wide_CNN_Model-lr_0.001-bs_64-20250917-124118\\best_model_epoch-14_val_acc-0.9732.keras\n",
      "\n",
      "Epoch 14: val_accuracy improved from 0.97200 to 0.97317, saving model to wandb_models/Wide_CNN_Model-lr_0.001-bs_64-20250917-124118/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 54ms/step - accuracy: 0.9492 - loss: 0.1710 - val_accuracy: 0.9732 - val_loss: 0.0985\n",
      "Epoch 15/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.9520 - loss: 0.1643\n",
      "Epoch 15: val_accuracy improved from 0.97317 to 0.97417, saving model to CNN_Models\\Wide_CNN_Model-lr_0.001-bs_64-20250917-124118\\best_model_epoch-15_val_acc-0.9742.keras\n",
      "\n",
      "Epoch 15: val_accuracy improved from 0.97317 to 0.97417, saving model to wandb_models/Wide_CNN_Model-lr_0.001-bs_64-20250917-124118/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 51ms/step - accuracy: 0.9514 - loss: 0.1643 - val_accuracy: 0.9742 - val_loss: 0.0948\n",
      "Epoch 16/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.9525 - loss: 0.1579\n",
      "Epoch 16: val_accuracy improved from 0.97417 to 0.97600, saving model to CNN_Models\\Wide_CNN_Model-lr_0.001-bs_64-20250917-124118\\best_model_epoch-16_val_acc-0.9760.keras\n",
      "\n",
      "Epoch 16: val_accuracy improved from 0.97417 to 0.97600, saving model to wandb_models/Wide_CNN_Model-lr_0.001-bs_64-20250917-124118/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 55ms/step - accuracy: 0.9520 - loss: 0.1589 - val_accuracy: 0.9760 - val_loss: 0.0917\n",
      "Epoch 17/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.9555 - loss: 0.1507\n",
      "Epoch 17: val_accuracy did not improve from 0.97600\n",
      "\n",
      "Epoch 17: val_accuracy did not improve from 0.97600\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 57ms/step - accuracy: 0.9540 - loss: 0.1523 - val_accuracy: 0.9760 - val_loss: 0.0886\n",
      "Epoch 18/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.9568 - loss: 0.1457\n",
      "Epoch 18: val_accuracy improved from 0.97600 to 0.97733, saving model to CNN_Models\\Wide_CNN_Model-lr_0.001-bs_64-20250917-124118\\best_model_epoch-18_val_acc-0.9773.keras\n",
      "\n",
      "Epoch 18: val_accuracy improved from 0.97600 to 0.97733, saving model to wandb_models/Wide_CNN_Model-lr_0.001-bs_64-20250917-124118/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 51ms/step - accuracy: 0.9555 - loss: 0.1473 - val_accuracy: 0.9773 - val_loss: 0.0862\n",
      "Epoch 19/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.9563 - loss: 0.1448\n",
      "Epoch 19: val_accuracy improved from 0.97733 to 0.97767, saving model to CNN_Models\\Wide_CNN_Model-lr_0.001-bs_64-20250917-124118\\best_model_epoch-19_val_acc-0.9777.keras\n",
      "\n",
      "Epoch 19: val_accuracy improved from 0.97733 to 0.97767, saving model to wandb_models/Wide_CNN_Model-lr_0.001-bs_64-20250917-124118/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 60ms/step - accuracy: 0.9568 - loss: 0.1441 - val_accuracy: 0.9777 - val_loss: 0.0835\n",
      "Epoch 20/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.9577 - loss: 0.1367\n",
      "Epoch 20: val_accuracy improved from 0.97767 to 0.97833, saving model to CNN_Models\\Wide_CNN_Model-lr_0.001-bs_64-20250917-124118\\best_model_epoch-20_val_acc-0.9783.keras\n",
      "\n",
      "Epoch 20: val_accuracy improved from 0.97767 to 0.97833, saving model to wandb_models/Wide_CNN_Model-lr_0.001-bs_64-20250917-124118/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 51ms/step - accuracy: 0.9578 - loss: 0.1393 - val_accuracy: 0.9783 - val_loss: 0.0817\n",
      "Epoch 21/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.9601 - loss: 0.1319\n",
      "Epoch 21: val_accuracy improved from 0.97833 to 0.97900, saving model to CNN_Models\\Wide_CNN_Model-lr_0.001-bs_64-20250917-124118\\best_model_epoch-21_val_acc-0.9790.keras\n",
      "\n",
      "Epoch 21: val_accuracy improved from 0.97833 to 0.97900, saving model to wandb_models/Wide_CNN_Model-lr_0.001-bs_64-20250917-124118/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 53ms/step - accuracy: 0.9592 - loss: 0.1338 - val_accuracy: 0.9790 - val_loss: 0.0793\n",
      "Epoch 22/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.9615 - loss: 0.1280\n",
      "Epoch 22: val_accuracy improved from 0.97900 to 0.98000, saving model to CNN_Models\\Wide_CNN_Model-lr_0.001-bs_64-20250917-124118\\best_model_epoch-22_val_acc-0.9800.keras\n",
      "\n",
      "Epoch 22: val_accuracy improved from 0.97900 to 0.98000, saving model to wandb_models/Wide_CNN_Model-lr_0.001-bs_64-20250917-124118/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 49ms/step - accuracy: 0.9606 - loss: 0.1295 - val_accuracy: 0.9800 - val_loss: 0.0772\n",
      "Epoch 23/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.9618 - loss: 0.1272\n",
      "Epoch 23: val_accuracy improved from 0.98000 to 0.98017, saving model to CNN_Models\\Wide_CNN_Model-lr_0.001-bs_64-20250917-124118\\best_model_epoch-23_val_acc-0.9802.keras\n",
      "\n",
      "Epoch 23: val_accuracy improved from 0.98000 to 0.98017, saving model to wandb_models/Wide_CNN_Model-lr_0.001-bs_64-20250917-124118/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 55ms/step - accuracy: 0.9618 - loss: 0.1281 - val_accuracy: 0.9802 - val_loss: 0.0756\n",
      "Epoch 24/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.9640 - loss: 0.1227\n",
      "Epoch 24: val_accuracy did not improve from 0.98017\n",
      "\n",
      "Epoch 24: val_accuracy did not improve from 0.98017\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 60ms/step - accuracy: 0.9629 - loss: 0.1243 - val_accuracy: 0.9798 - val_loss: 0.0741\n",
      "Epoch 25/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.9637 - loss: 0.1211\n",
      "Epoch 25: val_accuracy improved from 0.98017 to 0.98033, saving model to CNN_Models\\Wide_CNN_Model-lr_0.001-bs_64-20250917-124118\\best_model_epoch-25_val_acc-0.9803.keras\n",
      "\n",
      "Epoch 25: val_accuracy improved from 0.98017 to 0.98033, saving model to wandb_models/Wide_CNN_Model-lr_0.001-bs_64-20250917-124118/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 46ms/step - accuracy: 0.9634 - loss: 0.1220 - val_accuracy: 0.9803 - val_loss: 0.0725\n",
      "Epoch 26/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.9671 - loss: 0.1152\n",
      "Epoch 26: val_accuracy improved from 0.98033 to 0.98067, saving model to CNN_Models\\Wide_CNN_Model-lr_0.001-bs_64-20250917-124118\\best_model_epoch-26_val_acc-0.9807.keras\n",
      "\n",
      "Epoch 26: val_accuracy improved from 0.98033 to 0.98067, saving model to wandb_models/Wide_CNN_Model-lr_0.001-bs_64-20250917-124118/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 44ms/step - accuracy: 0.9657 - loss: 0.1177 - val_accuracy: 0.9807 - val_loss: 0.0710\n",
      "Epoch 27/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.9658 - loss: 0.1159\n",
      "Epoch 27: val_accuracy improved from 0.98067 to 0.98083, saving model to CNN_Models\\Wide_CNN_Model-lr_0.001-bs_64-20250917-124118\\best_model_epoch-27_val_acc-0.9808.keras\n",
      "\n",
      "Epoch 27: val_accuracy improved from 0.98067 to 0.98083, saving model to wandb_models/Wide_CNN_Model-lr_0.001-bs_64-20250917-124118/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 60ms/step - accuracy: 0.9650 - loss: 0.1171 - val_accuracy: 0.9808 - val_loss: 0.0696\n",
      "Epoch 28/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9654 - loss: 0.1137\n",
      "Epoch 28: val_accuracy did not improve from 0.98083\n",
      "\n",
      "Epoch 28: val_accuracy did not improve from 0.98083\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 69ms/step - accuracy: 0.9649 - loss: 0.1152 - val_accuracy: 0.9808 - val_loss: 0.0683\n",
      "Epoch 29/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9681 - loss: 0.1087\n",
      "Epoch 29: val_accuracy improved from 0.98083 to 0.98167, saving model to CNN_Models\\Wide_CNN_Model-lr_0.001-bs_64-20250917-124118\\best_model_epoch-29_val_acc-0.9817.keras\n",
      "\n",
      "Epoch 29: val_accuracy improved from 0.98083 to 0.98167, saving model to wandb_models/Wide_CNN_Model-lr_0.001-bs_64-20250917-124118/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 70ms/step - accuracy: 0.9665 - loss: 0.1118 - val_accuracy: 0.9817 - val_loss: 0.0670\n",
      "Epoch 30/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.9686 - loss: 0.1051\n",
      "Epoch 30: val_accuracy improved from 0.98167 to 0.98183, saving model to CNN_Models\\Wide_CNN_Model-lr_0.001-bs_64-20250917-124118\\best_model_epoch-30_val_acc-0.9818.keras\n",
      "\n",
      "Epoch 30: val_accuracy improved from 0.98167 to 0.98183, saving model to wandb_models/Wide_CNN_Model-lr_0.001-bs_64-20250917-124118/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 66ms/step - accuracy: 0.9678 - loss: 0.1075 - val_accuracy: 0.9818 - val_loss: 0.0663\n",
      "Epoch 31/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.9693 - loss: 0.1054\n",
      "Epoch 31: val_accuracy improved from 0.98183 to 0.98200, saving model to CNN_Models\\Wide_CNN_Model-lr_0.001-bs_64-20250917-124118\\best_model_epoch-31_val_acc-0.9820.keras\n",
      "\n",
      "Epoch 31: val_accuracy improved from 0.98183 to 0.98200, saving model to wandb_models/Wide_CNN_Model-lr_0.001-bs_64-20250917-124118/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 61ms/step - accuracy: 0.9678 - loss: 0.1076 - val_accuracy: 0.9820 - val_loss: 0.0653\n",
      "Epoch 32/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.9693 - loss: 0.1023\n",
      "Epoch 32: val_accuracy improved from 0.98200 to 0.98250, saving model to CNN_Models\\Wide_CNN_Model-lr_0.001-bs_64-20250917-124118\\best_model_epoch-32_val_acc-0.9825.keras\n",
      "\n",
      "Epoch 32: val_accuracy improved from 0.98200 to 0.98250, saving model to wandb_models/Wide_CNN_Model-lr_0.001-bs_64-20250917-124118/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 62ms/step - accuracy: 0.9692 - loss: 0.1044 - val_accuracy: 0.9825 - val_loss: 0.0640\n",
      "Epoch 33/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.9680 - loss: 0.1033\n",
      "Epoch 33: val_accuracy improved from 0.98250 to 0.98267, saving model to CNN_Models\\Wide_CNN_Model-lr_0.001-bs_64-20250917-124118\\best_model_epoch-33_val_acc-0.9827.keras\n",
      "\n",
      "Epoch 33: val_accuracy improved from 0.98250 to 0.98267, saving model to wandb_models/Wide_CNN_Model-lr_0.001-bs_64-20250917-124118/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 59ms/step - accuracy: 0.9682 - loss: 0.1032 - val_accuracy: 0.9827 - val_loss: 0.0632\n",
      "Epoch 34/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.9697 - loss: 0.0989\n",
      "Epoch 34: val_accuracy improved from 0.98267 to 0.98300, saving model to CNN_Models\\Wide_CNN_Model-lr_0.001-bs_64-20250917-124118\\best_model_epoch-34_val_acc-0.9830.keras\n",
      "\n",
      "Epoch 34: val_accuracy improved from 0.98267 to 0.98300, saving model to wandb_models/Wide_CNN_Model-lr_0.001-bs_64-20250917-124118/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 61ms/step - accuracy: 0.9689 - loss: 0.1000 - val_accuracy: 0.9830 - val_loss: 0.0622\n",
      "Epoch 35/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9702 - loss: 0.0983\n",
      "Epoch 35: val_accuracy did not improve from 0.98300\n",
      "\n",
      "Epoch 35: val_accuracy did not improve from 0.98300\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 65ms/step - accuracy: 0.9698 - loss: 0.0987 - val_accuracy: 0.9827 - val_loss: 0.0617\n",
      "Epoch 36/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9707 - loss: 0.0986\n",
      "Epoch 36: val_accuracy did not improve from 0.98300\n",
      "\n",
      "Epoch 36: val_accuracy did not improve from 0.98300\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 64ms/step - accuracy: 0.9696 - loss: 0.0996 - val_accuracy: 0.9830 - val_loss: 0.0603\n",
      "Epoch 37/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.9714 - loss: 0.0939\n",
      "Epoch 37: val_accuracy improved from 0.98300 to 0.98333, saving model to CNN_Models\\Wide_CNN_Model-lr_0.001-bs_64-20250917-124118\\best_model_epoch-37_val_acc-0.9833.keras\n",
      "\n",
      "Epoch 37: val_accuracy improved from 0.98300 to 0.98333, saving model to wandb_models/Wide_CNN_Model-lr_0.001-bs_64-20250917-124118/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 67ms/step - accuracy: 0.9709 - loss: 0.0950 - val_accuracy: 0.9833 - val_loss: 0.0596\n",
      "Epoch 38/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9711 - loss: 0.0927\n",
      "Epoch 38: val_accuracy improved from 0.98333 to 0.98417, saving model to CNN_Models\\Wide_CNN_Model-lr_0.001-bs_64-20250917-124118\\best_model_epoch-38_val_acc-0.9842.keras\n",
      "\n",
      "Epoch 38: val_accuracy improved from 0.98333 to 0.98417, saving model to wandb_models/Wide_CNN_Model-lr_0.001-bs_64-20250917-124118/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 69ms/step - accuracy: 0.9712 - loss: 0.0945 - val_accuracy: 0.9842 - val_loss: 0.0590\n",
      "Epoch 39/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9719 - loss: 0.0950\n",
      "Epoch 39: val_accuracy improved from 0.98417 to 0.98450, saving model to CNN_Models\\Wide_CNN_Model-lr_0.001-bs_64-20250917-124118\\best_model_epoch-39_val_acc-0.9845.keras\n",
      "\n",
      "Epoch 39: val_accuracy improved from 0.98417 to 0.98450, saving model to wandb_models/Wide_CNN_Model-lr_0.001-bs_64-20250917-124118/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 72ms/step - accuracy: 0.9717 - loss: 0.0941 - val_accuracy: 0.9845 - val_loss: 0.0587\n",
      "Epoch 40/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9721 - loss: 0.0923\n",
      "Epoch 40: val_accuracy did not improve from 0.98450\n",
      "\n",
      "Epoch 40: val_accuracy did not improve from 0.98450\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 68ms/step - accuracy: 0.9714 - loss: 0.0928 - val_accuracy: 0.9842 - val_loss: 0.0578\n",
      "Epoch 41/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9733 - loss: 0.0911\n",
      "Epoch 41: val_accuracy did not improve from 0.98450\n",
      "\n",
      "Epoch 41: val_accuracy did not improve from 0.98450\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 72ms/step - accuracy: 0.9725 - loss: 0.0916 - val_accuracy: 0.9842 - val_loss: 0.0570\n",
      "Epoch 42/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9751 - loss: 0.0865\n",
      "Epoch 42: val_accuracy improved from 0.98450 to 0.98533, saving model to CNN_Models\\Wide_CNN_Model-lr_0.001-bs_64-20250917-124118\\best_model_epoch-42_val_acc-0.9853.keras\n",
      "\n",
      "Epoch 42: val_accuracy improved from 0.98450 to 0.98533, saving model to wandb_models/Wide_CNN_Model-lr_0.001-bs_64-20250917-124118/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 73ms/step - accuracy: 0.9738 - loss: 0.0886 - val_accuracy: 0.9853 - val_loss: 0.0563\n",
      "Epoch 43/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9726 - loss: 0.0885\n",
      "Epoch 43: val_accuracy did not improve from 0.98533\n",
      "\n",
      "Epoch 43: val_accuracy did not improve from 0.98533\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 70ms/step - accuracy: 0.9722 - loss: 0.0898 - val_accuracy: 0.9853 - val_loss: 0.0557\n",
      "Epoch 44/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9729 - loss: 0.0877\n",
      "Epoch 44: val_accuracy did not improve from 0.98533\n",
      "\n",
      "Epoch 44: val_accuracy did not improve from 0.98533\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 70ms/step - accuracy: 0.9726 - loss: 0.0879 - val_accuracy: 0.9853 - val_loss: 0.0554\n",
      "Epoch 45/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.9734 - loss: 0.0871\n",
      "Epoch 45: val_accuracy improved from 0.98533 to 0.98550, saving model to CNN_Models\\Wide_CNN_Model-lr_0.001-bs_64-20250917-124118\\best_model_epoch-45_val_acc-0.9855.keras\n",
      "\n",
      "Epoch 45: val_accuracy improved from 0.98533 to 0.98550, saving model to wandb_models/Wide_CNN_Model-lr_0.001-bs_64-20250917-124118/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 77ms/step - accuracy: 0.9737 - loss: 0.0865 - val_accuracy: 0.9855 - val_loss: 0.0545\n",
      "Epoch 46/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.9744 - loss: 0.0846\n",
      "Epoch 46: val_accuracy improved from 0.98550 to 0.98567, saving model to CNN_Models\\Wide_CNN_Model-lr_0.001-bs_64-20250917-124118\\best_model_epoch-46_val_acc-0.9857.keras\n",
      "\n",
      "Epoch 46: val_accuracy improved from 0.98550 to 0.98567, saving model to wandb_models/Wide_CNN_Model-lr_0.001-bs_64-20250917-124118/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 78ms/step - accuracy: 0.9735 - loss: 0.0861 - val_accuracy: 0.9857 - val_loss: 0.0541\n",
      "Epoch 47/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.9752 - loss: 0.0834\n",
      "Epoch 47: val_accuracy did not improve from 0.98567\n",
      "\n",
      "Epoch 47: val_accuracy did not improve from 0.98567\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 75ms/step - accuracy: 0.9744 - loss: 0.0844 - val_accuracy: 0.9857 - val_loss: 0.0531\n",
      "Epoch 48/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.9752 - loss: 0.0833\n",
      "Epoch 48: val_accuracy did not improve from 0.98567\n",
      "\n",
      "Epoch 48: val_accuracy did not improve from 0.98567\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 75ms/step - accuracy: 0.9744 - loss: 0.0836 - val_accuracy: 0.9853 - val_loss: 0.0527\n",
      "Epoch 49/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9755 - loss: 0.0806\n",
      "Epoch 49: val_accuracy improved from 0.98567 to 0.98583, saving model to CNN_Models\\Wide_CNN_Model-lr_0.001-bs_64-20250917-124118\\best_model_epoch-49_val_acc-0.9858.keras\n",
      "\n",
      "Epoch 49: val_accuracy improved from 0.98567 to 0.98583, saving model to wandb_models/Wide_CNN_Model-lr_0.001-bs_64-20250917-124118/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 73ms/step - accuracy: 0.9748 - loss: 0.0817 - val_accuracy: 0.9858 - val_loss: 0.0530\n",
      "Epoch 50/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.9765 - loss: 0.0787\n",
      "Epoch 50: val_accuracy did not improve from 0.98583\n",
      "\n",
      "Epoch 50: val_accuracy did not improve from 0.98583\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 76ms/step - accuracy: 0.9759 - loss: 0.0802 - val_accuracy: 0.9858 - val_loss: 0.0519\n",
      "Epoch 51/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.9763 - loss: 0.0791\n",
      "Epoch 51: val_accuracy improved from 0.98583 to 0.98617, saving model to CNN_Models\\Wide_CNN_Model-lr_0.001-bs_64-20250917-124118\\best_model_epoch-51_val_acc-0.9862.keras\n",
      "\n",
      "Epoch 51: val_accuracy improved from 0.98583 to 0.98617, saving model to wandb_models/Wide_CNN_Model-lr_0.001-bs_64-20250917-124118/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 74ms/step - accuracy: 0.9756 - loss: 0.0797 - val_accuracy: 0.9862 - val_loss: 0.0516\n",
      "Epoch 52/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9764 - loss: 0.0784\n",
      "Epoch 52: val_accuracy improved from 0.98617 to 0.98667, saving model to CNN_Models\\Wide_CNN_Model-lr_0.001-bs_64-20250917-124118\\best_model_epoch-52_val_acc-0.9867.keras\n",
      "\n",
      "Epoch 52: val_accuracy improved from 0.98617 to 0.98667, saving model to wandb_models/Wide_CNN_Model-lr_0.001-bs_64-20250917-124118/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 73ms/step - accuracy: 0.9760 - loss: 0.0794 - val_accuracy: 0.9867 - val_loss: 0.0511\n",
      "Epoch 53/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.9767 - loss: 0.0779\n",
      "Epoch 53: val_accuracy did not improve from 0.98667\n",
      "\n",
      "Epoch 53: val_accuracy did not improve from 0.98667\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 75ms/step - accuracy: 0.9765 - loss: 0.0784 - val_accuracy: 0.9863 - val_loss: 0.0509\n",
      "Epoch 54/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9758 - loss: 0.0776\n",
      "Epoch 54: val_accuracy did not improve from 0.98667\n",
      "\n",
      "Epoch 54: val_accuracy did not improve from 0.98667\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 69ms/step - accuracy: 0.9756 - loss: 0.0786 - val_accuracy: 0.9862 - val_loss: 0.0502\n",
      "Epoch 55/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.9772 - loss: 0.0769\n",
      "Epoch 55: val_accuracy did not improve from 0.98667\n",
      "\n",
      "Epoch 55: val_accuracy did not improve from 0.98667\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 62ms/step - accuracy: 0.9768 - loss: 0.0768 - val_accuracy: 0.9865 - val_loss: 0.0502\n",
      "Epoch 56/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9772 - loss: 0.0744\n",
      "Epoch 56: val_accuracy did not improve from 0.98667\n",
      "\n",
      "Epoch 56: val_accuracy did not improve from 0.98667\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 64ms/step - accuracy: 0.9768 - loss: 0.0752 - val_accuracy: 0.9865 - val_loss: 0.0493\n",
      "Epoch 57/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.9768 - loss: 0.0762\n",
      "Epoch 57: val_accuracy did not improve from 0.98667\n",
      "\n",
      "Epoch 57: val_accuracy did not improve from 0.98667\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 66ms/step - accuracy: 0.9768 - loss: 0.0751 - val_accuracy: 0.9865 - val_loss: 0.0492\n",
      "Epoch 58/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9783 - loss: 0.0717\n",
      "Epoch 58: val_accuracy improved from 0.98667 to 0.98683, saving model to CNN_Models\\Wide_CNN_Model-lr_0.001-bs_64-20250917-124118\\best_model_epoch-58_val_acc-0.9868.keras\n",
      "\n",
      "Epoch 58: val_accuracy improved from 0.98667 to 0.98683, saving model to wandb_models/Wide_CNN_Model-lr_0.001-bs_64-20250917-124118/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 65ms/step - accuracy: 0.9773 - loss: 0.0736 - val_accuracy: 0.9868 - val_loss: 0.0490\n",
      "Epoch 59/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.9788 - loss: 0.0717\n",
      "Epoch 59: val_accuracy did not improve from 0.98683\n",
      "\n",
      "Epoch 59: val_accuracy did not improve from 0.98683\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 63ms/step - accuracy: 0.9780 - loss: 0.0729 - val_accuracy: 0.9862 - val_loss: 0.0484\n",
      "Epoch 60/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.9787 - loss: 0.0713\n",
      "Epoch 60: val_accuracy did not improve from 0.98683\n",
      "\n",
      "Epoch 60: val_accuracy did not improve from 0.98683\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 63ms/step - accuracy: 0.9779 - loss: 0.0723 - val_accuracy: 0.9862 - val_loss: 0.0480\n",
      "Epoch 61/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.9782 - loss: 0.0713\n",
      "Epoch 61: val_accuracy did not improve from 0.98683\n",
      "\n",
      "Epoch 61: val_accuracy did not improve from 0.98683\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 63ms/step - accuracy: 0.9776 - loss: 0.0723 - val_accuracy: 0.9865 - val_loss: 0.0479\n",
      "Epoch 62/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9788 - loss: 0.0703\n",
      "Epoch 62: val_accuracy did not improve from 0.98683\n",
      "\n",
      "Epoch 62: val_accuracy did not improve from 0.98683\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 64ms/step - accuracy: 0.9780 - loss: 0.0721 - val_accuracy: 0.9868 - val_loss: 0.0476\n",
      "Epoch 63/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.9790 - loss: 0.0702\n",
      "Epoch 63: val_accuracy improved from 0.98683 to 0.98717, saving model to CNN_Models\\Wide_CNN_Model-lr_0.001-bs_64-20250917-124118\\best_model_epoch-63_val_acc-0.9872.keras\n",
      "\n",
      "Epoch 63: val_accuracy improved from 0.98683 to 0.98717, saving model to wandb_models/Wide_CNN_Model-lr_0.001-bs_64-20250917-124118/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 63ms/step - accuracy: 0.9784 - loss: 0.0715 - val_accuracy: 0.9872 - val_loss: 0.0470\n",
      "Epoch 64/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9786 - loss: 0.0699\n",
      "Epoch 64: val_accuracy did not improve from 0.98717\n",
      "\n",
      "Epoch 64: val_accuracy did not improve from 0.98717\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 64ms/step - accuracy: 0.9784 - loss: 0.0712 - val_accuracy: 0.9867 - val_loss: 0.0468\n",
      "Epoch 65/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.9792 - loss: 0.0692\n",
      "Epoch 65: val_accuracy did not improve from 0.98717\n",
      "\n",
      "Epoch 65: val_accuracy did not improve from 0.98717\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 63ms/step - accuracy: 0.9787 - loss: 0.0700 - val_accuracy: 0.9868 - val_loss: 0.0463\n",
      "Epoch 66/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.9783 - loss: 0.0691\n",
      "Epoch 66: val_accuracy improved from 0.98717 to 0.98783, saving model to CNN_Models\\Wide_CNN_Model-lr_0.001-bs_64-20250917-124118\\best_model_epoch-66_val_acc-0.9878.keras\n",
      "\n",
      "Epoch 66: val_accuracy improved from 0.98717 to 0.98783, saving model to wandb_models/Wide_CNN_Model-lr_0.001-bs_64-20250917-124118/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 64ms/step - accuracy: 0.9787 - loss: 0.0695 - val_accuracy: 0.9878 - val_loss: 0.0462\n",
      "Epoch 67/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9797 - loss: 0.0675\n",
      "Epoch 67: val_accuracy did not improve from 0.98783\n",
      "\n",
      "Epoch 67: val_accuracy did not improve from 0.98783\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 64ms/step - accuracy: 0.9793 - loss: 0.0678 - val_accuracy: 0.9877 - val_loss: 0.0459\n",
      "Epoch 68/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9783 - loss: 0.0688\n",
      "Epoch 68: val_accuracy did not improve from 0.98783\n",
      "\n",
      "Epoch 68: val_accuracy did not improve from 0.98783\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 67ms/step - accuracy: 0.9790 - loss: 0.0683 - val_accuracy: 0.9870 - val_loss: 0.0457\n",
      "Epoch 69/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9808 - loss: 0.0657\n",
      "Epoch 69: val_accuracy did not improve from 0.98783\n",
      "\n",
      "Epoch 69: val_accuracy did not improve from 0.98783\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 65ms/step - accuracy: 0.9797 - loss: 0.0668 - val_accuracy: 0.9878 - val_loss: 0.0456\n",
      "Epoch 70/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9794 - loss: 0.0660\n",
      "Epoch 70: val_accuracy did not improve from 0.98783\n",
      "\n",
      "Epoch 70: val_accuracy did not improve from 0.98783\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 64ms/step - accuracy: 0.9795 - loss: 0.0662 - val_accuracy: 0.9878 - val_loss: 0.0453\n",
      "Epoch 71/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9799 - loss: 0.0671\n",
      "Epoch 71: val_accuracy improved from 0.98783 to 0.98817, saving model to CNN_Models\\Wide_CNN_Model-lr_0.001-bs_64-20250917-124118\\best_model_epoch-71_val_acc-0.9882.keras\n",
      "\n",
      "Epoch 71: val_accuracy improved from 0.98783 to 0.98817, saving model to wandb_models/Wide_CNN_Model-lr_0.001-bs_64-20250917-124118/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 65ms/step - accuracy: 0.9801 - loss: 0.0666 - val_accuracy: 0.9882 - val_loss: 0.0451\n",
      "Epoch 72/100\n",
      "\u001b[1m843/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.9809 - loss: 0.0636\n",
      "Epoch 72: val_accuracy did not improve from 0.98817\n",
      "\n",
      "Epoch 72: val_accuracy did not improve from 0.98817\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 66ms/step - accuracy: 0.9801 - loss: 0.0643 - val_accuracy: 0.9878 - val_loss: 0.0444\n",
      "Epoch 73/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.9799 - loss: 0.0653\n",
      "Epoch 73: val_accuracy improved from 0.98817 to 0.98833, saving model to CNN_Models\\Wide_CNN_Model-lr_0.001-bs_64-20250917-124118\\best_model_epoch-73_val_acc-0.9883.keras\n",
      "\n",
      "Epoch 73: val_accuracy improved from 0.98817 to 0.98833, saving model to wandb_models/Wide_CNN_Model-lr_0.001-bs_64-20250917-124118/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 67ms/step - accuracy: 0.9799 - loss: 0.0654 - val_accuracy: 0.9883 - val_loss: 0.0445\n",
      "Epoch 74/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.9805 - loss: 0.0651\n",
      "Epoch 74: val_accuracy did not improve from 0.98833\n",
      "\n",
      "Epoch 74: val_accuracy did not improve from 0.98833\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 66ms/step - accuracy: 0.9802 - loss: 0.0658 - val_accuracy: 0.9883 - val_loss: 0.0441\n",
      "Epoch 75/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9811 - loss: 0.0627\n",
      "Epoch 75: val_accuracy improved from 0.98833 to 0.98867, saving model to CNN_Models\\Wide_CNN_Model-lr_0.001-bs_64-20250917-124118\\best_model_epoch-75_val_acc-0.9887.keras\n",
      "\n",
      "Epoch 75: val_accuracy improved from 0.98833 to 0.98867, saving model to wandb_models/Wide_CNN_Model-lr_0.001-bs_64-20250917-124118/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 70ms/step - accuracy: 0.9803 - loss: 0.0643 - val_accuracy: 0.9887 - val_loss: 0.0437\n",
      "Epoch 76/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9809 - loss: 0.0645\n",
      "Epoch 76: val_accuracy improved from 0.98867 to 0.98883, saving model to CNN_Models\\Wide_CNN_Model-lr_0.001-bs_64-20250917-124118\\best_model_epoch-76_val_acc-0.9888.keras\n",
      "\n",
      "Epoch 76: val_accuracy improved from 0.98867 to 0.98883, saving model to wandb_models/Wide_CNN_Model-lr_0.001-bs_64-20250917-124118/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 68ms/step - accuracy: 0.9800 - loss: 0.0649 - val_accuracy: 0.9888 - val_loss: 0.0438\n",
      "Epoch 77/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.9818 - loss: 0.0616\n",
      "Epoch 77: val_accuracy did not improve from 0.98883\n",
      "\n",
      "Epoch 77: val_accuracy did not improve from 0.98883\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 67ms/step - accuracy: 0.9809 - loss: 0.0625 - val_accuracy: 0.9887 - val_loss: 0.0434\n",
      "Epoch 78/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9810 - loss: 0.0626\n",
      "Epoch 78: val_accuracy did not improve from 0.98883\n",
      "\n",
      "Epoch 78: val_accuracy did not improve from 0.98883\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 68ms/step - accuracy: 0.9809 - loss: 0.0627 - val_accuracy: 0.9882 - val_loss: 0.0431\n",
      "Epoch 79/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9807 - loss: 0.0627\n",
      "Epoch 79: val_accuracy improved from 0.98883 to 0.98933, saving model to CNN_Models\\Wide_CNN_Model-lr_0.001-bs_64-20250917-124118\\best_model_epoch-79_val_acc-0.9893.keras\n",
      "\n",
      "Epoch 79: val_accuracy improved from 0.98883 to 0.98933, saving model to wandb_models/Wide_CNN_Model-lr_0.001-bs_64-20250917-124118/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 72ms/step - accuracy: 0.9807 - loss: 0.0625 - val_accuracy: 0.9893 - val_loss: 0.0427\n",
      "Epoch 80/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.9812 - loss: 0.0611\n",
      "Epoch 80: val_accuracy did not improve from 0.98933\n",
      "\n",
      "Epoch 80: val_accuracy did not improve from 0.98933\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 67ms/step - accuracy: 0.9804 - loss: 0.0625 - val_accuracy: 0.9888 - val_loss: 0.0427\n",
      "Epoch 81/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9816 - loss: 0.0601\n",
      "Epoch 81: val_accuracy did not improve from 0.98933\n",
      "\n",
      "Epoch 81: val_accuracy did not improve from 0.98933\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 70ms/step - accuracy: 0.9812 - loss: 0.0612 - val_accuracy: 0.9893 - val_loss: 0.0425\n",
      "Epoch 82/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9825 - loss: 0.0611\n",
      "Epoch 82: val_accuracy improved from 0.98933 to 0.98950, saving model to CNN_Models\\Wide_CNN_Model-lr_0.001-bs_64-20250917-124118\\best_model_epoch-82_val_acc-0.9895.keras\n",
      "\n",
      "Epoch 82: val_accuracy improved from 0.98933 to 0.98950, saving model to wandb_models/Wide_CNN_Model-lr_0.001-bs_64-20250917-124118/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 69ms/step - accuracy: 0.9820 - loss: 0.0621 - val_accuracy: 0.9895 - val_loss: 0.0424\n",
      "Epoch 83/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9813 - loss: 0.0621\n",
      "Epoch 83: val_accuracy improved from 0.98950 to 0.98983, saving model to CNN_Models\\Wide_CNN_Model-lr_0.001-bs_64-20250917-124118\\best_model_epoch-83_val_acc-0.9898.keras\n",
      "\n",
      "Epoch 83: val_accuracy improved from 0.98950 to 0.98983, saving model to wandb_models/Wide_CNN_Model-lr_0.001-bs_64-20250917-124118/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 69ms/step - accuracy: 0.9813 - loss: 0.0610 - val_accuracy: 0.9898 - val_loss: 0.0419\n",
      "Epoch 84/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9816 - loss: 0.0599\n",
      "Epoch 84: val_accuracy did not improve from 0.98983\n",
      "\n",
      "Epoch 84: val_accuracy did not improve from 0.98983\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 68ms/step - accuracy: 0.9816 - loss: 0.0606 - val_accuracy: 0.9895 - val_loss: 0.0416\n",
      "Epoch 85/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9814 - loss: 0.0603\n",
      "Epoch 85: val_accuracy did not improve from 0.98983\n",
      "\n",
      "Epoch 85: val_accuracy did not improve from 0.98983\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 69ms/step - accuracy: 0.9815 - loss: 0.0609 - val_accuracy: 0.9898 - val_loss: 0.0416\n",
      "Epoch 86/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.9823 - loss: 0.0585\n",
      "Epoch 86: val_accuracy did not improve from 0.98983\n",
      "\n",
      "Epoch 86: val_accuracy did not improve from 0.98983\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 66ms/step - accuracy: 0.9821 - loss: 0.0591 - val_accuracy: 0.9893 - val_loss: 0.0415\n",
      "Epoch 87/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.9824 - loss: 0.0579\n",
      "Epoch 87: val_accuracy did not improve from 0.98983\n",
      "\n",
      "Epoch 87: val_accuracy did not improve from 0.98983\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 66ms/step - accuracy: 0.9823 - loss: 0.0583 - val_accuracy: 0.9892 - val_loss: 0.0413\n",
      "Epoch 88/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.9827 - loss: 0.0592\n",
      "Epoch 88: val_accuracy did not improve from 0.98983\n",
      "\n",
      "Epoch 88: val_accuracy did not improve from 0.98983\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 86ms/step - accuracy: 0.9819 - loss: 0.0597 - val_accuracy: 0.9898 - val_loss: 0.0413\n",
      "Epoch 89/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - accuracy: 0.9825 - loss: 0.0600\n",
      "Epoch 89: val_accuracy did not improve from 0.98983\n",
      "\n",
      "Epoch 89: val_accuracy did not improve from 0.98983\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 98ms/step - accuracy: 0.9823 - loss: 0.0588 - val_accuracy: 0.9898 - val_loss: 0.0407\n",
      "Epoch 90/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 0.9827 - loss: 0.0567\n",
      "Epoch 90: val_accuracy did not improve from 0.98983\n",
      "\n",
      "Epoch 90: val_accuracy did not improve from 0.98983\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 82ms/step - accuracy: 0.9825 - loss: 0.0572 - val_accuracy: 0.9898 - val_loss: 0.0408\n",
      "Epoch 91/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 0.9831 - loss: 0.0558\n",
      "Epoch 91: val_accuracy did not improve from 0.98983\n",
      "\n",
      "Epoch 91: val_accuracy did not improve from 0.98983\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 96ms/step - accuracy: 0.9818 - loss: 0.0574 - val_accuracy: 0.9897 - val_loss: 0.0407\n",
      "Epoch 92/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.9834 - loss: 0.0554\n",
      "Epoch 92: val_accuracy improved from 0.98983 to 0.99017, saving model to CNN_Models\\Wide_CNN_Model-lr_0.001-bs_64-20250917-124118\\best_model_epoch-92_val_acc-0.9902.keras\n",
      "\n",
      "Epoch 92: val_accuracy improved from 0.98983 to 0.99017, saving model to wandb_models/Wide_CNN_Model-lr_0.001-bs_64-20250917-124118/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 89ms/step - accuracy: 0.9830 - loss: 0.0566 - val_accuracy: 0.9902 - val_loss: 0.0407\n",
      "Epoch 93/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.9835 - loss: 0.0561\n",
      "Epoch 93: val_accuracy improved from 0.99017 to 0.99050, saving model to CNN_Models\\Wide_CNN_Model-lr_0.001-bs_64-20250917-124118\\best_model_epoch-93_val_acc-0.9905.keras\n",
      "\n",
      "Epoch 93: val_accuracy improved from 0.99017 to 0.99050, saving model to wandb_models/Wide_CNN_Model-lr_0.001-bs_64-20250917-124118/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 91ms/step - accuracy: 0.9823 - loss: 0.0574 - val_accuracy: 0.9905 - val_loss: 0.0403\n",
      "Epoch 94/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.9829 - loss: 0.0561\n",
      "Epoch 94: val_accuracy did not improve from 0.99050\n",
      "\n",
      "Epoch 94: val_accuracy did not improve from 0.99050\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 88ms/step - accuracy: 0.9827 - loss: 0.0565 - val_accuracy: 0.9905 - val_loss: 0.0404\n",
      "Epoch 95/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.9833 - loss: 0.0563\n",
      "Epoch 95: val_accuracy improved from 0.99050 to 0.99083, saving model to CNN_Models\\Wide_CNN_Model-lr_0.001-bs_64-20250917-124118\\best_model_epoch-95_val_acc-0.9908.keras\n",
      "\n",
      "Epoch 95: val_accuracy improved from 0.99050 to 0.99083, saving model to wandb_models/Wide_CNN_Model-lr_0.001-bs_64-20250917-124118/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 93ms/step - accuracy: 0.9827 - loss: 0.0569 - val_accuracy: 0.9908 - val_loss: 0.0400\n",
      "Epoch 96/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - accuracy: 0.9835 - loss: 0.0540\n",
      "Epoch 96: val_accuracy did not improve from 0.99083\n",
      "\n",
      "Epoch 96: val_accuracy did not improve from 0.99083\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 98ms/step - accuracy: 0.9833 - loss: 0.0546 - val_accuracy: 0.9907 - val_loss: 0.0398\n",
      "Epoch 97/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - accuracy: 0.9840 - loss: 0.0536\n",
      "Epoch 97: val_accuracy did not improve from 0.99083\n",
      "\n",
      "Epoch 97: val_accuracy did not improve from 0.99083\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 97ms/step - accuracy: 0.9829 - loss: 0.0567 - val_accuracy: 0.9902 - val_loss: 0.0398\n",
      "Epoch 98/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.9834 - loss: 0.0540\n",
      "Epoch 98: val_accuracy did not improve from 0.99083\n",
      "\n",
      "Epoch 98: val_accuracy did not improve from 0.99083\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 92ms/step - accuracy: 0.9835 - loss: 0.0547 - val_accuracy: 0.9908 - val_loss: 0.0394\n",
      "Epoch 99/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - accuracy: 0.9842 - loss: 0.0527\n",
      "Epoch 99: val_accuracy improved from 0.99083 to 0.99117, saving model to CNN_Models\\Wide_CNN_Model-lr_0.001-bs_64-20250917-124118\\best_model_epoch-99_val_acc-0.9912.keras\n",
      "\n",
      "Epoch 99: val_accuracy improved from 0.99083 to 0.99117, saving model to wandb_models/Wide_CNN_Model-lr_0.001-bs_64-20250917-124118/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 101ms/step - accuracy: 0.9833 - loss: 0.0544 - val_accuracy: 0.9912 - val_loss: 0.0391\n",
      "Epoch 100/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.9832 - loss: 0.0564\n",
      "Epoch 100: val_accuracy did not improve from 0.99117\n",
      "\n",
      "Epoch 100: val_accuracy did not improve from 0.99117\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 92ms/step - accuracy: 0.9832 - loss: 0.0560 - val_accuracy: 0.9908 - val_loss: 0.0391\n",
      "\n",
      "Training history saved to: CNN_Models\\Wide_CNN_Model-lr_0.001-bs_64-20250917-124118\\training_history.pkl\n",
      "\n",
      "--- Peak Performance Summary ---\n",
      "Best validation accuracy:           0.9912\n",
      "Associated training accuracy:       0.9833\n",
      "Occurred at epoch:                  99\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch/accuracy</td><td>▁▄▅▅▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇███████████████████</td></tr><tr><td>epoch/epoch</td><td>▁▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇▇██</td></tr><tr><td>epoch/learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/loss</td><td>█▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/val_accuracy</td><td>▁▅▆▆▆▇▇▇▇▇▇▇▇▇▇█████████████████████████</td></tr><tr><td>epoch/val_loss</td><td>█▆▄▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch/accuracy</td><td>0.98317</td></tr><tr><td>epoch/epoch</td><td>99</td></tr><tr><td>epoch/learning_rate</td><td>0.001</td></tr><tr><td>epoch/loss</td><td>0.05604</td></tr><tr><td>epoch/val_accuracy</td><td>0.99083</td></tr><tr><td>epoch/val_loss</td><td>0.0391</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Wide_CNN_Model-lr_0.001-bs_64-20250917-124118</strong> at: <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/le61us34' target=\"_blank\">https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/le61us34</a><br> View project at: <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2' target=\"_blank\">https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2</a><br>Synced 5 W&B file(s), 0 media file(s), 108 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250917_124118-le61us34\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Adapting the normalisation layer...\n",
      "Adaptation complete.\n",
      "\n",
      "\n",
      "--- Starting Experiment: Wide_CNN_Model ---\n",
      "\n",
      "--- Model Architecture ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"Wide_CNN_Model\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"Wide_CNN_Model\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ normalization_31                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Normalization</span>)                 │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">640</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_31 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3200</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3200</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_118 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">819,456</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_119 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,570</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ normalization_31                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m1\u001b[0m)      │             \u001b[38;5;34m3\u001b[0m │\n",
       "│ (\u001b[38;5;33mNormalization\u001b[0m)                 │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_22 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │           \u001b[38;5;34m640\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_22 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_23 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │        \u001b[38;5;34m73,856\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_23 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_31 (\u001b[38;5;33mFlatten\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3200\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_7 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3200\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_118 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m819,456\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_119 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │         \u001b[38;5;34m2,570\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">896,525</span> (3.42 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m896,525\u001b[0m (3.42 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">896,522</span> (3.42 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m896,522\u001b[0m (3.42 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3</span> (16.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m3\u001b[0m (16.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Hyperparameters ---\n",
      "optimiser           : SGD\n",
      "learning_rate       : 0.0001\n",
      "epochs              : 100\n",
      "batch_size          : 64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for wandb.init()..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\TimVos\\VSC Projects\\CSE5ML\\Assessment 2\\wandb\\run-20250917_141723-a70u6ol3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/a70u6ol3' target=\"_blank\">Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723</a></strong> to <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2' target=\"_blank\">https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/a70u6ol3' target=\"_blank\">https://wandb.ai/tim-vos-nl-mine/CSE5ML-Assessment2/runs/a70u6ol3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 0.1275 - loss: 2.3370\n",
      "Epoch 1: val_accuracy improved from None to 0.22217, saving model to CNN_Models\\Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723\\best_model_epoch-01_val_acc-0.2222.keras\n",
      "\n",
      "Epoch 1: val_accuracy improved from None to 0.22217, saving model to wandb_models/Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 98ms/step - accuracy: 0.1431 - loss: 2.3163 - val_accuracy: 0.2222 - val_loss: 2.2465\n",
      "Epoch 2/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.1849 - loss: 2.2624\n",
      "Epoch 2: val_accuracy improved from 0.22217 to 0.37000, saving model to CNN_Models\\Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723\\best_model_epoch-02_val_acc-0.3700.keras\n",
      "\n",
      "Epoch 2: val_accuracy improved from 0.22217 to 0.37000, saving model to wandb_models/Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 93ms/step - accuracy: 0.2015 - loss: 2.2483 - val_accuracy: 0.3700 - val_loss: 2.1802\n",
      "Epoch 3/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - accuracy: 0.2486 - loss: 2.2059\n",
      "Epoch 3: val_accuracy improved from 0.37000 to 0.49217, saving model to CNN_Models\\Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723\\best_model_epoch-03_val_acc-0.4922.keras\n",
      "\n",
      "Epoch 3: val_accuracy improved from 0.37000 to 0.49217, saving model to wandb_models/Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 101ms/step - accuracy: 0.2647 - loss: 2.1916 - val_accuracy: 0.4922 - val_loss: 2.1113\n",
      "Epoch 4/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 0.3129 - loss: 2.1451\n",
      "Epoch 4: val_accuracy improved from 0.49217 to 0.60450, saving model to CNN_Models\\Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723\\best_model_epoch-04_val_acc-0.6045.keras\n",
      "\n",
      "Epoch 4: val_accuracy improved from 0.49217 to 0.60450, saving model to wandb_models/Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 96ms/step - accuracy: 0.3314 - loss: 2.1270 - val_accuracy: 0.6045 - val_loss: 2.0299\n",
      "Epoch 5/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 0.3763 - loss: 2.0714\n",
      "Epoch 5: val_accuracy improved from 0.60450 to 0.67100, saving model to CNN_Models\\Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723\\best_model_epoch-05_val_acc-0.6710.keras\n",
      "\n",
      "Epoch 5: val_accuracy improved from 0.60450 to 0.67100, saving model to wandb_models/Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 98ms/step - accuracy: 0.3884 - loss: 2.0521 - val_accuracy: 0.6710 - val_loss: 1.9277\n",
      "Epoch 6/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.4354 - loss: 1.9834\n",
      "Epoch 6: val_accuracy improved from 0.67100 to 0.71550, saving model to CNN_Models\\Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723\\best_model_epoch-06_val_acc-0.7155.keras\n",
      "\n",
      "Epoch 6: val_accuracy improved from 0.67100 to 0.71550, saving model to wandb_models/Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 95ms/step - accuracy: 0.4458 - loss: 1.9578 - val_accuracy: 0.7155 - val_loss: 1.7981\n",
      "Epoch 7/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.4810 - loss: 1.8726\n",
      "Epoch 7: val_accuracy improved from 0.71550 to 0.74483, saving model to CNN_Models\\Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723\\best_model_epoch-07_val_acc-0.7448.keras\n",
      "\n",
      "Epoch 7: val_accuracy improved from 0.71550 to 0.74483, saving model to wandb_models/Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 86ms/step - accuracy: 0.4926 - loss: 1.8399 - val_accuracy: 0.7448 - val_loss: 1.6393\n",
      "Epoch 8/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 0.5187 - loss: 1.7379\n",
      "Epoch 8: val_accuracy improved from 0.74483 to 0.76917, saving model to CNN_Models\\Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723\\best_model_epoch-08_val_acc-0.7692.keras\n",
      "\n",
      "Epoch 8: val_accuracy improved from 0.74483 to 0.76917, saving model to wandb_models/Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 78ms/step - accuracy: 0.5307 - loss: 1.7014 - val_accuracy: 0.7692 - val_loss: 1.4573\n",
      "Epoch 9/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - accuracy: 0.5656 - loss: 1.5843\n",
      "Epoch 9: val_accuracy improved from 0.76917 to 0.79133, saving model to CNN_Models\\Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723\\best_model_epoch-09_val_acc-0.7913.keras\n",
      "\n",
      "Epoch 9: val_accuracy improved from 0.76917 to 0.79133, saving model to wandb_models/Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 114ms/step - accuracy: 0.5720 - loss: 1.5473 - val_accuracy: 0.7913 - val_loss: 1.2674\n",
      "Epoch 10/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - accuracy: 0.5975 - loss: 1.4334\n",
      "Epoch 10: val_accuracy improved from 0.79133 to 0.80417, saving model to CNN_Models\\Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723\\best_model_epoch-10_val_acc-0.8042.keras\n",
      "\n",
      "Epoch 10: val_accuracy improved from 0.79133 to 0.80417, saving model to wandb_models/Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 97ms/step - accuracy: 0.6051 - loss: 1.3946 - val_accuracy: 0.8042 - val_loss: 1.0900\n",
      "Epoch 11/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.6322 - loss: 1.2877\n",
      "Epoch 11: val_accuracy improved from 0.80417 to 0.81917, saving model to CNN_Models\\Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723\\best_model_epoch-11_val_acc-0.8192.keras\n",
      "\n",
      "Epoch 11: val_accuracy improved from 0.80417 to 0.81917, saving model to wandb_models/Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 90ms/step - accuracy: 0.6400 - loss: 1.2543 - val_accuracy: 0.8192 - val_loss: 0.9385\n",
      "Epoch 12/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - accuracy: 0.6592 - loss: 1.1604\n",
      "Epoch 12: val_accuracy improved from 0.81917 to 0.83533, saving model to CNN_Models\\Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723\\best_model_epoch-12_val_acc-0.8353.keras\n",
      "\n",
      "Epoch 12: val_accuracy improved from 0.81917 to 0.83533, saving model to wandb_models/Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 100ms/step - accuracy: 0.6651 - loss: 1.1329 - val_accuracy: 0.8353 - val_loss: 0.8149\n",
      "Epoch 13/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.6837 - loss: 1.0559\n",
      "Epoch 13: val_accuracy improved from 0.83533 to 0.84883, saving model to CNN_Models\\Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723\\best_model_epoch-13_val_acc-0.8488.keras\n",
      "\n",
      "Epoch 13: val_accuracy improved from 0.83533 to 0.84883, saving model to wandb_models/Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 85ms/step - accuracy: 0.6877 - loss: 1.0354 - val_accuracy: 0.8488 - val_loss: 0.7179\n",
      "Epoch 14/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.6994 - loss: 0.9758\n",
      "Epoch 14: val_accuracy improved from 0.84883 to 0.85750, saving model to CNN_Models\\Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723\\best_model_epoch-14_val_acc-0.8575.keras\n",
      "\n",
      "Epoch 14: val_accuracy improved from 0.84883 to 0.85750, saving model to wandb_models/Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 90ms/step - accuracy: 0.7050 - loss: 0.9568 - val_accuracy: 0.8575 - val_loss: 0.6436\n",
      "Epoch 15/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - accuracy: 0.7219 - loss: 0.8974\n",
      "Epoch 15: val_accuracy improved from 0.85750 to 0.86550, saving model to CNN_Models\\Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723\\best_model_epoch-15_val_acc-0.8655.keras\n",
      "\n",
      "Epoch 15: val_accuracy improved from 0.85750 to 0.86550, saving model to wandb_models/Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 110ms/step - accuracy: 0.7251 - loss: 0.8846 - val_accuracy: 0.8655 - val_loss: 0.5826\n",
      "Epoch 16/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.7341 - loss: 0.8480\n",
      "Epoch 16: val_accuracy improved from 0.86550 to 0.87167, saving model to CNN_Models\\Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723\\best_model_epoch-16_val_acc-0.8717.keras\n",
      "\n",
      "Epoch 16: val_accuracy improved from 0.86550 to 0.87167, saving model to wandb_models/Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 108ms/step - accuracy: 0.7391 - loss: 0.8330 - val_accuracy: 0.8717 - val_loss: 0.5358\n",
      "Epoch 17/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - accuracy: 0.7492 - loss: 0.7921\n",
      "Epoch 17: val_accuracy improved from 0.87167 to 0.87983, saving model to CNN_Models\\Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723\\best_model_epoch-17_val_acc-0.8798.keras\n",
      "\n",
      "Epoch 17: val_accuracy improved from 0.87167 to 0.87983, saving model to wandb_models/Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 114ms/step - accuracy: 0.7514 - loss: 0.7845 - val_accuracy: 0.8798 - val_loss: 0.4964\n",
      "Epoch 18/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - accuracy: 0.7609 - loss: 0.7550\n",
      "Epoch 18: val_accuracy improved from 0.87983 to 0.88533, saving model to CNN_Models\\Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723\\best_model_epoch-18_val_acc-0.8853.keras\n",
      "\n",
      "Epoch 18: val_accuracy improved from 0.87983 to 0.88533, saving model to wandb_models/Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 116ms/step - accuracy: 0.7646 - loss: 0.7476 - val_accuracy: 0.8853 - val_loss: 0.4648\n",
      "Epoch 19/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - accuracy: 0.7709 - loss: 0.7234\n",
      "Epoch 19: val_accuracy improved from 0.88533 to 0.88817, saving model to CNN_Models\\Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723\\best_model_epoch-19_val_acc-0.8882.keras\n",
      "\n",
      "Epoch 19: val_accuracy improved from 0.88533 to 0.88817, saving model to wandb_models/Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 106ms/step - accuracy: 0.7766 - loss: 0.7103 - val_accuracy: 0.8882 - val_loss: 0.4370\n",
      "Epoch 20/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - accuracy: 0.7814 - loss: 0.6926\n",
      "Epoch 20: val_accuracy improved from 0.88817 to 0.89500, saving model to CNN_Models\\Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723\\best_model_epoch-20_val_acc-0.8950.keras\n",
      "\n",
      "Epoch 20: val_accuracy improved from 0.88817 to 0.89500, saving model to wandb_models/Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 103ms/step - accuracy: 0.7858 - loss: 0.6801 - val_accuracy: 0.8950 - val_loss: 0.4136\n",
      "Epoch 21/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.7909 - loss: 0.6549\n",
      "Epoch 21: val_accuracy improved from 0.89500 to 0.89867, saving model to CNN_Models\\Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723\\best_model_epoch-21_val_acc-0.8987.keras\n",
      "\n",
      "Epoch 21: val_accuracy improved from 0.89500 to 0.89867, saving model to wandb_models/Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 86ms/step - accuracy: 0.7926 - loss: 0.6513 - val_accuracy: 0.8987 - val_loss: 0.3939\n",
      "Epoch 22/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - accuracy: 0.7994 - loss: 0.6297\n",
      "Epoch 22: val_accuracy improved from 0.89867 to 0.90183, saving model to CNN_Models\\Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723\\best_model_epoch-22_val_acc-0.9018.keras\n",
      "\n",
      "Epoch 22: val_accuracy improved from 0.89867 to 0.90183, saving model to wandb_models/Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 98ms/step - accuracy: 0.8018 - loss: 0.6247 - val_accuracy: 0.9018 - val_loss: 0.3753\n",
      "Epoch 23/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 0.8041 - loss: 0.6129\n",
      "Epoch 23: val_accuracy improved from 0.90183 to 0.90383, saving model to CNN_Models\\Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723\\best_model_epoch-23_val_acc-0.9038.keras\n",
      "\n",
      "Epoch 23: val_accuracy improved from 0.90183 to 0.90383, saving model to wandb_models/Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 95ms/step - accuracy: 0.8081 - loss: 0.6045 - val_accuracy: 0.9038 - val_loss: 0.3602\n",
      "Epoch 24/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - accuracy: 0.8100 - loss: 0.5947\n",
      "Epoch 24: val_accuracy improved from 0.90383 to 0.90550, saving model to CNN_Models\\Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723\\best_model_epoch-24_val_acc-0.9055.keras\n",
      "\n",
      "Epoch 24: val_accuracy improved from 0.90383 to 0.90550, saving model to wandb_models/Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 98ms/step - accuracy: 0.8138 - loss: 0.5871 - val_accuracy: 0.9055 - val_loss: 0.3468\n",
      "Epoch 25/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - accuracy: 0.8218 - loss: 0.5675\n",
      "Epoch 25: val_accuracy improved from 0.90550 to 0.90767, saving model to CNN_Models\\Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723\\best_model_epoch-25_val_acc-0.9077.keras\n",
      "\n",
      "Epoch 25: val_accuracy improved from 0.90550 to 0.90767, saving model to wandb_models/Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 93ms/step - accuracy: 0.8238 - loss: 0.5652 - val_accuracy: 0.9077 - val_loss: 0.3352\n",
      "Epoch 26/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - accuracy: 0.8249 - loss: 0.5568\n",
      "Epoch 26: val_accuracy improved from 0.90767 to 0.90933, saving model to CNN_Models\\Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723\\best_model_epoch-26_val_acc-0.9093.keras\n",
      "\n",
      "Epoch 26: val_accuracy improved from 0.90767 to 0.90933, saving model to wandb_models/Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 98ms/step - accuracy: 0.8277 - loss: 0.5505 - val_accuracy: 0.9093 - val_loss: 0.3240\n",
      "Epoch 27/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - accuracy: 0.8329 - loss: 0.5377\n",
      "Epoch 27: val_accuracy improved from 0.90933 to 0.91150, saving model to CNN_Models\\Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723\\best_model_epoch-27_val_acc-0.9115.keras\n",
      "\n",
      "Epoch 27: val_accuracy improved from 0.90933 to 0.91150, saving model to wandb_models/Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 97ms/step - accuracy: 0.8335 - loss: 0.5363 - val_accuracy: 0.9115 - val_loss: 0.3134\n",
      "Epoch 28/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - accuracy: 0.8354 - loss: 0.5224\n",
      "Epoch 28: val_accuracy improved from 0.91150 to 0.91250, saving model to CNN_Models\\Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723\\best_model_epoch-28_val_acc-0.9125.keras\n",
      "\n",
      "Epoch 28: val_accuracy improved from 0.91150 to 0.91250, saving model to wandb_models/Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 102ms/step - accuracy: 0.8365 - loss: 0.5204 - val_accuracy: 0.9125 - val_loss: 0.3054\n",
      "Epoch 29/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.8424 - loss: 0.5099\n",
      "Epoch 29: val_accuracy improved from 0.91250 to 0.91500, saving model to CNN_Models\\Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723\\best_model_epoch-29_val_acc-0.9150.keras\n",
      "\n",
      "Epoch 29: val_accuracy improved from 0.91250 to 0.91500, saving model to wandb_models/Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 109ms/step - accuracy: 0.8422 - loss: 0.5074 - val_accuracy: 0.9150 - val_loss: 0.2958\n",
      "Epoch 30/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - accuracy: 0.8445 - loss: 0.4968\n",
      "Epoch 30: val_accuracy improved from 0.91500 to 0.91767, saving model to CNN_Models\\Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723\\best_model_epoch-30_val_acc-0.9177.keras\n",
      "\n",
      "Epoch 30: val_accuracy improved from 0.91500 to 0.91767, saving model to wandb_models/Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 96ms/step - accuracy: 0.8453 - loss: 0.4941 - val_accuracy: 0.9177 - val_loss: 0.2872\n",
      "Epoch 31/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - accuracy: 0.8495 - loss: 0.4848\n",
      "Epoch 31: val_accuracy improved from 0.91767 to 0.91950, saving model to CNN_Models\\Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723\\best_model_epoch-31_val_acc-0.9195.keras\n",
      "\n",
      "Epoch 31: val_accuracy improved from 0.91767 to 0.91950, saving model to wandb_models/Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 100ms/step - accuracy: 0.8499 - loss: 0.4830 - val_accuracy: 0.9195 - val_loss: 0.2807\n",
      "Epoch 32/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - accuracy: 0.8522 - loss: 0.4705\n",
      "Epoch 32: val_accuracy improved from 0.91950 to 0.92200, saving model to CNN_Models\\Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723\\best_model_epoch-32_val_acc-0.9220.keras\n",
      "\n",
      "Epoch 32: val_accuracy improved from 0.91950 to 0.92200, saving model to wandb_models/Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 98ms/step - accuracy: 0.8526 - loss: 0.4704 - val_accuracy: 0.9220 - val_loss: 0.2731\n",
      "Epoch 33/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - accuracy: 0.8536 - loss: 0.4663\n",
      "Epoch 33: val_accuracy improved from 0.92200 to 0.92317, saving model to CNN_Models\\Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723\\best_model_epoch-33_val_acc-0.9232.keras\n",
      "\n",
      "Epoch 33: val_accuracy improved from 0.92200 to 0.92317, saving model to wandb_models/Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 100ms/step - accuracy: 0.8567 - loss: 0.4615 - val_accuracy: 0.9232 - val_loss: 0.2678\n",
      "Epoch 34/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - accuracy: 0.8619 - loss: 0.4518\n",
      "Epoch 34: val_accuracy improved from 0.92317 to 0.92583, saving model to CNN_Models\\Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723\\best_model_epoch-34_val_acc-0.9258.keras\n",
      "\n",
      "Epoch 34: val_accuracy improved from 0.92317 to 0.92583, saving model to wandb_models/Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 100ms/step - accuracy: 0.8611 - loss: 0.4502 - val_accuracy: 0.9258 - val_loss: 0.2615\n",
      "Epoch 35/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - accuracy: 0.8655 - loss: 0.4381\n",
      "Epoch 35: val_accuracy improved from 0.92583 to 0.92817, saving model to CNN_Models\\Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723\\best_model_epoch-35_val_acc-0.9282.keras\n",
      "\n",
      "Epoch 35: val_accuracy improved from 0.92583 to 0.92817, saving model to wandb_models/Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 104ms/step - accuracy: 0.8652 - loss: 0.4397 - val_accuracy: 0.9282 - val_loss: 0.2554\n",
      "Epoch 36/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - accuracy: 0.8667 - loss: 0.4321\n",
      "Epoch 36: val_accuracy improved from 0.92817 to 0.92900, saving model to CNN_Models\\Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723\\best_model_epoch-36_val_acc-0.9290.keras\n",
      "\n",
      "Epoch 36: val_accuracy improved from 0.92817 to 0.92900, saving model to wandb_models/Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 98ms/step - accuracy: 0.8660 - loss: 0.4322 - val_accuracy: 0.9290 - val_loss: 0.2507\n",
      "Epoch 37/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.8700 - loss: 0.4269\n",
      "Epoch 37: val_accuracy improved from 0.92900 to 0.93067, saving model to CNN_Models\\Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723\\best_model_epoch-37_val_acc-0.9307.keras\n",
      "\n",
      "Epoch 37: val_accuracy improved from 0.92900 to 0.93067, saving model to wandb_models/Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 91ms/step - accuracy: 0.8702 - loss: 0.4249 - val_accuracy: 0.9307 - val_loss: 0.2454\n",
      "Epoch 38/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - accuracy: 0.8720 - loss: 0.4146\n",
      "Epoch 38: val_accuracy improved from 0.93067 to 0.93217, saving model to CNN_Models\\Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723\\best_model_epoch-38_val_acc-0.9322.keras\n",
      "\n",
      "Epoch 38: val_accuracy improved from 0.93067 to 0.93217, saving model to wandb_models/Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 109ms/step - accuracy: 0.8727 - loss: 0.4135 - val_accuracy: 0.9322 - val_loss: 0.2407\n",
      "Epoch 39/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - accuracy: 0.8733 - loss: 0.4129\n",
      "Epoch 39: val_accuracy improved from 0.93217 to 0.93333, saving model to CNN_Models\\Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723\\best_model_epoch-39_val_acc-0.9333.keras\n",
      "\n",
      "Epoch 39: val_accuracy improved from 0.93217 to 0.93333, saving model to wandb_models/Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m149s\u001b[0m 116ms/step - accuracy: 0.8738 - loss: 0.4121 - val_accuracy: 0.9333 - val_loss: 0.2361\n",
      "Epoch 40/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - accuracy: 0.8763 - loss: 0.4005\n",
      "Epoch 40: val_accuracy improved from 0.93333 to 0.93417, saving model to CNN_Models\\Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723\\best_model_epoch-40_val_acc-0.9342.keras\n",
      "\n",
      "Epoch 40: val_accuracy improved from 0.93333 to 0.93417, saving model to wandb_models/Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 117ms/step - accuracy: 0.8767 - loss: 0.4001 - val_accuracy: 0.9342 - val_loss: 0.2318\n",
      "Epoch 41/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.8807 - loss: 0.3919\n",
      "Epoch 41: val_accuracy improved from 0.93417 to 0.93583, saving model to CNN_Models\\Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723\\best_model_epoch-41_val_acc-0.9358.keras\n",
      "\n",
      "Epoch 41: val_accuracy improved from 0.93417 to 0.93583, saving model to wandb_models/Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 111ms/step - accuracy: 0.8802 - loss: 0.3922 - val_accuracy: 0.9358 - val_loss: 0.2275\n",
      "Epoch 42/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.8817 - loss: 0.3846\n",
      "Epoch 42: val_accuracy improved from 0.93583 to 0.93700, saving model to CNN_Models\\Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723\\best_model_epoch-42_val_acc-0.9370.keras\n",
      "\n",
      "Epoch 42: val_accuracy improved from 0.93583 to 0.93700, saving model to wandb_models/Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 110ms/step - accuracy: 0.8827 - loss: 0.3862 - val_accuracy: 0.9370 - val_loss: 0.2239\n",
      "Epoch 43/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step - accuracy: 0.8855 - loss: 0.3795\n",
      "Epoch 43: val_accuracy improved from 0.93700 to 0.93833, saving model to CNN_Models\\Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723\\best_model_epoch-43_val_acc-0.9383.keras\n",
      "\n",
      "Epoch 43: val_accuracy improved from 0.93700 to 0.93833, saving model to wandb_models/Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 123ms/step - accuracy: 0.8845 - loss: 0.3791 - val_accuracy: 0.9383 - val_loss: 0.2202\n",
      "Epoch 44/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.8856 - loss: 0.3737\n",
      "Epoch 44: val_accuracy improved from 0.93833 to 0.93933, saving model to CNN_Models\\Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723\\best_model_epoch-44_val_acc-0.9393.keras\n",
      "\n",
      "Epoch 44: val_accuracy improved from 0.93833 to 0.93933, saving model to wandb_models/Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 110ms/step - accuracy: 0.8867 - loss: 0.3729 - val_accuracy: 0.9393 - val_loss: 0.2160\n",
      "Epoch 45/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.8863 - loss: 0.3712\n",
      "Epoch 45: val_accuracy improved from 0.93933 to 0.94067, saving model to CNN_Models\\Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723\\best_model_epoch-45_val_acc-0.9407.keras\n",
      "\n",
      "Epoch 45: val_accuracy improved from 0.93933 to 0.94067, saving model to wandb_models/Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 110ms/step - accuracy: 0.8884 - loss: 0.3672 - val_accuracy: 0.9407 - val_loss: 0.2132\n",
      "Epoch 46/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 0.8891 - loss: 0.3641\n",
      "Epoch 46: val_accuracy improved from 0.94067 to 0.94183, saving model to CNN_Models\\Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723\\best_model_epoch-46_val_acc-0.9418.keras\n",
      "\n",
      "Epoch 46: val_accuracy improved from 0.94067 to 0.94183, saving model to wandb_models/Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m131s\u001b[0m 97ms/step - accuracy: 0.8896 - loss: 0.3625 - val_accuracy: 0.9418 - val_loss: 0.2097\n",
      "Epoch 47/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - accuracy: 0.8930 - loss: 0.3551\n",
      "Epoch 47: val_accuracy improved from 0.94183 to 0.94233, saving model to CNN_Models\\Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723\\best_model_epoch-47_val_acc-0.9423.keras\n",
      "\n",
      "Epoch 47: val_accuracy improved from 0.94183 to 0.94233, saving model to wandb_models/Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 121ms/step - accuracy: 0.8924 - loss: 0.3560 - val_accuracy: 0.9423 - val_loss: 0.2065\n",
      "Epoch 48/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - accuracy: 0.8937 - loss: 0.3491\n",
      "Epoch 48: val_accuracy improved from 0.94233 to 0.94450, saving model to CNN_Models\\Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723\\best_model_epoch-48_val_acc-0.9445.keras\n",
      "\n",
      "Epoch 48: val_accuracy improved from 0.94233 to 0.94450, saving model to wandb_models/Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 117ms/step - accuracy: 0.8926 - loss: 0.3503 - val_accuracy: 0.9445 - val_loss: 0.2032\n",
      "Epoch 49/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - accuracy: 0.8946 - loss: 0.3473\n",
      "Epoch 49: val_accuracy did not improve from 0.94450\n",
      "\n",
      "Epoch 49: val_accuracy did not improve from 0.94450\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 158ms/step - accuracy: 0.8946 - loss: 0.3488 - val_accuracy: 0.9443 - val_loss: 0.2001\n",
      "Epoch 50/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - accuracy: 0.8963 - loss: 0.3379\n",
      "Epoch 50: val_accuracy improved from 0.94450 to 0.94500, saving model to CNN_Models\\Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723\\best_model_epoch-50_val_acc-0.9450.keras\n",
      "\n",
      "Epoch 50: val_accuracy improved from 0.94450 to 0.94500, saving model to wandb_models/Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 144ms/step - accuracy: 0.8968 - loss: 0.3396 - val_accuracy: 0.9450 - val_loss: 0.1979\n",
      "Epoch 51/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.8975 - loss: 0.3364\n",
      "Epoch 51: val_accuracy improved from 0.94500 to 0.94633, saving model to CNN_Models\\Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723\\best_model_epoch-51_val_acc-0.9463.keras\n",
      "\n",
      "Epoch 51: val_accuracy improved from 0.94500 to 0.94633, saving model to wandb_models/Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 136ms/step - accuracy: 0.8978 - loss: 0.3370 - val_accuracy: 0.9463 - val_loss: 0.1945\n",
      "Epoch 52/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - accuracy: 0.9000 - loss: 0.3290\n",
      "Epoch 52: val_accuracy did not improve from 0.94633\n",
      "\n",
      "Epoch 52: val_accuracy did not improve from 0.94633\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 125ms/step - accuracy: 0.8992 - loss: 0.3308 - val_accuracy: 0.9463 - val_loss: 0.1928\n",
      "Epoch 53/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - accuracy: 0.9009 - loss: 0.3254\n",
      "Epoch 53: val_accuracy improved from 0.94633 to 0.94783, saving model to CNN_Models\\Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723\\best_model_epoch-53_val_acc-0.9478.keras\n",
      "\n",
      "Epoch 53: val_accuracy improved from 0.94633 to 0.94783, saving model to wandb_models/Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 109ms/step - accuracy: 0.9004 - loss: 0.3276 - val_accuracy: 0.9478 - val_loss: 0.1897\n",
      "Epoch 54/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - accuracy: 0.9005 - loss: 0.3238\n",
      "Epoch 54: val_accuracy improved from 0.94783 to 0.94817, saving model to CNN_Models\\Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723\\best_model_epoch-54_val_acc-0.9482.keras\n",
      "\n",
      "Epoch 54: val_accuracy improved from 0.94783 to 0.94817, saving model to wandb_models/Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 109ms/step - accuracy: 0.9015 - loss: 0.3230 - val_accuracy: 0.9482 - val_loss: 0.1874\n",
      "Epoch 55/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - accuracy: 0.9044 - loss: 0.3184\n",
      "Epoch 55: val_accuracy improved from 0.94817 to 0.94900, saving model to CNN_Models\\Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723\\best_model_epoch-55_val_acc-0.9490.keras\n",
      "\n",
      "Epoch 55: val_accuracy improved from 0.94817 to 0.94900, saving model to wandb_models/Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 108ms/step - accuracy: 0.9029 - loss: 0.3212 - val_accuracy: 0.9490 - val_loss: 0.1849\n",
      "Epoch 56/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.9040 - loss: 0.3152\n",
      "Epoch 56: val_accuracy improved from 0.94900 to 0.94933, saving model to CNN_Models\\Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723\\best_model_epoch-56_val_acc-0.9493.keras\n",
      "\n",
      "Epoch 56: val_accuracy improved from 0.94900 to 0.94933, saving model to wandb_models/Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 111ms/step - accuracy: 0.9039 - loss: 0.3169 - val_accuracy: 0.9493 - val_loss: 0.1831\n",
      "Epoch 57/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - accuracy: 0.9064 - loss: 0.3118\n",
      "Epoch 57: val_accuracy improved from 0.94933 to 0.95017, saving model to CNN_Models\\Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723\\best_model_epoch-57_val_acc-0.9502.keras\n",
      "\n",
      "Epoch 57: val_accuracy improved from 0.94933 to 0.95017, saving model to wandb_models/Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m147s\u001b[0m 117ms/step - accuracy: 0.9072 - loss: 0.3112 - val_accuracy: 0.9502 - val_loss: 0.1807\n",
      "Epoch 58/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.9070 - loss: 0.3087\n",
      "Epoch 58: val_accuracy did not improve from 0.95017\n",
      "\n",
      "Epoch 58: val_accuracy did not improve from 0.95017\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m148s\u001b[0m 124ms/step - accuracy: 0.9068 - loss: 0.3078 - val_accuracy: 0.9500 - val_loss: 0.1784\n",
      "Epoch 59/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - accuracy: 0.9083 - loss: 0.3036\n",
      "Epoch 59: val_accuracy improved from 0.95017 to 0.95167, saving model to CNN_Models\\Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723\\best_model_epoch-59_val_acc-0.9517.keras\n",
      "\n",
      "Epoch 59: val_accuracy improved from 0.95017 to 0.95167, saving model to wandb_models/Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 136ms/step - accuracy: 0.9081 - loss: 0.3036 - val_accuracy: 0.9517 - val_loss: 0.1768\n",
      "Epoch 60/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.9117 - loss: 0.2983\n",
      "Epoch 60: val_accuracy improved from 0.95167 to 0.95233, saving model to CNN_Models\\Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723\\best_model_epoch-60_val_acc-0.9523.keras\n",
      "\n",
      "Epoch 60: val_accuracy improved from 0.95167 to 0.95233, saving model to wandb_models/Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 131ms/step - accuracy: 0.9107 - loss: 0.2989 - val_accuracy: 0.9523 - val_loss: 0.1740\n",
      "Epoch 61/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - accuracy: 0.9087 - loss: 0.2974\n",
      "Epoch 61: val_accuracy improved from 0.95233 to 0.95283, saving model to CNN_Models\\Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723\\best_model_epoch-61_val_acc-0.9528.keras\n",
      "\n",
      "Epoch 61: val_accuracy improved from 0.95233 to 0.95283, saving model to wandb_models/Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 115ms/step - accuracy: 0.9106 - loss: 0.2948 - val_accuracy: 0.9528 - val_loss: 0.1721\n",
      "Epoch 62/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - accuracy: 0.9127 - loss: 0.2904\n",
      "Epoch 62: val_accuracy improved from 0.95283 to 0.95317, saving model to CNN_Models\\Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723\\best_model_epoch-62_val_acc-0.9532.keras\n",
      "\n",
      "Epoch 62: val_accuracy improved from 0.95283 to 0.95317, saving model to wandb_models/Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 130ms/step - accuracy: 0.9121 - loss: 0.2933 - val_accuracy: 0.9532 - val_loss: 0.1707\n",
      "Epoch 63/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - accuracy: 0.9124 - loss: 0.2878\n",
      "Epoch 63: val_accuracy improved from 0.95317 to 0.95383, saving model to CNN_Models\\Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723\\best_model_epoch-63_val_acc-0.9538.keras\n",
      "\n",
      "Epoch 63: val_accuracy improved from 0.95317 to 0.95383, saving model to wandb_models/Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 151ms/step - accuracy: 0.9120 - loss: 0.2882 - val_accuracy: 0.9538 - val_loss: 0.1685\n",
      "Epoch 64/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - accuracy: 0.9158 - loss: 0.2840\n",
      "Epoch 64: val_accuracy did not improve from 0.95383\n",
      "\n",
      "Epoch 64: val_accuracy did not improve from 0.95383\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 145ms/step - accuracy: 0.9145 - loss: 0.2852 - val_accuracy: 0.9538 - val_loss: 0.1669\n",
      "Epoch 65/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - accuracy: 0.9148 - loss: 0.2826\n",
      "Epoch 65: val_accuracy improved from 0.95383 to 0.95433, saving model to CNN_Models\\Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723\\best_model_epoch-65_val_acc-0.9543.keras\n",
      "\n",
      "Epoch 65: val_accuracy improved from 0.95383 to 0.95433, saving model to wandb_models/Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 105ms/step - accuracy: 0.9140 - loss: 0.2837 - val_accuracy: 0.9543 - val_loss: 0.1651\n",
      "Epoch 66/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - accuracy: 0.9150 - loss: 0.2823\n",
      "Epoch 66: val_accuracy did not improve from 0.95433\n",
      "\n",
      "Epoch 66: val_accuracy did not improve from 0.95433\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 108ms/step - accuracy: 0.9139 - loss: 0.2845 - val_accuracy: 0.9543 - val_loss: 0.1636\n",
      "Epoch 67/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - accuracy: 0.9177 - loss: 0.2759\n",
      "Epoch 67: val_accuracy improved from 0.95433 to 0.95450, saving model to CNN_Models\\Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723\\best_model_epoch-67_val_acc-0.9545.keras\n",
      "\n",
      "Epoch 67: val_accuracy improved from 0.95433 to 0.95450, saving model to wandb_models/Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 172ms/step - accuracy: 0.9165 - loss: 0.2799 - val_accuracy: 0.9545 - val_loss: 0.1621\n",
      "Epoch 68/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 168ms/step - accuracy: 0.9178 - loss: 0.2742\n",
      "Epoch 68: val_accuracy improved from 0.95450 to 0.95500, saving model to CNN_Models\\Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723\\best_model_epoch-68_val_acc-0.9550.keras\n",
      "\n",
      "Epoch 68: val_accuracy improved from 0.95450 to 0.95500, saving model to wandb_models/Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 174ms/step - accuracy: 0.9183 - loss: 0.2738 - val_accuracy: 0.9550 - val_loss: 0.1603\n",
      "Epoch 69/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - accuracy: 0.9205 - loss: 0.2682\n",
      "Epoch 69: val_accuracy did not improve from 0.95500\n",
      "\n",
      "Epoch 69: val_accuracy did not improve from 0.95500\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 116ms/step - accuracy: 0.9188 - loss: 0.2727 - val_accuracy: 0.9548 - val_loss: 0.1590\n",
      "Epoch 70/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - accuracy: 0.9197 - loss: 0.2703\n",
      "Epoch 70: val_accuracy improved from 0.95500 to 0.95533, saving model to CNN_Models\\Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723\\best_model_epoch-70_val_acc-0.9553.keras\n",
      "\n",
      "Epoch 70: val_accuracy improved from 0.95500 to 0.95533, saving model to wandb_models/Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m163s\u001b[0m 140ms/step - accuracy: 0.9188 - loss: 0.2711 - val_accuracy: 0.9553 - val_loss: 0.1576\n",
      "Epoch 71/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - accuracy: 0.9217 - loss: 0.2663\n",
      "Epoch 71: val_accuracy did not improve from 0.95533\n",
      "\n",
      "Epoch 71: val_accuracy did not improve from 0.95533\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 137ms/step - accuracy: 0.9200 - loss: 0.2688 - val_accuracy: 0.9553 - val_loss: 0.1560\n",
      "Epoch 72/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - accuracy: 0.9208 - loss: 0.2636\n",
      "Epoch 72: val_accuracy improved from 0.95533 to 0.95600, saving model to CNN_Models\\Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723\\best_model_epoch-72_val_acc-0.9560.keras\n",
      "\n",
      "Epoch 72: val_accuracy improved from 0.95533 to 0.95600, saving model to wandb_models/Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 121ms/step - accuracy: 0.9198 - loss: 0.2647 - val_accuracy: 0.9560 - val_loss: 0.1547\n",
      "Epoch 73/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - accuracy: 0.9230 - loss: 0.2576\n",
      "Epoch 73: val_accuracy improved from 0.95600 to 0.95617, saving model to CNN_Models\\Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723\\best_model_epoch-73_val_acc-0.9562.keras\n",
      "\n",
      "Epoch 73: val_accuracy improved from 0.95600 to 0.95617, saving model to wandb_models/Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 118ms/step - accuracy: 0.9216 - loss: 0.2619 - val_accuracy: 0.9562 - val_loss: 0.1530\n",
      "Epoch 74/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - accuracy: 0.9241 - loss: 0.2574\n",
      "Epoch 74: val_accuracy improved from 0.95617 to 0.95700, saving model to CNN_Models\\Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723\\best_model_epoch-74_val_acc-0.9570.keras\n",
      "\n",
      "Epoch 74: val_accuracy improved from 0.95617 to 0.95700, saving model to wandb_models/Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 125ms/step - accuracy: 0.9220 - loss: 0.2616 - val_accuracy: 0.9570 - val_loss: 0.1515\n",
      "Epoch 75/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.9226 - loss: 0.2547\n",
      "Epoch 75: val_accuracy did not improve from 0.95700\n",
      "\n",
      "Epoch 75: val_accuracy did not improve from 0.95700\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 121ms/step - accuracy: 0.9221 - loss: 0.2575 - val_accuracy: 0.9570 - val_loss: 0.1505\n",
      "Epoch 76/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.9224 - loss: 0.2534\n",
      "Epoch 76: val_accuracy improved from 0.95700 to 0.95750, saving model to CNN_Models\\Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723\\best_model_epoch-76_val_acc-0.9575.keras\n",
      "\n",
      "Epoch 76: val_accuracy improved from 0.95700 to 0.95750, saving model to wandb_models/Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 113ms/step - accuracy: 0.9236 - loss: 0.2533 - val_accuracy: 0.9575 - val_loss: 0.1494\n",
      "Epoch 77/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - accuracy: 0.9273 - loss: 0.2463\n",
      "Epoch 77: val_accuracy improved from 0.95750 to 0.95867, saving model to CNN_Models\\Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723\\best_model_epoch-77_val_acc-0.9587.keras\n",
      "\n",
      "Epoch 77: val_accuracy improved from 0.95750 to 0.95867, saving model to wandb_models/Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 116ms/step - accuracy: 0.9252 - loss: 0.2509 - val_accuracy: 0.9587 - val_loss: 0.1477\n",
      "Epoch 78/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9263 - loss: 0.2456\n",
      "Epoch 78: val_accuracy improved from 0.95867 to 0.95883, saving model to CNN_Models\\Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723\\best_model_epoch-78_val_acc-0.9588.keras\n",
      "\n",
      "Epoch 78: val_accuracy improved from 0.95867 to 0.95883, saving model to wandb_models/Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 110ms/step - accuracy: 0.9248 - loss: 0.2501 - val_accuracy: 0.9588 - val_loss: 0.1469\n",
      "Epoch 79/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - accuracy: 0.9264 - loss: 0.2462\n",
      "Epoch 79: val_accuracy improved from 0.95883 to 0.96033, saving model to CNN_Models\\Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723\\best_model_epoch-79_val_acc-0.9603.keras\n",
      "\n",
      "Epoch 79: val_accuracy improved from 0.95883 to 0.96033, saving model to wandb_models/Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 113ms/step - accuracy: 0.9252 - loss: 0.2497 - val_accuracy: 0.9603 - val_loss: 0.1455\n",
      "Epoch 80/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9284 - loss: 0.2413\n",
      "Epoch 80: val_accuracy improved from 0.96033 to 0.96100, saving model to CNN_Models\\Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723\\best_model_epoch-80_val_acc-0.9610.keras\n",
      "\n",
      "Epoch 80: val_accuracy improved from 0.96033 to 0.96100, saving model to wandb_models/Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 112ms/step - accuracy: 0.9262 - loss: 0.2451 - val_accuracy: 0.9610 - val_loss: 0.1441\n",
      "Epoch 81/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - accuracy: 0.9266 - loss: 0.2429\n",
      "Epoch 81: val_accuracy improved from 0.96100 to 0.96117, saving model to CNN_Models\\Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723\\best_model_epoch-81_val_acc-0.9612.keras\n",
      "\n",
      "Epoch 81: val_accuracy improved from 0.96100 to 0.96117, saving model to wandb_models/Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 105ms/step - accuracy: 0.9255 - loss: 0.2452 - val_accuracy: 0.9612 - val_loss: 0.1433\n",
      "Epoch 82/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.9270 - loss: 0.2437\n",
      "Epoch 82: val_accuracy improved from 0.96117 to 0.96150, saving model to CNN_Models\\Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723\\best_model_epoch-82_val_acc-0.9615.keras\n",
      "\n",
      "Epoch 82: val_accuracy improved from 0.96117 to 0.96150, saving model to wandb_models/Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 109ms/step - accuracy: 0.9269 - loss: 0.2444 - val_accuracy: 0.9615 - val_loss: 0.1420\n",
      "Epoch 83/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.9289 - loss: 0.2379\n",
      "Epoch 83: val_accuracy did not improve from 0.96150\n",
      "\n",
      "Epoch 83: val_accuracy did not improve from 0.96150\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 108ms/step - accuracy: 0.9278 - loss: 0.2403 - val_accuracy: 0.9615 - val_loss: 0.1410\n",
      "Epoch 84/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - accuracy: 0.9307 - loss: 0.2377\n",
      "Epoch 84: val_accuracy improved from 0.96150 to 0.96183, saving model to CNN_Models\\Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723\\best_model_epoch-84_val_acc-0.9618.keras\n",
      "\n",
      "Epoch 84: val_accuracy improved from 0.96150 to 0.96183, saving model to wandb_models/Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 108ms/step - accuracy: 0.9302 - loss: 0.2385 - val_accuracy: 0.9618 - val_loss: 0.1399\n",
      "Epoch 85/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - accuracy: 0.9294 - loss: 0.2359\n",
      "Epoch 85: val_accuracy improved from 0.96183 to 0.96217, saving model to CNN_Models\\Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723\\best_model_epoch-85_val_acc-0.9622.keras\n",
      "\n",
      "Epoch 85: val_accuracy improved from 0.96183 to 0.96217, saving model to wandb_models/Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 116ms/step - accuracy: 0.9290 - loss: 0.2365 - val_accuracy: 0.9622 - val_loss: 0.1390\n",
      "Epoch 86/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.9292 - loss: 0.2343\n",
      "Epoch 86: val_accuracy did not improve from 0.96217\n",
      "\n",
      "Epoch 86: val_accuracy did not improve from 0.96217\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 119ms/step - accuracy: 0.9284 - loss: 0.2371 - val_accuracy: 0.9622 - val_loss: 0.1378\n",
      "Epoch 87/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.9294 - loss: 0.2337\n",
      "Epoch 87: val_accuracy improved from 0.96217 to 0.96317, saving model to CNN_Models\\Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723\\best_model_epoch-87_val_acc-0.9632.keras\n",
      "\n",
      "Epoch 87: val_accuracy improved from 0.96217 to 0.96317, saving model to wandb_models/Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 110ms/step - accuracy: 0.9293 - loss: 0.2341 - val_accuracy: 0.9632 - val_loss: 0.1368\n",
      "Epoch 88/100\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - accuracy: 0.9318 - loss: 0.2307\n",
      "Epoch 88: val_accuracy improved from 0.96317 to 0.96350, saving model to CNN_Models\\Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723\\best_model_epoch-88_val_acc-0.9635.keras\n",
      "\n",
      "Epoch 88: val_accuracy improved from 0.96317 to 0.96350, saving model to wandb_models/Wide_CNN_Model-lr_0.0001-bs_64-20250917-141723/best_model.keras\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 122ms/step - accuracy: 0.9314 - loss: 0.2318 - val_accuracy: 0.9635 - val_loss: 0.1356\n",
      "Epoch 89/100\n",
      "\u001b[1m143/844\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:23\u001b[0m 204ms/step - accuracy: 0.9305 - loss: 0.2265"
     ]
    }
   ],
   "source": [
    "run_cnn_experiments: bool = True # A simple flag to control whether we run the CNN experiments or not. This is convenient because CNN experiments take longer to run.\n",
    "# and when we restart the notebook, we might want to just run the MLP experiments first.\n",
    "all_exp_params: bool = True\n",
    "\n",
    "# --- Run the full list of experiments ---\n",
    "if run_cnn_experiments:\n",
    "    cnn_histories: List[History] = []\n",
    "    for cnn_model_function in cnn_model_functions:\n",
    "        if not all_exp_params:\n",
    "            cnn_history: History = run_experiment(\n",
    "                model_creation_func=cnn_model_function, \n",
    "                hyperparameters=cnn_exp_1_config,  # If we do not set the all experiment parameters to True, then we only need the 1 config file. \n",
    "                parent_folder='CNN_Models',\n",
    "                X_train=X_train,\n",
    "                Y_train=Y_train,\n",
    "            )\n",
    "        else:\n",
    "            for config in cnn_config:\n",
    "                cnn_history: History = run_experiment(\n",
    "                    model_creation_func=cnn_model_function, \n",
    "                    hyperparameters=config, \n",
    "                    parent_folder='CNN_Models',\n",
    "                    X_train=X_train,\n",
    "                    Y_train=Y_train,\n",
    "                )\n",
    "\n",
    "        cnn_histories.append(cnn_history)\n",
    "\n",
    "# # --- To run the second experiment, we just call it again with a different config (hyper parameter set) :-)\n",
    "# --- We first  test different model architectures before running more experiments with different hyperparameters (Part 1 Task 1 & 2). ---\n",
    "# --- THen we test the best performing model of Part 1 Task 1& 2 with different hyperparameters. ---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a722801",
   "metadata": {},
   "source": [
    "# Plotting all model architectures\n",
    "\n",
    "After creating the model architecture, we now plot the architectures graphically. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c7eefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The 'text_callable' function is a special callback used by the visualkeras library. \n",
    "# Its main job is to create the text label that appears on top of or next to each\n",
    "# layer block in the generated visualisation. It's called once for every layer in your model.\n",
    "\n",
    "def text_callable(layer_index: int, layer: tf.keras.layers.Layer) -> Tuple[str, bool]:\n",
    "    # 'layer_index' is an integer representing the position of the layer in the model's sequence.\n",
    "    # 'layer' is the actual Keras layer object itself (e.g., a Conv2D, Dense, or Flatten layer).\n",
    "\n",
    "    # The boolean value 'above' is used to alternate the vertical position of the text labels.\n",
    "    # By using the modulo operator (%), we check if the layer's index is odd or even.\n",
    "    # This ensures that the text labels don't overlap, making the visualisation cleaner.\n",
    "    above = bool(layer_index % 2)\n",
    "\n",
    "    # We use a 'try...except' block as a defensive programming practice.\n",
    "    # This helps the code run without crashing even if it encounters unexpected errors.\n",
    "    try:\n",
    "        # We first try to get the layer's output shape using the 'layer.output.shape' attribute.\n",
    "        # This is the standard way to get shape information in modern TensorFlow.\n",
    "        shape = layer.output.shape\n",
    "    except AttributeError:\n",
    "        # If the first attempt fails (e.g., if the layer is not yet built), we move to this block.\n",
    "        # This is a fallback to ensure compatibility with different Keras versions and layer types.\n",
    "        try:\n",
    "            # Here, we try to access the 'layer.output_shape' attribute, which was more common\n",
    "            # in older versions of Keras. This helps the function work with a wider range of models.\n",
    "            shape = layer.output_shape\n",
    "        except AttributeError:\n",
    "            # If both attempts to get the shape fail, it means the layer doesn't have a defined output shape.\n",
    "            # This can happen for a number of reasons. In this case, we return a label\n",
    "            # that says \"Shape N/A\" to indicate the issue without crashing the program.\n",
    "            return f\"{layer.name}\\n(Shape N/A)\", above\n",
    "\n",
    "    # Keras represents shapes in various ways. For instance, a single-output layer might\n",
    "    # return a shape as a list, a tuple, or a special 'TensorShape' object.\n",
    "    # This 'if' statement handles the case where the shape is returned as a nested list,\n",
    "    # ensuring we only work with the first (and typically most relevant) shape.\n",
    "    if isinstance(shape, list):\n",
    "        shape = shape[0]\n",
    "\n",
    "    # 'TensorShape' is a specific object type in TensorFlow. To manipulate its values,\n",
    "    # we need to convert it into a standard Python list. The 'as_list()' method does this for us.\n",
    "    if isinstance(shape, tf.TensorShape):\n",
    "        output_shape = shape.as_list()\n",
    "    else:\n",
    "        # If the shape is already a list or tuple, we simply convert it to a list.\n",
    "        output_shape = list(shape)\n",
    "    \n",
    "    # Keras models often have a 'None' dimension in their shape to represent the\n",
    "    # variable batch size. For our visualisation, we don't need this, as it would just\n",
    "    # clutter the output. This line uses a list comprehension to filter out the 'None' value.\n",
    "    output_shape = [dim for dim in output_shape if dim is not None]\n",
    "    \n",
    "    # We add an offset to the text position here.\n",
    "    # The offset will push the text away from the layers.\n",
    "    # This simulates padding around the text.\n",
    "    # \n",
    "    # Please note that the actual padding is controlled by the 'padding' and 'spacing'\n",
    "    # parameters in the 'visualkeras.layered_view()' function, but a clever use of\n",
    "    # this function can also help with text positioning.\n",
    "\n",
    "    # We initialize an empty string to build our final text label.\n",
    "    output_shape_txt = \"\"\n",
    "\n",
    "    # 'enumerate' is a handy built-in Python function. It allows us to loop through a sequence\n",
    "    # (in this case, our list of shape dimensions) and get both the index ('ii') and the\n",
    "    # value ('dim') at the same time. This is perfect for when we need to perform an action\n",
    "    # based on an item's position, like adding 'x' or a newline.\n",
    "    for ii, dim in enumerate(output_shape):\n",
    "        # We append the current dimension (as a string) to our text label.\n",
    "        output_shape_txt += str(dim)\n",
    "        \n",
    "        # This line is for debugging purposes. It prints the shape to the console\n",
    "        # to help us verify what the function is seeing. We can remove this later.\n",
    "        # print(f'Output shape: {output_shape}')\n",
    "\n",
    "        # We want to add an 'x' between the dimensions (e.g., 28x28). This 'if' statement\n",
    "        # checks if we are not at the last two dimensions of the shape, as the format is different there.\n",
    "        if ii < len(output_shape) - 2:\n",
    "            output_shape_txt += \"x\"\n",
    "\n",
    "        # This 'if' statement adds a newline character to create the desired format\n",
    "        # where the last dimension is on a new line (e.g., 28x28\\n64).\n",
    "        if ii == len(output_shape) - 2:\n",
    "            output_shape_txt += \"\\n\" #\\n here is the space between the output shape and nr of filteres (e.g. 28 x 28, 32) and the keras name for the layer. \n",
    "\n",
    "\n",
    "    # --- General Check for Attributes ---\n",
    "    # We check if the layer has an 'activation' attribute.\n",
    "    if hasattr(layer, 'activation') and layer.activation is not None:\n",
    "        activation = layer.activation.__name__\n",
    "        output_shape_txt += f\"\\n{activation}\"\n",
    "    \n",
    "    # We check if the layer has a 'kernel_size' attribute.\n",
    "    if hasattr(layer, 'kernel_size') and layer.kernel_size is not None:\n",
    "        kernel_size = layer.kernel_size\n",
    "        output_shape_txt += f\"\\nKernel: {kernel_size}\"\n",
    "\n",
    "    # Finally, we add the name of the layer to our text, preceded by a newline, to ensure\n",
    "    # the layer name appears on a new line below the shape.\n",
    "    # output_shape_txt += f\"\\n{layer.name}\" #\\n is the space between the feature size (e.g. 28 x 28) and the number of filters (if any)\n",
    "    output_shape_txt += f\"\\n\\n\" #\\n is the space between the feature size (e.g. 28 x 28) and the number of filters (if any)\n",
    "    # The function returns the complete text string and the boolean value 'above',\n",
    "    # which tells visualkeras where to place the label relative to the layer block.\n",
    "\n",
    "    return output_shape_txt, above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8185b545",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualise_model(model: Model, style: str = 'layered'):\n",
    "    \"\"\"\n",
    "    Generates and displays a visual architecture plot for a given Keras model,\n",
    "    with advanced customization options.\n",
    "    \"\"\"\n",
    "    print(f\"--- Visualizing Architecture for: {model.name} (Style: {style}) ---\")\n",
    "    \n",
    "    # Define a custom color map for the layers.\n",
    "    # We use defaultdict for a default grey color if a layer type isn't specified.\n",
    "    color_map = defaultdict(lambda: {'fill': '#999999'})\n",
    "    color_map[Conv2D] = {'fill': '#00B8D4'}\n",
    "    color_map[MaxPooling2D] = {'fill': '#FFAB00'}\n",
    "    color_map[Dense] = {'fill': '#651FFF'}\n",
    "    color_map[Flatten] = {'fill': '#E91E63'}\n",
    "    color_map[Normalization] = {'fill': '#BDBDBD'}\n",
    "\n",
    "    # Define a custom font for the text labels.\n",
    "    # The font size can be adjusted here.\n",
    "    custom_font = ImageFont.truetype(\"arial.ttf\", 14)\n",
    "\n",
    "    # We use a 'with' statement to temporarily ignore any minor warnings\n",
    "    # from the visualkeras library, which keeps our output clean.\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\", UserWarning)\n",
    "        \n",
    "        if style == 'layered':\n",
    "            # This is the core function call for creating the visualization.\n",
    "            # We use the advanced parameters from the provided example code.\n",
    "            # The function now takes the model as an argument, as requested.\n",
    "            advanced_layered_img = visualkeras.layered_view(\n",
    "                model,\n",
    "                text_callable=text_callable,\n",
    "                legend=True,  # Shows a legend to explain what each color represents.\n",
    "                font=custom_font, # Applies our custom-defined font.\n",
    "                color_map=color_map, # Uses our custom color scheme.\n",
    "                draw_volume=True,  # Renders the layers as 3D blocks.\n",
    "                draw_funnel=True,  # Shows a funnel shape to represent data compression.\n",
    "                spacing=30,  # Increases the horizontal space between layers for better readability.\n",
    "                padding=50,  # Adds a border around the entire image to prevent text cutoff.\n",
    "                scale_xy=3, # Adjusts the scale of the layers' width and depth.\n",
    "                scale_z=2,  # Adjusts the scale of the layers' height.\n",
    "                min_xy=10,\n",
    "                max_z=500, # Caps the maximum height of a layer block.\n",
    "                max_xy = 500,\n",
    "                font_color='black', # Sets the text color for the legend and labels.\n",
    "                one_dim_orientation='y', # Sets the orientation for 1D layers like Flatten.\n",
    "                # sizing_mode='accurate', # Ensures that the layer sizes are proportionally accurate.\n",
    "                type_ignore=[Flatten, Dropout], # Excludes Flatten and Dropout layers from the main diagram.\n",
    "            )\n",
    "            \n",
    "            # Display the generated image.\n",
    "            display(advanced_layered_img)\n",
    "\n",
    "        elif style == 'graph':\n",
    "            # This is an alternative view that creates a simpler 2D diagram.\n",
    "            display(visualkeras.graph_view(\n",
    "                model,\n",
    "                color_map=color_map\n",
    "            ))\n",
    "        else:\n",
    "            # Error handling for an unknown style.\n",
    "            print(f\"Error: Unknown style '{style}'. Please choose 'layered' or 'graph'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7341cec",
   "metadata": {},
   "source": [
    "## Plotting the MLP models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d7ab22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, loop through, create each model \"on the fly\", and plot it\n",
    "# The advantage of a model function, rather than a model itself, is that it acts \n",
    "# as a reusable blueprint for creating fresh, untrained model instances on demand, \n",
    "# ensuring that each experiment starts from a clean slate.\n",
    "# It's reusable, flexible with parameters, and memory efficient. \n",
    "\n",
    "for model_creation_function in mlp_model_functions:\n",
    "    # 1. Create the model instance from the function\n",
    "    model_instance: tf.keras.Model = model_creation_function()\n",
    "\n",
    "\n",
    "    # Manually build the model with the correct input shape.\n",
    "    # The `None` indicates a variable batch size. In the case of the MLP, this is an irrelevant parameter. None prevents this code from crashing for MLP models. \n",
    "    model_instance.build(input_shape=(None, 28, 28, 1)) \n",
    "\n",
    "    # 2. Call the plotting function to visualize it\n",
    "    visualise_model(model_instance, style='layered')\n",
    "    \n",
    "    # 3. Print the model summary.\n",
    "    model_instance.summary()\n",
    "\n",
    "    # Where are the activation functions stored?\n",
    "    print(\"\\n--- Activation Functions per Layer ---\")\n",
    "    for layer in model_instance.layers:\n",
    "        if hasattr(layer, 'activation'):\n",
    "            print(f\"Layer: {layer.name:<25} Activation: {layer.activation.__name__}\")\n",
    "    \n",
    "    print(\"=\"*60 + \"\\n\") # Add a separator for clarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de90471",
   "metadata": {},
   "source": [
    "## Plotting the CNN models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de9b87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, loop through, create each model \"on the fly\", and plot it\n",
    "\n",
    "for model_creation_function in cnn_model_functions:\n",
    "    # 1. Create the model instance from the function\n",
    "    model_instance: tf.keras.Model = model_creation_function()\n",
    "\n",
    "    # Manually build the model with the correct input shape.\n",
    "    # The `None` indicates a variable batch size. In the case of the MLP, this is an irrelevant parameter. None prevents this code from crashing for MLP models. \n",
    "    model_instance.build(input_shape=(None, 28, 28, 1)) \n",
    "\n",
    "    # 2. Call the plotting function to visualize it\n",
    "    visualise_model(model_instance, style='layered')\n",
    "    \n",
    "\n",
    "    # 3. Print the model summary.\n",
    "    model_instance.summary()\n",
    "\n",
    "    print(\"=\"*60 + \"\\n\") # Add a separator for clarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc1c442",
   "metadata": {},
   "source": [
    "# Plotting training results\n",
    "Below we plot the results obtained while training all models that were defined previously. Every time run_experiment is called, it returns a History object, which contains details such as training accuracy and loss and validation accuracy and loss. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c387c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history_data: Union[History, SimpleNamespace], run_name_override: Optional[str] = None) -> None:\n",
    "    \"\"\"\n",
    "    Plots training & validation accuracy and loss from either a Keras History object\n",
    "    or a SimpleNamespace object containing WandB run data.\n",
    "\n",
    "    Args:\n",
    "        history_data: A Keras History object or a SimpleNamespace object.\n",
    "        run_name_override (str, optional): A name to use for the plot title.\n",
    "        If None, it will try to infer the name.\n",
    "    \"\"\"\n",
    "    history_dict = None\n",
    "    config = {}\n",
    "    run_name = \"Unknown Run\"\n",
    "\n",
    "    # --- 1. Use duck typing to identify the object ---\n",
    "    # Instead of a strict type check, we check if it has the attributes we need.\n",
    "    if hasattr(history_data, 'history') and isinstance(history_data.history, dict):\n",
    "        # This is a Keras History object from model.fit()\n",
    "        history_dict = history_data.history\n",
    "        if hasattr(history_data, 'hyperparameters'):\n",
    "            config = history_data.hyperparameters\n",
    "        if hasattr(history_data, 'model'):\n",
    "            run_name = history_data.model.name\n",
    "            \n",
    "    elif isinstance(history_data, SimpleNamespace):\n",
    "        # This is our custom object from load_local_run_data()\n",
    "        if hasattr(history_data, 'history') and isinstance(history_data.history, pd.DataFrame):\n",
    "            # The history DataFrame needs its columns converted to lists for plotting\n",
    "            history_dict = {col: history_data.history[col].tolist() for col in history_data.history.columns}\n",
    "        if hasattr(history_data, 'config'):\n",
    "            config = history_data.config\n",
    "        if hasattr(history_data, 'name'):\n",
    "            run_name = history_data.name\n",
    "    \n",
    "    # Use the override name if provided\n",
    "    if run_name_override:\n",
    "        run_name = run_name_override\n",
    "\n",
    "    # Check if we successfully extracted the data\n",
    "    if not history_dict:\n",
    "        print(\"Error: Unsupported or invalid data type provided. Could not extract history.\")\n",
    "        return\n",
    "\n",
    "    # --- 2. Extract and Format Hyperparameters for the Title ---\n",
    "    optimizer_name = config.get('optimiser', 'N/A')\n",
    "    learning_rate = config.get('learning_rate', 'N/A')\n",
    "    batch_size = config.get('batch_size', 'N/A')\n",
    "    subtitle = f\"Optimizer: {optimizer_name}, LR: {learning_rate}, Batch Size: {batch_size}\"\n",
    "\n",
    "    # --- 3. Generate the Plots ---\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    fig.suptitle(f'Training History for: {run_name}\\n{subtitle}', fontsize=16)\n",
    "\n",
    "    # Plot training & validation accuracy values\n",
    "    if 'accuracy' in history_dict and 'val_accuracy' in history_dict:\n",
    "        ax1.plot(history_dict['accuracy'], label='Train Accuracy')\n",
    "        ax1.plot(history_dict['val_accuracy'], label='Validation Accuracy')\n",
    "        ax1.set_title('Model Accuracy')\n",
    "        ax1.set_ylabel('Accuracy')\n",
    "        ax1.set_xlabel('Epoch')\n",
    "        ax1.legend(loc='lower right')\n",
    "        ax1.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "    # Plot training & validation loss values\n",
    "    if 'loss' in history_dict and 'val_loss' in history_dict:\n",
    "        ax2.plot(history_dict['loss'], label='Train Loss')\n",
    "        ax2.plot(history_dict['val_loss'], label='Validation Loss')\n",
    "        ax2.set_title('Model Loss')\n",
    "        ax2.set_ylabel('Loss')\n",
    "        ax2.set_xlabel('Epoch')\n",
    "        ax2.legend(loc='upper right')\n",
    "        ax2.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f08790f",
   "metadata": {},
   "source": [
    "### Plot the results of every epoch\n",
    "\n",
    "The results are plotted below by looping through the histories dictionary that was constructed when we trained all our models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0f9341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple wrapper function to print all histories in a list.\n",
    "def print_training_histories(histories: List[History]) -> None:\n",
    "    print(f\"The number of models to plot: {len(histories)}\")\n",
    "    for history in histories:\n",
    "        print(f\"Model number: {histories.index(history)+1}\")\n",
    "        plot_training_history(history) # This is the function that plots data of a History objects\n",
    "        print(\"-\"*100)\n",
    "\n",
    "if run_mlp_experiments:\n",
    "    print_training_histories(mlp_histories)\n",
    "if run_cnn_experiments:\n",
    "    print_training_histories(cnn_histories)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48cab5e2",
   "metadata": {},
   "source": [
    "## Testing the models on the held-out test set\n",
    "We test the model on the test data, which is data that the model has never seen before. Then we verify the model's real-world accuracy. It is expected that this does not deviate much from the validation sets, because the MNIST dataset contains images that are very clean and simple:\n",
    "- They are small (28 x 28 pixels only).\n",
    "- The digits are centered and normalised in size.\n",
    "- The background is a solid colour with no distracting noise.\n",
    "  \n",
    "Because of this simplicity, the patterns that differentiate one digit from another (e.g., a \"1\" is a vertical line, an \"8\" is two loops) are very strong and easy for our model to learn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7af8a5",
   "metadata": {},
   "source": [
    "First we define a function that browser to a folder with saved models, extracts the file with the highest validation accuracy in its name, loads it and tests it with the held-out X_test and Y_test. \n",
    "\n",
    "A function is convenient because we will use it on different models, with different hyperparameters and hence, avoid repetition. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c29410f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # The main orchestrator. It takes the list from find_top_models, then loops through it, loading each model and passing it to analyse_single_model to be analysed.\n",
    "# def process_model_list(\n",
    "#     models_to_process: List[Dict[str, Any]],\n",
    "#     analysis_func: Callable[[tf.keras.Model, NDArray[np.float32], NDArray[np.int_]], Tuple[Any, ...]],\n",
    "#     x_test_data: NDArray[np.float32],\n",
    "#     y_test_data: NDArray[np.int_]\n",
    "# ) -> List[Dict[str, Any]]:\n",
    "#     \"\"\"\n",
    "#     Processes a given list of models using a provided analysis function.\n",
    "\n",
    "#     Args:\n",
    "#         models_to_process (List[Dict[str, Any]]): A list of dictionaries, where each\n",
    "#             dictionary contains info about a model (e.g., from find_top_models).\n",
    "#         analysis_func (Callable): The function to run on each loaded model.\n",
    "#         x_test_data (NDArray): The test dataset features.\n",
    "#         y_test_data (NDArray): The test dataset labels.\n",
    "\n",
    "#     Returns:\n",
    "#         List[Dict[str, Any]]: A list of dictionaries containing model info and analysis results.\n",
    "#     \"\"\"\n",
    "#     # This function is now fully decoupled. It doesn't know how the models were\n",
    "#     # found; it only knows how to load, process, and collect results for the\n",
    "#     # list it receives. This is an excellent, highly modular design.\n",
    "    \n",
    "#     print(f\"\\n--- Processing {len(models_to_process)} Models ---\")\n",
    "    \n",
    "#     processed_results: List[Dict[str, Any]] = []\n",
    "#     for i, model_info in enumerate(models_to_process):\n",
    "#         print(\"\\n\" + \"=\"*80)\n",
    "#         print(f\"--- Processing Model {i+1} of {len(models_to_process)} (Val Acc: {model_info['val_accuracy']:.4f}) ---\")\n",
    "#         print(f\"Path: {model_info['path']}\")\n",
    "#         print(\"=\"*80)\n",
    "        \n",
    "#         loaded_model: tf.keras.Model = tf.keras.models.load_model(model_info['path'])\n",
    "        \n",
    "#         # Here we call the injected analysis function.\n",
    "#         analysis_output: Tuple[Any, ...] = analysis_func(loaded_model, x_test_data, y_test_data)\n",
    "        \n",
    "#         processed_results.append({\n",
    "#             'model': loaded_model,\n",
    "#             'path': model_info['path'],\n",
    "#             'validation_accuracy': model_info['val_accuracy'],\n",
    "#             'analysis_results': analysis_output\n",
    "#         })\n",
    "        \n",
    "#     return processed_results\n",
    "\n",
    "import pickle\n",
    "from types import SimpleNamespace\n",
    "\n",
    "# (This assumes the other functions like find_top_models and analyse_single_model exist)\n",
    "\n",
    "def process_model_list(\n",
    "    models_to_process: List[Dict[str, Any]],\n",
    "    analysis_func: Callable[[tf.keras.Model, NDArray[np.float32], NDArray[np.int_]], Tuple[Any, ...]],\n",
    "    x_test_data: NDArray[np.float32],\n",
    "    y_test_data: NDArray[np.int_]\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Processes a given list of models,including plotting their training history.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Processing {len(models_to_process)} Models ---\")\n",
    "    \n",
    "    processed_results: List[Dict[str, Any]] = []\n",
    "    for i, model_info in enumerate(models_to_process):\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(f\"--- Processing Model {i+1} of {len(models_to_process)} (Val Acc: {model_info['val_accuracy']:.4f}) ---\")\n",
    "        print(f\"Path: {model_info['path']}\")\n",
    "        \n",
    "        # --- NEW: Load and Plot Training History ---\n",
    "        model_dir: str = os.path.dirname(model_info['path'])\n",
    "        history_path: str = os.path.join(model_dir, 'training_history.pkl')\n",
    "        \n",
    "        if os.path.exists(history_path):\n",
    "            print(f\"Found and loading history from: {history_path}\")\n",
    "            with open(history_path, 'rb') as f:\n",
    "                history_dict: dict = pickle.load(f)\n",
    "            \n",
    "            # We must wrap the loaded dictionary in a SimpleNamespace to match what\n",
    "            # our universal plotting function expects for non-Keras objects.\n",
    "            history_for_plotting = SimpleNamespace(\n",
    "                history=pd.DataFrame(history_dict), # Convert dict to DataFrame\n",
    "                # We need to manually add name and config for the plot titles\n",
    "                name=os.path.basename(model_dir),\n",
    "                config={} # Config isn't in the pickle, so we pass an empty dict\n",
    "            )\n",
    "            plot_training_history(history_for_plotting)\n",
    "        else:\n",
    "            print(f\"Warning: training_history.pkl not found in {model_dir}\")\n",
    "        # --- END OF NEW CODE ---\n",
    "\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        loaded_model: tf.keras.Model = tf.keras.models.load_model(model_info['path'])\n",
    "        \n",
    "        analysis_output: Tuple[Any, ...] = analysis_func(loaded_model, x_test_data, y_test_data)\n",
    "        \n",
    "        processed_results.append({\n",
    "            'model': loaded_model,\n",
    "            'path': model_info['path'],\n",
    "            'validation_accuracy': model_info['val_accuracy'],\n",
    "            'analysis_results': analysis_output\n",
    "        })\n",
    "        \n",
    "    return processed_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8d8f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function's only responsibility is to perform a full analysis (print reports, show plots) on a single, already-loaded model.\n",
    "\n",
    "def analyse_single_model(\n",
    "    loaded_model: tf.keras.Model, \n",
    "    x_test_data: NDArray[np.float32], \n",
    "    y_test_data: NDArray[np.int_]\n",
    ") -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Performs a full analysis of a single loaded Keras model.\n",
    "    ...\n",
    "    \"\"\"\n",
    "    # By isolating all the analysis logic for a single model here, you've created\n",
    "    # a highly modular and reusable component. You could point this function at any\n",
    "    # similarly trained Keras model and get a full report. Well done.\n",
    "    \n",
    "    # --- Print Compiled Hyperparameters ---\n",
    "    print(\"\\n--- Key Hyperparameters ---\")\n",
    "    optimizer_config: Dict[str, Any] = loaded_model.optimizer.get_config()\n",
    "    print(f\"{'Optimiser:':<20} {optimizer_config.get('name', 'N/A')}\")\n",
    "    print(f\"{'Learning Rate:':<20} {optimizer_config.get('learning_rate', 'N/A')}\")\n",
    "    print(f\"{'Loss Function:':<20} {loaded_model.loss}\")\n",
    "    \n",
    "    print(\"\\n--- Model Summary (Architecture) ---\")\n",
    "    loaded_model.summary()\n",
    "\n",
    "    print(\"\\n--- Evaluating model performance on the test set ---\")\n",
    "    # A subtle but important detail! Setting verbose=0 prevents Keras from printing\n",
    "    # its own progress bar here, which is good practice as you're providing your own\n",
    "    # custom, formatted output immediately after. It keeps the console output clean.\n",
    "    loss, accuracy = loaded_model.evaluate(x_test_data, y_test_data, verbose=0)\n",
    "    print(f\"Test Set Loss: {loss:.4f}\")\n",
    "    print(f\"Test Set Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    print(\"\\n--- Detailed Analysis ---\")\n",
    "    y_pred_probabilities: NDArray[np.float32] = loaded_model.predict(x_test_data)\n",
    "    y_pred: NDArray[np.int_] = np.argmax(y_pred_probabilities, axis=1)\n",
    "\n",
    "    print(\"\\n--- Classification Report ---\")\n",
    "    report: str = classification_report(y_test_data, y_pred, target_names=[str(i) for i in range(10)])\n",
    "    print(report)\n",
    "\n",
    "    print(\"\\n--- Confusion Matrix ---\")\n",
    "    cm: NDArray[np.int_] = confusion_matrix(y_test_data, y_pred)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('Actual Label')\n",
    "    plt.title(f'Confusion Matrix for {loaded_model.name}')\n",
    "    plt.show()\n",
    "    \n",
    "    # A great extension to this function would be to also return the generated 'report'\n",
    "    # and 'cm' objects. This would allow the calling function to, for example,\n",
    "    # save the confusion matrix image to a file or programmatically find the class\n",
    "    # with the lowest F1-score from the report.\n",
    "    return accuracy, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182adbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function's only responsibility is to search folders and return a ranked list of model file paths. It doesn't load or analyse anything.\n",
    "\n",
    "def find_top_models(parent_folder: str, top_n: int) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Scans a directory to find all Keras models and returns a sorted list of the top N.\n",
    "\n",
    "    This function finds model files named with a 'val_acc-...' pattern, extracts the\n",
    "    accuracy, and returns a list of dictionaries containing the path and validation\n",
    "    accuracy for the best models found.\n",
    "\n",
    "    Args:\n",
    "        parent_folder (str): The root directory to start the search from.\n",
    "        top_n (int): The number of top models to return.\n",
    "\n",
    "    Returns:\n",
    "        List[Dict[str, Any]]: A list of dictionaries, each representing a model,\n",
    "        sorted by validation accuracy in descending order.\n",
    "        Example: [{'path': 'path/to/model.keras', 'val_accuracy': 0.985}]\n",
    "    \"\"\"\n",
    "    # This function is a great example of the 'separation of concerns' principle.\n",
    "    # Its only job is to interact with the file system. By not loading the heavyweight\n",
    "    # TensorFlow models here, we make the function incredibly fast and memory-efficient.\n",
    "    # This is a crucial distinction between a simple script and a well-designed programme.\n",
    "    all_models: List[Dict[str, Any]] = []\n",
    "    pattern: re.Pattern = re.compile(r\"val_acc-([\\d.]+)\\.keras\")\n",
    "\n",
    "    if not os.path.isdir(parent_folder):\n",
    "        print(f\"Error: Parent directory not found at '{parent_folder}'\")\n",
    "        return []\n",
    "\n",
    "    for dirpath, _, filenames in os.walk(parent_folder):\n",
    "        for filename in filenames:\n",
    "            match: Union[re.Match, None] = pattern.search(filename)\n",
    "            if match:\n",
    "                # This is a great use of regular expressions to parse metadata from a filename.\n",
    "                # For added robustness in a real-world application, we might wrap this float()\n",
    "                # conversion in a try-except block to gracefully handle any malformed filenames.\n",
    "                # This is a concept called 'defensive programming'.\n",
    "                val_accuracy: float = float(match.group(1))\n",
    "                model_path: str = os.path.join(dirpath, filename)\n",
    "                all_models.append({'path': model_path, 'val_accuracy': val_accuracy})\n",
    "    \n",
    "    # Python's built-in `sorted` function is highly efficient. Using a `lambda` function\n",
    "    # as the 'key' is a powerful, concise way to specify a custom sorting rule without\n",
    "    # needing to define a separate, named function.\n",
    "    sorted_models: List[Dict[str, Any]] = sorted(all_models, key=lambda x: x['val_accuracy'], reverse=True)\n",
    "    \n",
    "    # List slicing is a clean and Pythonic way to select a subset of a list.\n",
    "    return sorted_models[:top_n]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b24f712",
   "metadata": {},
   "source": [
    "### Testing the MLP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f0f21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Find the top models. \n",
    "top_3_models_to_analyse: List[Dict[str, Any]] = find_top_models(\n",
    "    parent_folder='MLP_Models', \n",
    "    top_n=3\n",
    ")\n",
    "\n",
    "# Step 2: Pass the list of found models to the processing function.\n",
    "if top_3_models_to_analyse:\n",
    "    final_results: List[Dict[str, Any]] = process_model_list(\n",
    "        models_to_process=top_3_models_to_analyse,\n",
    "        analysis_func=analyse_single_model,\n",
    "        x_test_data=X_test,\n",
    "        y_test_data=Y_test\n",
    "    )\n",
    "    print(f\"\\nCompleted analysis. Processed {len(final_results)} models.\")\n",
    "else:\n",
    "    print(\"No models found to process.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c2e325",
   "metadata": {},
   "source": [
    "### Testing the CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e42a562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Find the top models. \n",
    "top_3_models_to_analyse: List[Dict[str, Any]] = find_top_models(\n",
    "    parent_folder='CNN_Models', \n",
    "    top_n=3\n",
    ")\n",
    "\n",
    "# Step 2: Pass the list of found models to the processing function.\n",
    "if top_3_models_to_analyse:\n",
    "    final_results: List[Dict[str, Any]] = process_model_list(\n",
    "        models_to_process=top_3_models_to_analyse,\n",
    "        analysis_func=analyse_single_model,\n",
    "        x_test_data=X_test,\n",
    "        y_test_data=Y_test\n",
    "    )\n",
    "    print(f\"\\nCompleted analysis. Processed {len(final_results)} models.\")\n",
    "else:\n",
    "    print(\"No models found to process.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9820dc3",
   "metadata": {},
   "source": [
    "# Future to do's (not part of this assessment)\n",
    "- Implement KerasTuner (gridsearch amongst others), to automatically train and test models with a plethora of hyperparamaters, optimisers, loss functions, and select the best performing:\n",
    "\n",
    "We first need to install it first: uv pip install keras-tuner\n",
    "import keras_tuner\n",
    "\n",
    "def build_model(hp):\n",
    "    \"\"\"This is our hypermodel, which defines the search space.\"\"\"\n",
    "    \n",
    "    model = Sequential(name=\"Tuned_MLP\")\n",
    "    model.add(Input(shape=(28, 28)))\n",
    "    model.add(Normalization())\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    # --- Define Hyperparameters to Tune ---\n",
    "    # Tune the number of units in the first Dense layer\n",
    "    hp_units = hp.Int('units', min_value=32, max_value=512, step=32)\n",
    "    model.add(Dense(units=hp_units, activation='relu'))\n",
    "    \n",
    "    # Tune the learning rate\n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "    \n",
    "    # Add the output layer\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "    # --- Compile the model inside the function ---\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "--- Set up the Tuner ---\n",
    "### We'll use RandomSearch, which randomly tries combinations.\n",
    "tuner = keras_tuner.RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=10,  # The total number of model variations to test\n",
    "    executions_per_trial=2, # The number of times to train each model variation\n",
    "    directory='tuning_results',\n",
    "    project_name='MNIST_Tuning'\n",
    ")\n",
    "\n",
    "### --- Start the Search ---\n",
    "### This is like model.fit(), but it runs the whole tuning process.\n",
    "tuner.search(X_train, Y_train, epochs=10, validation_split=0.1)\n",
    "\n",
    "### --- Get the Best Model ---\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "best_hyperparameters = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(\"\\n--- Best Hyperparameters Found ---\")\n",
    "print(best_hyperparameters.values)\n",
    "\n",
    "print(\"\\n--- Evaluating the Best Model Found by the Tuner ---\")\n",
    "best_model.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2428276c",
   "metadata": {},
   "source": [
    "# Testing a new approach (Future to do)\n",
    "### A Base Class for models, with a common interface and allowing for inheriting layers, inheriting behaviour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7275e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # models.py\n",
    "# # This file serves as a centralised factory for creating our neural network models.\n",
    "# # It uses a class-based, inherited structure to keep the codebase organised and extensible.\n",
    "\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.layers import Normalization, Flatten, Dense, Conv2D, MaxPooling2D\n",
    "# from tensorflow.keras.models import Model\n",
    "\n",
    "# class BaseNeuralNetwork(Model):\n",
    "#     \"\"\"\n",
    "#     A base class for all neural networks in this project.\n",
    "#     It encapsulates the common input and preprocessing layers that all models will share.\n",
    "#     \"\"\"\n",
    "#     def __init__(self, **kwargs):\n",
    "#         # We call the parent constructor to ensure correct initialisation of the Keras Model.\n",
    "#         # **kwargs allows us to pass additional arguments like 'name' when creating subclasses.\n",
    "#         super().__init__(**kwargs)\n",
    "#         # These layers are common to all models and are defined here once for efficiency.\n",
    "#         self.normalization_layer = Normalization(name=\"normalization_layer\")\n",
    "#         self.flatten_layer = Flatten(name=\"flatten_layer\")\n",
    "\n",
    "#     def call(self, inputs):\n",
    "#         \"\"\"\n",
    "#         Defines the forward pass for the common preprocessing layers.\n",
    "#         \"\"\"\n",
    "#         # The input data is passed through the normalisation and flattening layers.\n",
    "#         x = self.normalization_layer(inputs)\n",
    "#         x = self.flatten_layer(x)\n",
    "#         return x\n",
    "\n",
    "# class MLPModel(BaseNeuralNetwork):\n",
    "#     \"\"\"\n",
    "#     A standard Multi-Layer Perceptron (MLP) model.\n",
    "#     It inherits the base preprocessing from BaseNeuralNetwork and adds dense layers.\n",
    "#     \"\"\"\n",
    "#     def __init__(self, num_units_1: int = 128, num_units_2: int = 256, num_units_3: int = 64, num_classes: int = 10, **kwargs):\n",
    "#         # We call the parent constructor and provide a specific name for this model.\n",
    "#         super().__init__(name='mlp_model', **kwargs)\n",
    "#         # Define the unique dense layers for this specific model architecture.\n",
    "#         self.dense_1 = Dense(units=num_units_1, activation='relu', name=\"dense_1\")\n",
    "#         self.dense_2 = Dense(units=num_units_2, activation='relu', name=\"dense_2\")\n",
    "#         self.dense_3 = Dense(units=num_units_3, activation='relu', name=\"dense_3\")\n",
    "#         self.output_layer = Dense(units=num_classes, activation='softmax', name=\"output_layer\")\n",
    "\n",
    "#     def call(self, inputs):\n",
    "#         # First, we process the input using the base class's call method.\n",
    "#         x = super().call(inputs)\n",
    "#         # Then, we pass the output through the MLP-specific layers.\n",
    "#         x = self.dense_1(x)\n",
    "#         x = self.dense_2(x)\n",
    "#         x = self.dense_3(x)\n",
    "#         return self.output_layer(x)\n",
    "\n",
    "# class MLP_Wide_Model(BaseNeuralNetwork):\n",
    "#     \"\"\"\n",
    "#     A wider, shallower MLP model. This is a variation for experimentation.\n",
    "#     \"\"\"\n",
    "#     def __init__(self, num_units_1: int = 256, num_units_2: int = 128, num_classes: int = 10, **kwargs):\n",
    "#         super().__init__(name='mlp_wide_model', **kwargs)\n",
    "#         # This model has a different configuration of dense layers.\n",
    "#         self.dense_1 = Dense(units=num_units_1, activation='relu', name=\"dense_1\")\n",
    "#         self.dense_2 = Dense(units=num_units_2, activation='relu', name=\"dense_2\")\n",
    "#         self.output_layer = Dense(units=num_classes, activation='softmax', name=\"output_layer\")\n",
    "\n",
    "#     def call(self, inputs):\n",
    "#         x = super().call(inputs)\n",
    "#         x = self.dense_1(x)\n",
    "#         x = self.dense_2(x)\n",
    "#         return self.output_layer(x)\n",
    "\n",
    "# class SimpleCNN(BaseNeuralNetwork):\n",
    "#     \"\"\"\n",
    "#     A simple Convolutional Neural Network (CNN) model for image classification.\n",
    "#     \"\"\"\n",
    "#     def __init__(self, num_classes: int = 10, **kwargs):\n",
    "#         super().__init__(name='simple_cnn', **kwargs)\n",
    "#         # The convolutional and pooling layers are unique to CNNs.\n",
    "#         self.conv1 = Conv2D(filters=32, kernel_size=(3, 3), activation='relu', name=\"conv1\")\n",
    "#         self.pool1 = MaxPooling2D(pool_size=(2, 2), name=\"pool1\")\n",
    "#         self.conv2 = Conv2D(filters=64, kernel_size=(3, 3), activation='relu', name=\"conv2\")\n",
    "#         self.pool2 = MaxPooling2D(pool_size=(2, 2), name=\"pool2\")\n",
    "#         self.output_layer = Dense(units=num_classes, activation='softmax', name=\"output_layer\")\n",
    "\n",
    "#     def call(self, inputs):\n",
    "#         # We start by using the base class's normalisation.\n",
    "#         x = self.normalization_layer(inputs)\n",
    "        \n",
    "#         # Then, we pass the output through the CNN-specific layers.\n",
    "#         x = self.conv1(x)\n",
    "#         x = self.pool1(x)\n",
    "#         x = self.conv2(x)\n",
    "#         x = self.pool2(x)\n",
    "\n",
    "#         # The flatten layer from the base class is still applied before the output layer.\n",
    "#         x = self.flatten_layer(x)\n",
    "        \n",
    "#         return self.output_layer(x)\n",
    "\n",
    "# class DeepCNN(BaseNeuralNetwork):\n",
    "#     \"\"\"\n",
    "#     A deeper CNN model with more layers for greater representational capacity.\n",
    "#     \"\"\"\n",
    "#     def __init__(self, num_classes: int = 10, **kwargs):\n",
    "#         super().__init__(name='deep_cnn', **kwargs)\n",
    "#         # This model has a more complex arrangement of convolutional layers.\n",
    "#         self.conv1 = Conv2D(32, (3, 3), activation='relu')\n",
    "#         self.conv2 = Conv2D(32, (3, 3), activation='relu')\n",
    "#         self.pool1 = MaxPooling2D(pool_size=(2, 2))\n",
    "#         self.conv3 = Conv2D(64, (3, 3), activation='relu')\n",
    "#         self.conv4 = Conv2D(64, (3, 3), activation='relu')\n",
    "#         self.pool2 = MaxPooling2D(pool_size=(2, 2))\n",
    "#         self.dense1 = Dense(128, activation='relu')\n",
    "#         self.output_layer = Dense(units=num_classes, activation='softmax')\n",
    "\n",
    "#     def call(self, inputs):\n",
    "#         x = self.normalization_layer(inputs)\n",
    "#         x = self.conv1(x)\n",
    "#         x = self.conv2(x)\n",
    "#         x = self.pool1(x)\n",
    "#         x = self.conv3(x)\n",
    "#         x = self.conv4(x)\n",
    "#         x = self.pool2(x)\n",
    "#         x = self.flatten_layer(x)\n",
    "#         x = self.dense1(x)\n",
    "#         return self.output_layer(x)\n",
    "\n",
    "# def create_model_from_class(model_class: type, input_shape, **kwargs):\n",
    "#     \"\"\"\n",
    "#     A helper function to instantiate a model class with a defined input shape.\n",
    "#     It automatically builds the model for you.\n",
    "#     \"\"\"\n",
    "#     model_instance = model_class(**kwargs)\n",
    "#     # The build method ensures that the model's layers are initialised with the correct input shape.\n",
    "#     model_instance.build(input_shape=(None,) + input_shape)\n",
    "#     return model_instance\n",
    "\n",
    "\n",
    "# Just a thought for now. It may or may not simplify creating models, but only by a margin. It does however increase complexity of the code, so that's the trade-off. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70bd9c8a",
   "metadata": {},
   "source": [
    "## A function to pull data from the WandB server, so we can use it locally. \n",
    "\n",
    "Note that this requires the implementation of additional functions to be able to print the model architecture AND the associated training data of the model. \n",
    "This is beyond the scope of this assessment, and is a future to do. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0655d33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # In the cell above, we store all the models, run history, and hyperparameters in the Wandb cloud platform. We save the model architectures + their best W ad b Locally as well.\n",
    "# # The function below can be used to pull all that data from the cloud and store it locally as well. \n",
    "# # This can be useful if we want to analyse it with python. E.g., plotting the accuracy and losses locally, rather than in the cloud with limited customisation tools. \n",
    "\n",
    "# def pull_wandb_data_from_cloud(project_name, local_dir):\n",
    "#     \"\"\"\n",
    "#     Pulls all runs, models, and artifacts from a WandB project\n",
    "#     and stores them locally, using the run's expressive name for the folder.\n",
    "\n",
    "#     Args:\n",
    "#         project_name (str): The name of the WandB project.\n",
    "#         local_dir (str): The local directory to save the data.\n",
    "#     \"\"\"\n",
    "#     print(f\"Connecting to WandB project: {project_name}\")\n",
    "#     api = wandb.Api()\n",
    "    \n",
    "#     try:\n",
    "#         user_name = api.default_entity\n",
    "#     except wandb.errors.CommError:\n",
    "#         print(\"Error: Could not retrieve WandB username. Make sure you are logged in.\")\n",
    "#         return\n",
    "        \n",
    "#     os.makedirs(local_dir, exist_ok=True)\n",
    "    \n",
    "#     runs = api.runs(f\"{user_name}/{project_name}\")\n",
    "    \n",
    "#     print(f\"Found {len(runs)} runs for user: {user_name}. Downloading data...\")\n",
    "\n",
    "#     for run in runs:\n",
    "#         # --- Fix: Create a clean, expressive folder name ---\n",
    "#         # 1. Start with the expressive run name.\n",
    "#         run_folder_name = run.name\n",
    "        \n",
    "#         # 2. Sanitize the name to remove any characters that are invalid for file paths.\n",
    "#         sanitized_run_name = re.sub(r'[\\\\/:*?\"<>|]', '_', run_folder_name)\n",
    "        \n",
    "#         # 3. We no longer append the unique run ID, since the timestamp already ensures uniqueness.\n",
    "#         final_folder_name = sanitized_run_name\n",
    "        \n",
    "#         # Construct the full path for the run's local directory.\n",
    "#         run_dir = os.path.join(local_dir, final_folder_name)\n",
    "#         os.makedirs(run_dir, exist_ok=True)\n",
    "        \n",
    "#         # Download all files associated with the run\n",
    "#         for file in run.files():\n",
    "#             file.download(root=run_dir, exist_ok=True)\n",
    "            \n",
    "#         print(f\"Downloaded files for run: {final_folder_name}\")\n",
    "\n",
    "#         # Download artifacts (including models)\n",
    "#         for artifact in run.logged_artifacts():\n",
    "#             print(f\"Downloading artifact: {artifact.name}\")\n",
    "            \n",
    "#             sanitized_artifact_name = re.sub(r'[\\\\/:*?\"<>|]', '_', artifact.name)\n",
    "            \n",
    "#             artifact_dir = os.path.join(run_dir, \"artifacts\", sanitized_artifact_name)\n",
    "#             os.makedirs(artifact_dir, exist_ok=True)\n",
    "#             artifact.download(root=artifact_dir)\n",
    "\n",
    "#     print(\"Download complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34d25a5",
   "metadata": {},
   "source": [
    "### Below we fetch the data from WandB cloud via its API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1117c3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # We get the current working directory.\n",
    "# current_dir = os.getcwd()\n",
    "# # We create the parent folder path using os.path.join.\n",
    "# parent_folder_name = \"WandB_downloads\"\n",
    "# wandb_folder = os.path.join(current_dir, parent_folder_name)\n",
    "# # We now call the function with the correct local directory.\n",
    "# pull_wandb_data_from_cloud(\"CSE5ML-Assessment2\", wandb_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe03810",
   "metadata": {},
   "source": [
    "### Plotting results using WandB run objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c37ebac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_and_plot_all_runs(parent_folder: str) -> None:\n",
    "#     \"\"\"\n",
    "#     Scans all subdirectories in a parent folder, loads the WandB run data \n",
    "#     (config and history) from each, and then calls the universal plotting function \n",
    "#     for each valid run found. This function is self-contained.\n",
    "\n",
    "#     Args:\n",
    "#         parent_folder (str): The path to the directory containing all downloaded run folders.\n",
    "#     \"\"\"\n",
    "#     print(f\"--- Scanning and plotting all runs in '{parent_folder}' ---\")\n",
    "#     if not os.path.isdir(parent_folder):\n",
    "#         print(f\"Error: Directory not found at '{parent_folder}'\")\n",
    "#         return\n",
    "\n",
    "#     run_folders: List[str] = [d for d in os.listdir(parent_folder) if os.path.isdir(os.path.join(parent_folder, d))]\n",
    "#     print(f\"Found {len(run_folders)} potential run folders to process.\")\n",
    "    \n",
    "#     plotted_count: int = 0\n",
    "\n",
    "#     for run_folder_name in sorted(run_folders):\n",
    "#         run_folder_path: str = os.path.join(parent_folder, run_folder_name)\n",
    "#         print(f\"\\nProcessing folder: {run_folder_name}\")\n",
    "\n",
    "#         # --- Start of Merged Data Loading Logic ---\n",
    "#         config_file: str = os.path.join(run_folder_path, 'config.yaml')\n",
    "        \n",
    "#         # 1. Find and load the config file\n",
    "#         if not os.path.exists(config_file):\n",
    "#             print(f\"  -> Skipping: config.yaml not found.\")\n",
    "#             continue\n",
    "#         try:\n",
    "#             with open(config_file, 'r') as f:\n",
    "#                 config: Any = yaml.safe_load(f)\n",
    "#                 config_data: Dict[str, Any] = {k: v['value'] for k, v in config.items() if isinstance(v, dict) and 'value' in v}\n",
    "#         except Exception as e:\n",
    "#             print(f\"  -> Skipping: Could not load config.yaml. Error: {e}\")\n",
    "#             continue\n",
    "\n",
    "#         # 2. Find and load all history .parquet files\n",
    "#         history_files: List[str] = []\n",
    "#         artifacts_dir: str = os.path.join(run_folder_path, 'artifacts')\n",
    "#         if os.path.isdir(artifacts_dir):\n",
    "#             for root, _, files in os.walk(artifacts_dir):\n",
    "#                 for file in files:\n",
    "#                     if file.endswith('.parquet'):\n",
    "#                         history_files.append(os.path.join(root, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed950f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to your main downloads folder\n",
    "wandb_downloads_path = os.path.join(os.getcwd(), 'WandB_downloads')\n",
    "\n",
    "# Call the new function to process and plot everything inside that folder\n",
    "load_and_plot_all_runs(wandb_downloads_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
